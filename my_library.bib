@misc{210412081HowWell,
  title = {[2104.12081] {{How Well Does Self-Supervised Pre-Training Perform}} with {{Streaming Data}}?},
  urldate = {2025-10-03},
  howpublished = {https://arxiv.org/abs/2104.12081},
  file = {/Users/giulialanzillotta/Zotero/storage/WPLYA2C3/2104.html}
}

@misc{240313512ScaleDecoupled,
  title = {[2403.13512] {{Scale Decoupled Distillation}}},
  urldate = {2025-10-13},
  howpublished = {https://arxiv.org/abs/2403.13512?utm\_source=chatgpt.com},
  file = {/Users/giulialanzillotta/Zotero/storage/A55WVRFY/2403.html}
}

@misc{abbesRevisitingReplayGradient2025,
  title = {Revisiting {{Replay}} and {{Gradient Alignment}} for {{Continual Pre-Training}} of {{Large Language Models}}},
  author = {Abbes, Istabrak and Subbaraj, Gopeshh and Riemer, Matthew and Islah, Nizar and Therien, Benjamin and Tabaru, Tsuguchika and Kingetsu, Hiroaki and Chandar, Sarath and Rish, Irina},
  year = 2025,
  month = aug,
  number = {arXiv:2508.01908},
  eprint = {2508.01908},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2508.01908},
  urldate = {2025-10-03},
  abstract = {Training large language models (LLMs) typically involves pre-training on massive corpora, only to restart the process entirely when new data becomes available. A more efficient and resource-conserving approach would be continual pre-training, where models are updated with new data rather than retraining from scratch. However, the introduction of new data often causes distribution shifts, leading to performance degradation on previously learned tasks. In this paper, we take a deeper look at two popular proposals for addressing this distribution shift within the continual learning literature: experience replay and gradient alignment. We consider continual pre-training of models within the Llama family of architectures at a large scale across languages with 100 billion tokens of training data in each language, finding that both replay and gradient alignment lead to more stable learning without forgetting. This conclusion holds both as we vary the model scale and as we vary the number and diversity of tasks. Moreover, we are the first to demonstrate the effectiveness of gradient alignment techniques in the context of LLM pre-training and propose an efficient implementation of meta-experience replay (MER) that imbues experience replay with the benefits of gradient alignment despite negligible compute and memory overhead. Our scaling analysis across model sizes and replay rates indicates that small rates of replaying old examples are definitely a more valuable use of compute than investing in model size, but that it is more compute efficient to scale the size of the model than invest in high rates of replaying old examples.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/ADQSYIMF/Abbes et al. - 2025 - Revisiting Replay and Gradient Alignment for Continual Pre-Training of Large Language Models.pdf;/Users/giulialanzillotta/Zotero/storage/B8I4X2JP/2508.html}
}

@misc{abbesRevisitingReplayGradient2025a,
  title = {Revisiting {{Replay}} and {{Gradient Alignment}} for {{Continual Pre-Training}} of {{Large Language Models}}},
  author = {Abbes, Istabrak and Subbaraj, Gopeshh and Riemer, Matthew and Islah, Nizar and Therien, Benjamin and Tabaru, Tsuguchika and Kingetsu, Hiroaki and Chandar, Sarath and Rish, Irina},
  year = 2025,
  month = aug,
  number = {arXiv:2508.01908},
  eprint = {2508.01908},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2508.01908},
  urldate = {2025-10-03},
  abstract = {Training large language models (LLMs) typically involves pre-training on massive corpora, only to restart the process entirely when new data becomes available. A more efficient and resource-conserving approach would be continual pre-training, where models are updated with new data rather than retraining from scratch. However, the introduction of new data often causes distribution shifts, leading to performance degradation on previously learned tasks. In this paper, we take a deeper look at two popular proposals for addressing this distribution shift within the continual learning literature: experience replay and gradient alignment. We consider continual pre-training of models within the Llama family of architectures at a large scale across languages with 100 billion tokens of training data in each language, finding that both replay and gradient alignment lead to more stable learning without forgetting. This conclusion holds both as we vary the model scale and as we vary the number and diversity of tasks. Moreover, we are the first to demonstrate the effectiveness of gradient alignment techniques in the context of LLM pre-training and propose an efficient implementation of meta-experience replay (MER) that imbues experience replay with the benefits of gradient alignment despite negligible compute and memory overhead. Our scaling analysis across model sizes and replay rates indicates that small rates of replaying old examples are definitely a more valuable use of compute than investing in model size, but that it is more compute efficient to scale the size of the model than invest in high rates of replaying old examples.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/5ABYH868/Abbes et al. - 2025 - Revisiting Replay and Gradient Alignment for Continual Pre-Training of Large Language Models.pdf;/Users/giulialanzillotta/Zotero/storage/8IDK6EBQ/2508.html}
}

@misc{abbesRevisitingReplayGradient2025b,
  title = {Revisiting {{Replay}} and {{Gradient Alignment}} for {{Continual Pre-Training}} of {{Large Language Models}}},
  author = {Abbes, Istabrak and Subbaraj, Gopeshh and Riemer, Matthew and Islah, Nizar and Therien, Benjamin and Tabaru, Tsuguchika and Kingetsu, Hiroaki and Chandar, Sarath and Rish, Irina},
  year = 2025,
  month = aug,
  number = {arXiv:2508.01908},
  eprint = {2508.01908},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2508.01908},
  urldate = {2025-10-13},
  abstract = {Training large language models (LLMs) typically involves pre-training on massive corpora, only to restart the process entirely when new data becomes available. A more efficient and resource-conserving approach would be continual pre-training, where models are updated with new data rather than retraining from scratch. However, the introduction of new data often causes distribution shifts, leading to performance degradation on previously learned tasks. In this paper, we take a deeper look at two popular proposals for addressing this distribution shift within the continual learning literature: experience replay and gradient alignment. We consider continual pre-training of models within the Llama family of architectures at a large scale across languages with 100 billion tokens of training data in each language, finding that both replay and gradient alignment lead to more stable learning without forgetting. This conclusion holds both as we vary the model scale and as we vary the number and diversity of tasks. Moreover, we are the first to demonstrate the effectiveness of gradient alignment techniques in the context of LLM pre-training and propose an efficient implementation of meta-experience replay (MER) that imbues experience replay with the benefits of gradient alignment despite negligible compute and memory overhead. Our scaling analysis across model sizes and replay rates indicates that small rates of replaying old examples are definitely a more valuable use of compute than investing in model size, but that it is more compute efficient to scale the size of the model than invest in high rates of replaying old examples.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/V8R2KRW8/Abbes et al. - 2025 - Revisiting Replay and Gradient Alignment for Continual Pre-Training of Large Language Models.pdf;/Users/giulialanzillotta/Zotero/storage/ZKPCDI2M/2508.html}
}

@misc{abbesRevisitingReplayGradient2025c,
  title = {Revisiting {{Replay}} and {{Gradient Alignment}} for {{Continual Pre-Training}} of {{Large Language Models}}},
  author = {Abbes, Istabrak and Subbaraj, Gopeshh and Riemer, Matthew and Islah, Nizar and Therien, Benjamin and Tabaru, Tsuguchika and Kingetsu, Hiroaki and Chandar, Sarath and Rish, Irina},
  year = 2025,
  month = aug,
  number = {arXiv:2508.01908},
  eprint = {2508.01908},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2508.01908},
  urldate = {2025-10-13},
  abstract = {Training large language models (LLMs) typically involves pre-training on massive corpora, only to restart the process entirely when new data becomes available. A more efficient and resource-conserving approach would be continual pre-training, where models are updated with new data rather than retraining from scratch. However, the introduction of new data often causes distribution shifts, leading to performance degradation on previously learned tasks. In this paper, we take a deeper look at two popular proposals for addressing this distribution shift within the continual learning literature: experience replay and gradient alignment. We consider continual pre-training of models within the Llama family of architectures at a large scale across languages with 100 billion tokens of training data in each language, finding that both replay and gradient alignment lead to more stable learning without forgetting. This conclusion holds both as we vary the model scale and as we vary the number and diversity of tasks. Moreover, we are the first to demonstrate the effectiveness of gradient alignment techniques in the context of LLM pre-training and propose an efficient implementation of meta-experience replay (MER) that imbues experience replay with the benefits of gradient alignment despite negligible compute and memory overhead. Our scaling analysis across model sizes and replay rates indicates that small rates of replaying old examples are definitely a more valuable use of compute than investing in model size, but that it is more compute efficient to scale the size of the model than invest in high rates of replaying old examples.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/8AV4AGES/Abbes et al. - 2025 - Revisiting Replay and Gradient Alignment for Continual Pre-Training of Large Language Models.pdf;/Users/giulialanzillotta/Zotero/storage/W8BS85GD/2508.html}
}

@article{abbesRevisitingReplayGradient2025d,
  title = {Revisiting {{Replay}} and {{Gradient Alignment}} for {{Continual Pre-Training}} of {{Large Language Models}}},
  author = {Abbes, Istabrak and Subbaraj, Gopeshh and Riemer, Matthew and Islah, Nizar and Therien, Benjamin and Tabaru, Tsuguchika and Kingetsu, Hiroaki and Chandar, Sarath and Rish, Irina},
  year = 2025,
  month = aug,
  journal = {arXiv e-prints},
  pages = {arXiv:2508.01908},
  doi = {10.48550/arXiv.2508.01908},
  urldate = {2025-10-13},
  abstract = {Training large language models (LLMs) typically involves pre-training on massive corpora, only to restart the process entirely when new data becomes available. A more efficient and resource-conserving approach would be continual pre-training, where models are updated with new data rather than retraining from scratch. However, the introduction of new data often causes distribution shifts, leading to performance degradation on previously learned tasks. In this paper, we take a deeper look at two popular proposals for addressing this distribution shift within the continual learning literature: experience replay and gradient alignment. We consider continual pre-training of models within the Llama family of architectures at a large scale across languages with 100 billion tokens of training data in each language, finding that both replay and gradient alignment lead to more stable learning without forgetting. This conclusion holds both as we vary the model scale and as we vary the number and diversity of tasks. Moreover, we are the first to demonstrate the effectiveness of gradient alignment techniques in the context of LLM pre-training and propose an efficient implementation of meta-experience replay (MER) that imbues experience replay with the benefits of gradient alignment despite negligible compute and memory overhead. Our scaling analysis across model sizes and replay rates indicates that small rates of replaying old examples are definitely a more valuable use of compute than investing in model size, but that it is more compute efficient to scale the size of the model than invest in high rates of replaying old examples.},
  langid = {english},
  file = {/Users/giulialanzillotta/Zotero/storage/B66KNZ3K/Abbes et al. - 2025 - Revisiting Replay and Gradient Alignment for Continual Pre-Training of Large Language Models.pdf}
}

@article{abrahamMemoryRetentionSynaptic2005,
  title = {Memory Retention -- the Synaptic Stability versus Plasticity Dilemma},
  author = {Abraham, Wickliffe C. and Robins, Anthony},
  year = 2005,
  month = feb,
  journal = {Trends in Neurosciences},
  volume = {28},
  number = {2},
  pages = {73--78},
  issn = {01662236},
  doi = {10.1016/j.tins.2004.12.003},
  urldate = {2025-05-02},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  file = {/Users/giulialanzillotta/Zotero/storage/KYUZP2FF/Abraham and Robins - 2005 - Memory retention â€“ the synaptic stability versus plasticity dilemma.pdf}
}

@misc{ahnUncertaintybasedContinualLearning2019,
  title = {Uncertainty-Based {{Continual Learning}} with {{Adaptive Regularization}}},
  author = {Ahn, Hongjoon and Cha, Sungmin and Lee, Donggyu and Moon, Taesup},
  year = 2019,
  month = nov,
  number = {arXiv:1905.11614},
  eprint = {1905.11614},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1905.11614},
  urldate = {2025-10-01},
  abstract = {We introduce a new neural network-based continual learning algorithm, dubbed as Uncertainty-regularized Continual Learning (UCL), which builds on traditional Bayesian online learning framework with variational inference. We focus on two significant drawbacks of the recently proposed regularization-based methods: a) considerable additional memory cost for determining the per-weight regularization strengths and b) the absence of gracefully forgetting scheme, which can prevent performance degradation in learning new tasks. In this paper, we show UCL can solve these two problems by introducing a fresh interpretation on the Kullback-Leibler (KL) divergence term of the variational lower bound for Gaussian mean-field approximation. Based on the interpretation, we propose the notion of node-wise uncertainty, which drastically reduces the number of additional parameters for implementing per-weight regularization. Moreover, we devise two additional regularization terms that enforce stability by freezing important parameters for past tasks and allow plasticity by controlling the actively learning parameters for a new task. Through extensive experiments, we show UCL convincingly outperforms most of recent state-of-the-art baselines not only on popular supervised learning benchmarks, but also on challenging lifelong reinforcement learning tasks. The source code of our algorithm is available at https://github.com/csm9493/UCL.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/UDXYY9XJ/Ahn et al. - 2019 - Uncertainty-based Continual Learning with Adaptive Regularization.pdf;/Users/giulialanzillotta/Zotero/storage/3NIY99T4/1905.html}
}

@misc{aljundiExpertGateLifelong2017,
  title = {Expert {{Gate}}: {{Lifelong Learning}} with a {{Network}} of {{Experts}}},
  shorttitle = {Expert {{Gate}}},
  author = {Aljundi, Rahaf and Chakravarty, Punarjay and Tuytelaars, Tinne},
  year = 2017,
  month = apr,
  number = {arXiv:1611.06194},
  eprint = {1611.06194},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1611.06194},
  urldate = {2025-10-01},
  abstract = {In this paper we introduce a model of lifelong learning, based on a Network of Experts. New tasks / experts are learned and added to the model sequentially, building on what was learned before. To ensure scalability of this process,data from previous tasks cannot be stored and hence is not available when learning a new task. A critical issue in such context, not addressed in the literature so far, relates to the decision which expert to deploy at test time. We introduce a set of gating autoencoders that learn a representation for the task at hand, and, at test time, automatically forward the test sample to the relevant expert. This also brings memory efficiency as only one expert network has to be loaded into memory at any given time. Further, the autoencoders inherently capture the relatedness of one task to another, based on which the most relevant prior model to be used for training a new expert, with finetuning or learning without-forgetting, can be selected. We evaluate our method on image classification and video prediction problems.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Statistics - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/LSUCC5ER/Aljundi et al. - 2017 - Expert Gate Lifelong Learning with a Network of Experts.pdf;/Users/giulialanzillotta/Zotero/storage/WAVJ9JFH/1611.html}
}

@article{aljundiGradientBasedSample2019,
  title = {Gradient Based Sample Selection for Online Continual Learning},
  author = {Aljundi, Rahaf and Lin, Min and Goujaud, Baptiste and Bengio, Yoshua},
  year = 2019,
  journal = {Advances in neural information processing systems},
  volume = {32}
}

@misc{aljundiGradientBasedSample2019a,
  title = {Gradient Based Sample Selection for Online Continual Learning},
  author = {Aljundi, Rahaf and Lin, Min and Goujaud, Baptiste and Bengio, Yoshua},
  year = 2019,
  month = oct,
  number = {arXiv:1903.08671},
  eprint = {1903.08671},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1903.08671},
  urldate = {2025-09-30},
  abstract = {A continual learning agent learns online with a non-stationary and never-ending stream of data. The key to such learning process is to overcome the catastrophic forgetting of previously seen data, which is a well known problem of neural networks. To prevent forgetting, a replay buffer is usually employed to store the previous data for the purpose of rehearsal. Previous works often depend on task boundary and i.i.d. assumptions to properly select samples for the replay buffer. In this work, we formulate sample selection as a constraint reduction problem based on the constrained optimization view of continual learning. The goal is to select a fixed subset of constraints that best approximate the feasible region defined by the original constraints. We show that it is equivalent to maximizing the diversity of samples in the replay buffer with parameters gradient as the feature. We further develop a greedy alternative that is cheap and efficient. The advantage of the proposed method is demonstrated by comparing to other alternatives under the continual learning setting. Further comparisons are made against state of the art methods that rely on task boundaries which show comparable or even better results for our method.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/7PWVW79W/Aljundi et al. - 2019 - Gradient based sample selection for online continual learning.pdf;/Users/giulialanzillotta/Zotero/storage/2464UPCX/1903.html}
}

@inproceedings{aljundiMemoryAwareSynapses2018,
  title = {Memory {{Aware Synapses}}: {{Learning What}} (Not) to {{Forget}}},
  booktitle = {Proceedings of the {{European Conference}} on {{Computer Vision}} ({{ECCV}})},
  author = {Aljundi, Rahaf and Lin, Min and Goujaud, Baptiste and Bengio, Yoshua},
  year = 2018,
  pages = {139--154},
  publisher = {Springer}
}

@article{aljundiOnlineContinualLearning2019,
  title = {Online Continual Learning with Maximal Interfered Retrieval},
  author = {Aljundi, Rahaf and Belilovsky, Eugene and Tuytelaars, Tinne and Charlin, Laurent and Caccia, Massimo and Lin, Min and {Page-Caccia}, Lucas},
  year = 2019,
  journal = {Advances in neural information processing systems},
  volume = {32}
}

@misc{aljundiOnlineContinualLearning2019a,
  title = {Online {{Continual Learning}} with {{Maximally Interfered Retrieval}}},
  author = {Aljundi, Rahaf and Caccia, Lucas and Belilovsky, Eugene and Caccia, Massimo and Lin, Min and Charlin, Laurent and Tuytelaars, Tinne},
  year = 2019,
  month = oct,
  number = {arXiv:1908.04742},
  eprint = {1908.04742},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1908.04742},
  urldate = {2025-09-30},
  abstract = {Continual learning, the setting where a learning agent is faced with a never ending stream of data, continues to be a great challenge for modern machine learning systems. In particular the online or "single-pass through the data" setting has gained attention recently as a natural setting that is difficult to tackle. Methods based on replay, either generative or from a stored memory, have been shown to be effective approaches for continual learning, matching or exceeding the state of the art in a number of standard benchmarks. These approaches typically rely on randomly selecting samples from the replay memory or from a generative model, which is suboptimal. In this work, we consider a controlled sampling of memories for replay. We retrieve the samples which are most interfered, i.e. whose prediction will be most negatively impacted by the foreseen parameters update. We show a formulation for this sampling criterion in both the generative replay and the experience replay setting, producing consistent gains in performance and greatly reduced forgetting. We release an implementation of our method at https://github.com/optimass/Maximally\_Interfered\_Retrieval.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/F7PRQD9S/Aljundi et al. - 2019 - Online Continual Learning with Maximally Interfered Retrieval.pdf;/Users/giulialanzillotta/Zotero/storage/RZGVEEED/1908.html}
}

@inproceedings{aljundiTaskfreeContinualLearning2019,
  title = {Task-Free Continual Learning},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF}} Conference on Computer Vision and Pattern Recognition},
  author = {Aljundi, Rahaf and Kelchtermans, Klaas and Tuytelaars, Tinne},
  year = 2019,
  pages = {11254--11263}
}

@misc{ammarNECONEuralCollapse2024,
  title = {{{NECO}}: {{NEural Collapse Based Out-of-distribution}} Detection},
  shorttitle = {{{NECO}}},
  author = {Ammar, Mou{\"i}n Ben and Belkhir, Nacim and Popescu, Sebastian and Manzanera, Antoine and Franchi, Gianni},
  year = 2024,
  month = feb,
  number = {arXiv:2310.06823},
  eprint = {2310.06823},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2310.06823},
  urldate = {2025-09-16},
  abstract = {Detecting out-of-distribution (OOD) data is a critical challenge in machine learning due to model overconfidence, often without awareness of their epistemological limits. We hypothesize that ``neural collapse'', a phenomenon affecting in-distribution data for models trained beyond loss convergence, also influences OOD data. To benefit from this interplay, we introduce NECO, a novel post-hoc method for OOD detection, which leverages the geometric properties of ``neural collapse'' and of principal component spaces to identify OOD data. Our extensive experiments demonstrate that NECO achieves state-of-the-art results on both small and large-scale OOD detection tasks while exhibiting strong generalization capabilities across different network architectures. Furthermore, we provide a theoretical explanation for the effectiveness of our method in OOD detection. Code is available at https://gitlab.com/drti/neco},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/YQ9AUHHE/Ammar et al. - 2024 - NECO NEural Collapse Based Out-of-distribution detection.pdf;/Users/giulialanzillotta/Zotero/storage/NUJHMDYI/2310.html}
}

@inproceedings{ammarOnlineMultitaskLearning2014,
  title = {Online Multi-Task Learning for Policy Gradient Methods},
  booktitle = {International Conference on Machine Learning},
  author = {Ammar, Haitham Bou and Eaton, Eric and Ruvolo, Paul and Taylor, Matthew},
  year = 2014,
  pages = {1206--1214},
  publisher = {PMLR}
}

@misc{ashWarmStartingNeuralNetwork2020,
  title = {On {{Warm-Starting Neural Network Training}}},
  author = {Ash, Jordan T. and Adams, Ryan P.},
  year = 2020,
  month = dec,
  number = {arXiv:1910.08475},
  eprint = {1910.08475},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1910.08475},
  urldate = {2025-05-02},
  abstract = {In many real-world deployments of machine learning systems, data arrive piecemeal. These learning scenarios may be passive, where data arrive incrementally due to structural properties of the problem (e.g., daily financial data) or active, where samples are selected according to a measure of their quality (e.g., experimental design). In both of these cases, we are building a sequence of models that incorporate an increasing amount of data. We would like each of these models in the sequence to be performant and take advantage of all the data that are available to that point. Conventional intuition suggests that when solving a sequence of related optimization problems of this form, it should be possible to initialize using the solution of the previous iterate -- to "warm start" the optimization rather than initialize from scratch -- and see reductions in wall-clock time. However, in practice this warm-starting seems to yield poorer generalization performance than models that have fresh random initializations, even though the final training losses are similar. While it appears that some hyperparameter settings allow a practitioner to close this generalization gap, they seem to only do so in regimes that damage the wall-clock gains of the warm start. Nevertheless, it is highly desirable to be able to warm-start neural network training, as it would dramatically reduce the resource usage associated with the construction of performant deep learning systems. In this work, we take a closer look at this empirical phenomenon and try to understand when and how it occurs. We also provide a surprisingly simple trick that overcomes this pathology in several important situations, and present experiments that elucidate some of its properties.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/KZDQIM4C/Ash and Adams - 2020 - On Warm-Starting Neural Network Training.pdf;/Users/giulialanzillotta/Zotero/storage/WFLMLIB4/1910.html}
}

@inproceedings{azarMinimaxRegretBounds2017,
  title = {Minimax Regret Bounds for Reinforcement Learning},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Azar, Mohammad Gheshlaghi and Osband, Ian and Munos, R{\'e}mi},
  year = 2017,
  pages = {263--272},
  publisher = {PMLR}
}

@misc{baiLinearizationQuadraticHigherOrder2020,
  title = {Beyond {{Linearization}}: {{On Quadratic}} and {{Higher-Order Approximation}} of {{Wide Neural Networks}}},
  shorttitle = {Beyond {{Linearization}}},
  author = {Bai, Yu and Lee, Jason D.},
  year = 2020,
  month = feb,
  number = {arXiv:1910.01619},
  eprint = {1910.01619},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1910.01619},
  urldate = {2025-07-28},
  abstract = {Recent theoretical work has established connections between over-parametrized neural networks and linearized models governed by he Neural Tangent Kernels (NTKs). NTK theory leads to concrete convergence and generalization results, yet the empirical performance of neural networks are observed to exceed their linearized models, suggesting insufficiency of this theory. Towards closing this gap, we investigate the training of over-parametrized neural networks that are beyond the NTK regime yet still governed by the Taylor expansion of the network. We bring forward the idea of {\textbackslash}emph\{randomizing\} the neural networks, which allows them to escape their NTK and couple with quadratic models. We show that the optimization landscape of randomized two-layer networks are nice and amenable to escaping-saddle algorithms. We prove concrete generalization and expressivity results on these randomized networks, which lead to sample complexity bounds (of learning certain simple functions) that match the NTK and can in addition be better by a dimension factor when mild distributional assumptions are present. We demonstrate that our randomization technique can be generalized systematically beyond the quadratic case, by using it to find networks that are coupled with higher-order terms in their Taylor series.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/THCC9SND/Bai and Lee - 2020 - Beyond Linearization On Quadratic and Higher-Order Approximation of Wide Neural Networks.pdf;/Users/giulialanzillotta/Zotero/storage/A95Z6FF6/1910.html}
}

@misc{baiTaylorizedTrainingBetter2020,
  title = {Taylorized {{Training}}: {{Towards Better Approximation}} of {{Neural Network Training}} at {{Finite Width}}},
  shorttitle = {Taylorized {{Training}}},
  author = {Bai, Yu and Krause, Ben and Wang, Huan and Xiong, Caiming and Socher, Richard},
  year = 2020,
  month = feb,
  number = {arXiv:2002.04010},
  eprint = {2002.04010},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2002.04010},
  urldate = {2025-07-28},
  abstract = {We propose {\textbackslash}emph\{Taylorized training\} as an initiative towards better understanding neural network training at finite width. Taylorized training involves training the \$k\$-th order Taylor expansion of the neural network at initialization, and is a principled extension of linearized training---a recently proposed theory for understanding the success of deep learning. We experiment with Taylorized training on modern neural network architectures, and show that Taylorized training (1) agrees with full neural network training increasingly better as we increase \$k\$, and (2) can significantly close the performance gap between linearized and full training. Compared with linearized training, higher-order training works in more realistic settings such as standard parameterization and large (initial) learning rate. We complement our experiments with theoretical results showing that the approximation error of \$k\$-th order Taylorized models decay exponentially over \$k\$ in wide neural networks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/6FI39UWZ/Bai et al. - 2020 - Taylorized Training Towards Better Approximation of Neural Network Training at Finite Width.pdf;/Users/giulialanzillotta/Zotero/storage/GY9KABN4/2002.html}
}

@misc{baLayerNormalization2016,
  title = {Layer {{Normalization}}},
  author = {Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E.},
  year = 2016
}

@inproceedings{belouadahIL2MClassIncremental2019,
  title = {{{IL2M}}: {{Class Incremental Learning With Dual Memory}}},
  shorttitle = {{{IL2M}}},
  booktitle = {2019 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Belouadah, Eden and Popescu, Adrian},
  year = 2019,
  month = oct,
  pages = {583--592},
  publisher = {IEEE},
  address = {Seoul, Korea (South)},
  doi = {10.1109/ICCV.2019.00067},
  urldate = {2025-09-30},
  abstract = {This paper presents a class incremental learning (IL) method which exploits fine tuning and a dual memory to reduce the negative effect of catastrophic forgetting in image recognition. First, we simplify the current fine tuning based approaches which use a combination of classification and distillation losses to compensate for the limited availability of past data. We find that the distillation term actually hurts performance when a memory is allowed. Then, we modify the usual class IL memory component. Similar to existing works, a first memory stores exemplar images of past classes. A second memory is introduced here to store past class statistics obtained when they were initially learned. The intuition here is that classes are best modeled when all their data are available and that their initial statistics are useful across different incremental states. A prediction bias towards newly learned classes appears during inference because the dataset is imbalanced in their favor. The challenge is to make predictions of new and past classes more comparable. To do this, scores of past classes are rectified by leveraging contents from both memories. The method has negligible added cost, both in terms of memory and of inference complexity. Experiments with three large public datasets show that the proposed approach is more effective than a range of competitive state-of-the-art methods.},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  isbn = {978-1-7281-4803-8},
  langid = {english},
  file = {/Users/giulialanzillotta/Zotero/storage/7VCJHPC6/Belouadah and Popescu - 2019 - IL2M Class Incremental Learning With Dual Memory.pdf}
}

@misc{bengioConditionalComputationNeural2016,
  title = {Conditional {{Computation}} in {{Neural Networks}} for Faster Models},
  author = {Bengio, Emmanuel and Bacon, Pierre-Luc and Pineau, Joelle and Precup, Doina},
  year = 2016,
  month = jan,
  number = {arXiv:1511.06297},
  eprint = {1511.06297},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1511.06297},
  urldate = {2025-10-03},
  abstract = {Deep learning has become the state-of-art tool in many applications, but the evaluation and training of deep models can be time-consuming and computationally expensive. The conditional computation approach has been proposed to tackle this problem (Bengio et al., 2013; Davis \& Arel, 2013). It operates by selectively activating only parts of the network at a time. In this paper, we use reinforcement learning as a tool to optimize conditional computation policies. More specifically, we cast the problem of learning activation-dependent policies for dropping out blocks of units as a reinforcement learning problem. We propose a learning scheme motivated by computation speed, capturing the idea of wanting to have parsimonious activations while maintaining prediction accuracy. We apply a policy gradient algorithm for learning policies that optimize this loss function and propose a regularization mechanism that encourages diversification of the dropout policy. We present encouraging empirical results showing that this approach improves the speed of computation without impacting the quality of the approximation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/7UIRY56J/Bengio et al. - 2016 - Conditional Computation in Neural Networks for faster models.pdf;/Users/giulialanzillotta/Zotero/storage/JW5LE2QI/1511.html}
}

@misc{bengioConditionalComputationNeural2016a,
  title = {Conditional {{Computation}} in {{Neural Networks}} for Faster Models},
  author = {Bengio, Emmanuel and Bacon, Pierre-Luc and Pineau, Joelle and Precup, Doina},
  year = 2016,
  month = jan,
  number = {arXiv:1511.06297},
  eprint = {1511.06297},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1511.06297},
  urldate = {2025-10-03},
  abstract = {Deep learning has become the state-of-art tool in many applications, but the evaluation and training of deep models can be time-consuming and computationally expensive. The conditional computation approach has been proposed to tackle this problem (Bengio et al., 2013; Davis \& Arel, 2013). It operates by selectively activating only parts of the network at a time. In this paper, we use reinforcement learning as a tool to optimize conditional computation policies. More specifically, we cast the problem of learning activation-dependent policies for dropping out blocks of units as a reinforcement learning problem. We propose a learning scheme motivated by computation speed, capturing the idea of wanting to have parsimonious activations while maintaining prediction accuracy. We apply a policy gradient algorithm for learning policies that optimize this loss function and propose a regularization mechanism that encourages diversification of the dropout policy. We present encouraging empirical results showing that this approach improves the speed of computation without impacting the quality of the approximation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/74UNT2BM/Bengio et al. - 2016 - Conditional Computation in Neural Networks for faster models.pdf;/Users/giulialanzillotta/Zotero/storage/X6TQ2RHY/1511.html}
}

@misc{bengioEstimatingPropagatingGradients2013,
  title = {Estimating or {{Propagating Gradients Through Stochastic Neurons}} for {{Conditional Computation}}},
  author = {Bengio, Yoshua and L{\'e}onard, Nicholas and Courville, Aaron},
  year = 2013,
  month = aug,
  number = {arXiv:1308.3432},
  eprint = {1308.3432},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1308.3432},
  urldate = {2025-10-03},
  abstract = {Stochastic neurons and hard non-linearities can be useful for a number of reasons in deep learning models, but in many cases they pose a challenging problem: how to estimate the gradient of a loss function with respect to the input of such stochastic or non-smooth neurons? I.e., can we "back-propagate" through these stochastic neurons? We examine this question, existing approaches, and compare four families of solutions, applicable in different settings. One of them is the minimum variance unbiased gradient estimator for stochatic binary neurons (a special case of the REINFORCE algorithm). A second approach, introduced here, decomposes the operation of a binary stochastic neuron into a stochastic binary part and a smooth differentiable part, which approximates the expected effect of the pure stochatic binary neuron to first order. A third approach involves the injection of additive or multiplicative noise in a computational graph that is otherwise differentiable. A fourth approach heuristically copies the gradient with respect to the stochastic output directly as an estimator of the gradient with respect to the sigmoid argument (we call this the straight-through estimator). To explore a context where these estimators are useful, we consider a small-scale version of \{{\textbackslash}em conditional computation\}, where sparse stochastic units form a distributed representation of gaters that can turn off in combinatorially many ways large chunks of the computation performed in the rest of the neural network. In this case, it is important that the gating units produce an actual 0 most of the time. The resulting sparsity can be potentially be exploited to greatly reduce the computational cost of large deep networks for which conditional computation would be useful.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning}
}

@misc{bengioEstimatingPropagatingGradients2013a,
  title = {Estimating or {{Propagating Gradients Through Stochastic Neurons}} for {{Conditional Computation}}},
  author = {Bengio, Yoshua and L{\'e}onard, Nicholas and Courville, Aaron},
  year = 2013,
  month = aug,
  number = {arXiv:1308.3432},
  eprint = {1308.3432},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1308.3432},
  urldate = {2025-10-03},
  abstract = {Stochastic neurons and hard non-linearities can be useful for a number of reasons in deep learning models, but in many cases they pose a challenging problem: how to estimate the gradient of a loss function with respect to the input of such stochastic or non-smooth neurons? I.e., can we "back-propagate" through these stochastic neurons? We examine this question, existing approaches, and compare four families of solutions, applicable in different settings. One of them is the minimum variance unbiased gradient estimator for stochatic binary neurons (a special case of the REINFORCE algorithm). A second approach, introduced here, decomposes the operation of a binary stochastic neuron into a stochastic binary part and a smooth differentiable part, which approximates the expected effect of the pure stochatic binary neuron to first order. A third approach involves the injection of additive or multiplicative noise in a computational graph that is otherwise differentiable. A fourth approach heuristically copies the gradient with respect to the stochastic output directly as an estimator of the gradient with respect to the sigmoid argument (we call this the straight-through estimator). To explore a context where these estimators are useful, we consider a small-scale version of \{{\textbackslash}em conditional computation\}, where sparse stochastic units form a distributed representation of gaters that can turn off in combinatorially many ways large chunks of the computation performed in the rest of the neural network. In this case, it is important that the gating units produce an actual 0 most of the time. The resulting sparsity can be potentially be exploited to greatly reduce the computational cost of large deep networks for which conditional computation would be useful.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning}
}

@misc{bengioEstimatingPropagatingGradients2013b,
  title = {Estimating or {{Propagating Gradients Through Stochastic Neurons}} for {{Conditional Computation}}},
  author = {Bengio, Yoshua and L{\'e}onard, Nicholas and Courville, Aaron},
  year = 2013,
  month = aug,
  number = {arXiv:1308.3432},
  eprint = {1308.3432},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1308.3432},
  urldate = {2025-10-03},
  abstract = {Stochastic neurons and hard non-linearities can be useful for a number of reasons in deep learning models, but in many cases they pose a challenging problem: how to estimate the gradient of a loss function with respect to the input of such stochastic or non-smooth neurons? I.e., can we "back-propagate" through these stochastic neurons? We examine this question, existing approaches, and compare four families of solutions, applicable in different settings. One of them is the minimum variance unbiased gradient estimator for stochatic binary neurons (a special case of the REINFORCE algorithm). A second approach, introduced here, decomposes the operation of a binary stochastic neuron into a stochastic binary part and a smooth differentiable part, which approximates the expected effect of the pure stochatic binary neuron to first order. A third approach involves the injection of additive or multiplicative noise in a computational graph that is otherwise differentiable. A fourth approach heuristically copies the gradient with respect to the stochastic output directly as an estimator of the gradient with respect to the sigmoid argument (we call this the straight-through estimator). To explore a context where these estimators are useful, we consider a small-scale version of \{{\textbackslash}em conditional computation\}, where sparse stochastic units form a distributed representation of gaters that can turn off in combinatorially many ways large chunks of the computation performed in the rest of the neural network. In this case, it is important that the gating units produce an actual 0 most of the time. The resulting sparsity can be potentially be exploited to greatly reduce the computational cost of large deep networks for which conditional computation would be useful.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning}
}

@misc{benzingUnifyingRegularisationMethods2021,
  title = {Unifying {{Regularisation Methods}} for {{Continual Learning}}},
  author = {Benzing, Frederik},
  year = 2021,
  month = feb,
  number = {arXiv:2006.06357},
  eprint = {2006.06357},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2006.06357},
  urldate = {2025-06-30},
  abstract = {Continual Learning addresses the challenge of learning a number of different tasks sequentially. The goal of maintaining knowledge of earlier tasks without re-accessing them starkly conflicts with standard SGD training for artificial neural networks. An influential method to tackle this problem without storing old data are so-called regularisation approaches. They measure the importance of each parameter for solving a given task and subsequently protect important parameters from large changes. In the literature, three ways to measure parameter importance have been put forward and they have inspired a large body of follow-up work. Here, we present strong theoretical and empirical evidence that these three methods, Elastic Weight Consolidation (EWC), Synaptic Intelligence (SI) and Memory Aware Synapses (MAS), are surprisingly similar and are all linked to the same theoretical quantity. Concretely, we show that, despite stemming from very different motivations, both SI and MAS approximate the square root of the Fisher Information, with the Fisher being the theoretically justified basis of EWC. Moreover, we show that for SI the relation to the Fisher -- and in fact its performance -- is due to a previously unknown bias. On top of uncovering unknown similarities and unifying regularisation approaches, we also demonstrate that our insights enable practical performance improvements for large batch training.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/DR943EWA/Benzing - 2021 - Unifying Regularisation Methods for Continual Learning.pdf;/Users/giulialanzillotta/Zotero/storage/AKMQJ4SV/2006.html}
}

@article{besbesNonstationaryStochasticOptimization2015,
  title = {Non-Stationary Stochastic Optimization},
  author = {Besbes, Omar and Gur, Yonatan and Zeevi, Assaf},
  year = 2015,
  journal = {Operations research},
  volume = {63},
  number = {5},
  pages = {1227--1244},
  publisher = {INFORMS}
}

@misc{bordelonSelfConsistentDynamicalField2022,
  title = {Self-{{Consistent Dynamical Field Theory}} of {{Kernel Evolution}} in {{Wide Neural Networks}}},
  author = {Bordelon, Blake and Pehlevan, Cengiz},
  year = 2022,
  month = oct,
  number = {arXiv:2205.09653},
  eprint = {2205.09653},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2205.09653},
  urldate = {2025-07-28},
  abstract = {We analyze feature learning in infinite-width neural networks trained with gradient flow through a self-consistent dynamical field theory. We construct a collection of deterministic dynamical order parameters which are inner-product kernels for hidden unit activations and gradients in each layer at pairs of time points, providing a reduced description of network activity through training. These kernel order parameters collectively define the hidden layer activation distribution, the evolution of the neural tangent kernel, and consequently output predictions. We show that the field theory derivation recovers the recursive stochastic process of infinite-width feature learning networks obtained from Yang and Hu (2021) with Tensor Programs . For deep linear networks, these kernels satisfy a set of algebraic matrix equations. For nonlinear networks, we provide an alternating sampling procedure to self-consistently solve for the kernel order parameters. We provide comparisons of the self-consistent solution to various approximation schemes including the static NTK approximation, gradient independence assumption, and leading order perturbation theory, showing that each of these approximations can break down in regimes where general self-consistent solutions still provide an accurate description. Lastly, we provide experiments in more realistic settings which demonstrate that the loss and kernel dynamics of CNNs at fixed feature learning strength is preserved across different widths on a CIFAR classification task.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Condensed Matter - Disordered Systems and Neural Networks,Statistics - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/BBJUMHZ3/Bordelon and Pehlevan - 2022 - Self-Consistent Dynamical Field Theory of Kernel Evolution in Wide Neural Networks.pdf;/Users/giulialanzillotta/Zotero/storage/MTK8Y4T3/2205.html}
}

@article{bornscheinSequentialLearningNeural2022,
  title = {Sequential {{Learning Of Neural Networks}} for {{Prequential MDL}}},
  author = {Bornschein, Jorg and Li, Yazhe and Hutter, Marcus},
  year = 2022,
  journal = {arXiv preprint arXiv:2210.07931},
  eprint = {2210.07931},
  archiveprefix = {arXiv}
}

@article{borsosCoresetsBilevelOptimization2020,
  title = {Coresets via Bilevel Optimization for Continual Learning and Streaming},
  author = {Borsos, Zal{\'a}n and Mutny, Mojmir and Krause, Andreas},
  year = 2020,
  journal = {Advances in neural information processing systems},
  volume = {33},
  pages = {14879--14890}
}

@misc{borsosCoresetsBilevelOptimization2020a,
  title = {Coresets via {{Bilevel Optimization}} for {{Continual Learning}} and {{Streaming}}},
  author = {Borsos, Zal{\'a}n and Mutn{\'y}, Mojm{\'i}r and Krause, Andreas},
  year = 2020,
  month = oct,
  number = {arXiv:2006.03875},
  eprint = {2006.03875},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2006.03875},
  urldate = {2025-09-30},
  abstract = {Coresets are small data summaries that are sufficient for model training. They can be maintained online, enabling efficient handling of large data streams under resource constraints. However, existing constructions are limited to simple models such as k-means and logistic regression. In this work, we propose a novel coreset construction via cardinality-constrained bilevel optimization. We show how our framework can efficiently generate coresets for deep neural networks, and demonstrate its empirical benefits in continual learning and in streaming settings.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/NYVX84SQ/Borsos et al. - 2020 - Coresets via Bilevel Optimization for Continual Learning and Streaming.pdf;/Users/giulialanzillotta/Zotero/storage/MFPTJNA2/2006.html}
}

@article{boschiniClassIncrementalContinualLearning2023,
  title = {Class-{{Incremental Continual Learning}} into the {{eXtended DER-verse}}},
  author = {Boschini, Matteo and Bonicelli, Lorenzo and Buzzega, Pietro and Porrello, Angelo and Calderara, Simone},
  year = 2023,
  month = may,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {45},
  number = {5},
  eprint = {2201.00766},
  primaryclass = {cs},
  pages = {5497--5512},
  issn = {0162-8828, 2160-9292, 1939-3539},
  doi = {10.1109/TPAMI.2022.3206549},
  urldate = {2025-09-30},
  abstract = {The staple of human intelligence is the capability of acquiring knowledge in a continuous fashion. In stark contrast, Deep Networks forget catastrophically and, for this reason, the sub-field of Class-Incremental Continual Learning fosters methods that learn a sequence of tasks incrementally, blending sequentially-gained knowledge into a comprehensive prediction. This work aims at assessing and overcoming the pitfalls of our previous proposal Dark Experience Replay (DER), a simple and effective approach that combines rehearsal and Knowledge Distillation. Inspired by the way our minds constantly rewrite past recollections and set expectations for the future, we endow our model with the abilities to i) revise its replay memory to welcome novel information regarding past data ii) pave the way for learning yet unseen classes. We show that the application of these strategies leads to remarkable improvements; indeed, the resulting method - termed eXtended-DER (X-DER) - outperforms the state of the art on both standard benchmarks (such as CIFAR-100 and miniImagenet) and a novel one here introduced. To gain a better understanding, we further provide extensive ablation studies that corroborate and extend the findings of our previous research (e.g. the value of Knowledge Distillation and flatter minima in continual learning setups).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/WH362P5Z/Boschini et al. - 2023 - Class-Incremental Continual Learning into the eXtended DER-verse.pdf;/Users/giulialanzillotta/Zotero/storage/LLKIT5KR/2201.html}
}

@inproceedings{bossardFood101MiningDiscriminative2014,
  title = {Food-101 -- {{Mining Discriminative Components}} with {{Random Forests}}},
  booktitle = {European {{Conference}} on {{Computer Vision}}},
  author = {Bossard, Lukas and Guillaumin, Matthieu and Van Gool, Luc},
  year = 2014
}

@inproceedings{bossardThreeScenariosContinual2019,
  title = {Three Scenarios for Continual Learning},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Bossard, Lukas and Guillaumin, Matthieu and Van Gool, Luc and Parkhi, Omkar M. and Vedaldi, Andrea and Zisserman, Andrew and Jawahar, C. V. and Krizhevsky, Alex and Hinton, Geoffrey and Lin, Long-Ji and Zhang, Shangtong and Sutton, Richard S. and Schulman, John and Moritz, Philipp and Levine, Sergey and Jordan, Michael and Abbeel, Pieter and Robins, Anthony and Buzzega, Pietro and Boschini, Matteo and Porrello, Angelo and Abati, Davide and CALDERARA, {\relax SIMONE} and van de Ven, Gido M. and Tolias, Andreas S.},
  editor = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M. F. and Lin, H.},
  year = 2019,
  volume = {33},
  pages = {15920--15930},
  publisher = {Curran Associates, Inc.},
  annotation = {Backup Publisher: University of Toronto}
}

@book{boydConvexOptimization2004,
  title = {Convex Optimization},
  author = {Boyd, Stephen and Vandenberghe, Lieven},
  year = 2004,
  publisher = {Cambridge university press}
}

@misc{buiStreamingSparseGaussian2017,
  title = {Streaming {{Sparse Gaussian Process Approximations}}},
  author = {Bui, Thang D. and Nguyen, Cuong V. and Turner, Richard E.},
  year = 2017,
  month = nov,
  number = {arXiv:1705.07131},
  eprint = {1705.07131},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1705.07131},
  urldate = {2025-06-21},
  abstract = {Sparse pseudo-point approximations for Gaussian process (GP) models provide a suite of methods that support deployment of GPs in the large data regime and enable analytic intractabilities to be sidestepped. However, the field lacks a principled method to handle streaming data in which both the posterior distribution over function values and the hyperparameter estimates are updated in an online fashion. The small number of existing approaches either use suboptimal hand-crafted heuristics for hyperparameter learning, or suffer from catastrophic forgetting or slow updating when new data arrive. This paper develops a new principled framework for deploying Gaussian process probabilistic models in the streaming setting, providing methods for learning hyperparameters and optimising pseudo-input locations. The proposed framework is assessed using synthetic and real-world datasets.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/5CS7R7CC/Bui et al. - 2017 - Streaming Sparse Gaussian Process Approximations.pdf;/Users/giulialanzillotta/Zotero/storage/DRBLIW8I/1705.html}
}

@misc{buiStreamingSparseGaussian2017a,
  title = {Streaming {{Sparse Gaussian Process Approximations}}},
  author = {Bui, Thang D. and Nguyen, Cuong V. and Turner, Richard E.},
  year = 2017,
  month = nov,
  number = {arXiv:1705.07131},
  eprint = {1705.07131},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1705.07131},
  urldate = {2025-10-14},
  abstract = {Sparse pseudo-point approximations for Gaussian process (GP) models provide a suite of methods that support deployment of GPs in the large data regime and enable analytic intractabilities to be sidestepped. However, the field lacks a principled method to handle streaming data in which both the posterior distribution over function values and the hyperparameter estimates are updated in an online fashion. The small number of existing approaches either use suboptimal hand-crafted heuristics for hyperparameter learning, or suffer from catastrophic forgetting or slow updating when new data arrive. This paper develops a new principled framework for deploying Gaussian process probabilistic models in the streaming setting, providing methods for learning hyperparameters and optimising pseudo-input locations. The proposed framework is assessed using synthetic and real-world datasets.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/4CHXVP9W/Bui et al. - 2017 - Streaming Sparse Gaussian Process Approximations.pdf;/Users/giulialanzillotta/Zotero/storage/TQUZ7N2R/1705.html}
}

@article{buzzegaDarkExperienceGeneral,
  title = {Dark {{Experience}} for {{General Continual Learning}}: A {{Strong}}, {{Simple Baseline}}},
  author = {Buzzega, Pietro and Boschini, Matteo and Porrello, Angelo and Abati, Davide and Calderara, Simone},
  abstract = {Continual Learning has inspired a plethora of approaches and evaluation settings; however, the majority of them overlooks the properties of a practical scenario, where the data stream cannot be shaped as a sequence of tasks and offline training is not viable. We work towards General Continual Learning (GCL), where task boundaries blur and the domain and class distributions shift either gradually or suddenly. We address it through mixing rehearsal with knowledge distillation and regularization; our simple baseline, Dark Experience Replay, matches the network's logits sampled throughout the optimization trajectory, thus promoting consistency with its past. By conducting an extensive analysis on both standard benchmarks and a novel GCL evaluation setting (MNIST-360), we show that such a seemingly simple baseline outperforms consolidated approaches and leverages limited resources. We further explore the generalization capabilities of our objective, showing its regularization being beneficial beyond mere performance. Code is available at https://github.com/aimagelab/mammoth.},
  langid = {english},
  file = {/Users/giulialanzillotta/Zotero/storage/CH7U8JBZ/Buzzega et al. - Dark Experience for General Continual Learning a Strong, Simple Baseline.pdf}
}

@inproceedings{buzzegaDarkExperienceGeneral2020,
  title = {Dark {{Experience}} for {{General Continual Learning}}: A {{Strong}}, {{Simple Baseline}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Buzzega, Pietro and Boschini, Matteo and Porrello, Angelo and Abati, Davide and CALDERARA, {\relax SIMONE}},
  editor = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M. F. and Lin, H.},
  year = 2020,
  volume = {33},
  pages = {15920--15930},
  publisher = {Curran Associates, Inc.}
}

@misc{buzzegaRethinkingExperienceReplay2020,
  title = {Rethinking {{Experience Replay}}: A {{Bag}} of {{Tricks}} for {{Continual Learning}}},
  shorttitle = {Rethinking {{Experience Replay}}},
  author = {Buzzega, Pietro and Boschini, Matteo and Porrello, Angelo and Calderara, Simone},
  year = 2020,
  month = oct,
  number = {arXiv:2010.05595},
  eprint = {2010.05595},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2010.05595},
  urldate = {2025-09-30},
  abstract = {In Continual Learning, a Neural Network is trained on a stream of data whose distribution shifts over time. Under these assumptions, it is especially challenging to improve on classes appearing later in the stream while remaining accurate on previous ones. This is due to the infamous problem of catastrophic forgetting, which causes a quick performance degradation when the classifier focuses on learning new categories. Recent literature proposed various approaches to tackle this issue, often resorting to very sophisticated techniques. In this work, we show that naive rehearsal can be patched to achieve similar performance. We point out some shortcomings that restrain Experience Replay (ER) and propose five tricks to mitigate them. Experiments show that ER, thus enhanced, displays an accuracy gain of 51.2 and 26.9 percentage points on the CIFAR-10 and CIFAR-100 datasets respectively (memory buffer size 1000). As a result, it surpasses current state-of-the-art rehearsal-based methods.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/UAMKM4MC/Buzzega et al. - 2020 - Rethinking Experience Replay a Bag of Tricks for Continual Learning.pdf;/Users/giulialanzillotta/Zotero/storage/QW52KXJT/2010.html}
}

@inproceedings{buzzegaRethinkingExperienceReplay2021,
  title = {Rethinking Experience Replay: A Bag of Tricks for Continual Learning},
  booktitle = {2020 25th {{International Conference}} on {{Pattern Recognition}} ({{ICPR}})},
  author = {Buzzega, Pietro and Boschini, Matteo and Porrello, Angelo and Calderara, Simone},
  year = 2021,
  pages = {2180--2187},
  publisher = {IEEE}
}

@article{cacciaNewInsightsReducing2021,
  title = {New Insights on Reducing Abrupt Representation Change in Online Continual Learning},
  author = {Caccia, Lucas and Aljundi, Rahaf and Asadi, Nader and Tuytelaars, Tinne and Pineau, Joelle and Belilovsky, Eugene},
  year = 2021,
  journal = {arXiv preprint arXiv:2104.05025},
  eprint = {2104.05025},
  archiveprefix = {arXiv}
}

@misc{caiImplicitBiasGradient2025,
  title = {Implicit {{Bias}} of {{Gradient Descent}} for {{Non-Homogeneous Deep Networks}}},
  author = {Cai, Yuhang and Zhou, Kangjie and Wu, Jingfeng and Mei, Song and Lindsey, Michael and Bartlett, Peter L.},
  year = 2025,
  month = jul,
  number = {arXiv:2502.16075},
  eprint = {2502.16075},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2502.16075},
  urldate = {2025-09-16},
  abstract = {We establish the asymptotic implicit bias of gradient descent (GD) for generic non-homogeneous deep networks under exponential loss. Specifically, we characterize three key properties of GD iterates starting from a sufficiently small empirical risk, where the threshold is determined by a measure of the network's non-homogeneity. First, we show that a normalized margin induced by the GD iterates increases nearly monotonically. Second, we prove that while the norm of the GD iterates diverges to infinity, the iterates themselves converge in direction. Finally, we establish that this directional limit satisfies the Karush-Kuhn-Tucker (KKT) conditions of a margin maximization problem. Prior works on implicit bias have focused exclusively on homogeneous networks; in contrast, our results apply to a broad class of non-homogeneous networks satisfying a mild near-homogeneity condition. In particular, our results apply to networks with residual connections and non-homogeneous activation functions, thereby resolving an open problem posed by Ji and Telgarsky (2020).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/RC8Y5ATZ/Cai et al. - 2025 - Implicit Bias of Gradient Descent for Non-Homogeneous Deep Networks.pdf;/Users/giulialanzillotta/Zotero/storage/U9FQLTNA/2502.html}
}

@inproceedings{caiOnlineContinualLearning2021,
  title = {Online Continual Learning with Natural Distribution Shifts: {{An}} Empirical Study with Visual Data},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF}} International Conference on Computer Vision},
  author = {Cai, Zhipeng and Sener, Ozan and Koltun, Vladlen},
  year = 2021,
  pages = {8281--8290}
}

@article{caruanaMultitaskLearning1997,
  title = {Multitask Learning},
  author = {Caruana, Rich},
  year = 1997,
  journal = {Machine learning},
  volume = {28},
  pages = {41--75},
  publisher = {Springer}
}

@book{cesa-bianchiPredictionLearningGames2006,
  title = {Prediction, Learning, and Games},
  author = {{Cesa-Bianchi}, Nicolo and Lugosi, G{\'a}bor},
  year = 2006,
  publisher = {Cambridge university press}
}

@misc{chaCo$^2$LContrastiveContinual2021,
  title = {Co\${\textasciicircum}2\${{L}}: {{Contrastive Continual Learning}}},
  shorttitle = {Co\${\textasciicircum}2\${{L}}},
  author = {Cha, Hyuntak and Lee, Jaeho and Shin, Jinwoo},
  year = 2021,
  month = jun,
  number = {arXiv:2106.14413},
  eprint = {2106.14413},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2106.14413},
  urldate = {2025-10-03},
  abstract = {Recent breakthroughs in self-supervised learning show that such algorithms learn visual representations that can be transferred better to unseen tasks than joint-training methods relying on task-specific supervision. In this paper, we found that the similar holds in the continual learning con-text: contrastively learned representations are more robust against the catastrophic forgetting than jointly trained representations. Based on this novel observation, we propose a rehearsal-based continual learning algorithm that focuses on continually learning and maintaining transferable representations. More specifically, the proposed scheme (1) learns representations using the contrastive learning objective, and (2) preserves learned representations using a self-supervised distillation step. We conduct extensive experimental validations under popular benchmark image classification datasets, where our method sets the new state-of-the-art performance.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/LRFEASM5/Cha et al. - 2021 - Co$^2$L Contrastive Continual Learning.pdf;/Users/giulialanzillotta/Zotero/storage/ZE47BSRV/2106.html}
}

@misc{changMemoryBasedDualGaussian2023,
  title = {Memory-{{Based Dual Gaussian Processes}} for {{Sequential Learning}}},
  author = {Chang, Paul E. and Verma, Prakhar and John, S. T. and Solin, Arno and Khan, Mohammad Emtiyaz},
  year = 2023,
  month = jun,
  number = {arXiv:2306.03566},
  eprint = {2306.03566},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2306.03566},
  urldate = {2025-10-14},
  abstract = {Sequential learning with Gaussian processes (GPs) is challenging when access to past data is limited, for example, in continual and active learning. In such cases, errors can accumulate over time due to inaccuracies in the posterior, hyperparameters, and inducing points, making accurate learning challenging. Here, we present a method to keep all such errors in check using the recently proposed dual sparse variational GP. Our method enables accurate inference for generic likelihoods and improves learning by actively building and updating a memory of past data. We demonstrate its effectiveness in several applications involving Bayesian optimization, active learning, and continual learning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/UKU289X4/Chang et al. - 2023 - Memory-Based Dual Gaussian Processes for Sequential Learning.pdf;/Users/giulialanzillotta/Zotero/storage/9HPWPG3R/2306.html}
}

@misc{changMemoryBasedDualGaussian2023a,
  title = {Memory-{{Based Dual Gaussian Processes}} for {{Sequential Learning}}},
  author = {Chang, Paul E. and Verma, Prakhar and John, S. T. and Solin, Arno and Khan, Mohammad Emtiyaz},
  year = 2023,
  month = jun,
  number = {arXiv:2306.03566},
  eprint = {2306.03566},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2306.03566},
  urldate = {2025-10-14},
  abstract = {Sequential learning with Gaussian processes (GPs) is challenging when access to past data is limited, for example, in continual and active learning. In such cases, errors can accumulate over time due to inaccuracies in the posterior, hyperparameters, and inducing points, making accurate learning challenging. Here, we present a method to keep all such errors in check using the recently proposed dual sparse variational GP. Our method enables accurate inference for generic likelihoods and improves learning by actively building and updating a memory of past data. We demonstrate its effectiveness in several applications involving Bayesian optimization, active learning, and continual learning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/7BDXRVC9/Chang et al. - 2023 - Memory-Based Dual Gaussian Processes for Sequential Learning.pdf;/Users/giulialanzillotta/Zotero/storage/Q4LUVYHH/2306.html}
}

@misc{chaudhryEfficientLifelongLearning2019,
  title = {Efficient {{Lifelong Learning}} with {{A-GEM}}},
  author = {Chaudhry, Arslan and Ranzato, Marc'Aurelio and Rohrbach, Marcus and Elhoseiny, Mohamed},
  year = 2019,
  month = jan,
  number = {arXiv:1812.00420},
  eprint = {1812.00420},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1812.00420},
  urldate = {2025-09-30},
  abstract = {In lifelong learning, the learner is presented with a sequence of tasks, incrementally building a data-driven prior which may be leveraged to speed up learning of a new task. In this work, we investigate the efficiency of current lifelong approaches, in terms of sample complexity, computational and memory cost. Towards this end, we first introduce a new and a more realistic evaluation protocol, whereby learners observe each example only once and hyper-parameter selection is done on a small and disjoint set of tasks, which is not used for the actual learning experience and evaluation. Second, we introduce a new metric measuring how quickly a learner acquires a new skill. Third, we propose an improved version of GEM (Lopez-Paz \& Ranzato, 2017), dubbed Averaged GEM (A-GEM), which enjoys the same or even better performance as GEM, while being almost as computationally and memory efficient as EWC (Kirkpatrick et al., 2016) and other regularization-based methods. Finally, we show that all algorithms including A-GEM can learn even more quickly if they are provided with task descriptors specifying the classification tasks under consideration. Our experiments on several standard lifelong learning benchmarks demonstrate that A-GEM has the best trade-off between accuracy and efficiency.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/H9MCI8U9/Chaudhry et al. - 2019 - Efficient Lifelong Learning with A-GEM.pdf;/Users/giulialanzillotta/Zotero/storage/CUJRP6JM/1812.html}
}

@article{chaudhryTinyEpisodicMemories2019,
  title = {On Tiny Episodic Memories in Continual Learning},
  author = {Chaudhry, Arslan and Rohrbach, Marcus and Elhoseiny, Mohamed and Ajanthan, Thalaiyasingam and Dokania, Puneet K and Torr, Philip HS and Ranzato, Marc'Aurelio},
  year = 2019,
  journal = {arXiv preprint arXiv:1902.10486},
  eprint = {1902.10486},
  archiveprefix = {arXiv}
}

@inproceedings{chaudhryUsingHindsightAnchor2021,
  title = {Using Hindsight to Anchor Past Knowledge in Continual Learning},
  booktitle = {Proceedings of the {{AAAI}} Conference on Artificial Intelligence},
  author = {Chaudhry, Arslan and Gordo, Albert and Dokania, Puneet and Torr, Philip and {Lopez-Paz}, David},
  year = 2021,
  volume = {35},
  pages = {6993--7001}
}

@misc{chaudhryUsingHindsightAnchor2021a,
  title = {Using {{Hindsight}} to {{Anchor Past Knowledge}} in {{Continual Learning}}},
  author = {Chaudhry, Arslan and Gordo, Albert and Dokania, Puneet K. and Torr, Philip and {Lopez-Paz}, David},
  year = 2021,
  month = mar,
  number = {arXiv:2002.08165},
  eprint = {2002.08165},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2002.08165},
  urldate = {2025-09-30},
  abstract = {In continual learning, the learner faces a stream of data whose distribution changes over time. Modern neural networks are known to suffer under this setting, as they quickly forget previously acquired knowledge. To address such catastrophic forgetting, many continual learning methods implement different types of experience replay, re-learning on past data stored in a small buffer known as episodic memory. In this work, we complement experience replay with a new objective that we call anchoring, where the learner uses bilevel optimization to update its knowledge on the current task, while keeping intact the predictions on some anchor points of past tasks. These anchor points are learned using gradient-based optimization to maximize forgetting, which is approximated by fine-tuning the currently trained model on the episodic memory of past tasks. Experiments on several supervised learning benchmarks for continual learning demonstrate that our approach improves the standard experience replay in terms of both accuracy and forgetting metrics and for various sizes of episodic memories.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/NQAAZ4IV/Chaudhry et al. - 2021 - Using Hindsight to Anchor Past Knowledge in Continual Learning.pdf;/Users/giulialanzillotta/Zotero/storage/6SRH62E6/2002.html}
}

@misc{chenLifelongLanguagePretraining2023,
  title = {Lifelong {{Language Pretraining}} with {{Distribution-Specialized Experts}}},
  author = {Chen, Wuyang and Zhou, Yanqi and Du, Nan and Huang, Yanping and Laudon, James and Chen, Zhifeng and Cu, Claire},
  year = 2023,
  month = may,
  number = {arXiv:2305.12281},
  eprint = {2305.12281},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.12281},
  urldate = {2025-10-01},
  abstract = {Pretraining on a large-scale corpus has become a standard method to build general language models (LMs). Adapting a model to new data distributions targeting different downstream tasks poses significant challenges. Naive fine-tuning may incur catastrophic forgetting when the over-parameterized LMs overfit the new data but fail to preserve the pretrained features. Lifelong learning (LLL) aims to enable information systems to learn from a continuous data stream across time. However, most prior work modifies the training recipe assuming a static fixed network architecture. We find that additional model capacity and proper regularization are key elements to achieving strong LLL performance. Thus, we propose Lifelong-MoE, an extensible MoE (Mixture-of-Experts) architecture that dynamically adds model capacity via adding experts with regularized pretraining. Our results show that by only introducing a limited number of extra experts while keeping the computation cost constant, our model can steadily adapt to data distribution shifts while preserving the previous knowledge. Compared to existing lifelong learning approaches, Lifelong-MoE achieves better few-shot performance on 19 downstream NLP tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/W6SPW6R6/Chen et al. - 2023 - Lifelong Language Pretraining with Distribution-Specialized Experts.pdf;/Users/giulialanzillotta/Zotero/storage/V85JBYQ4/2305.html}
}

@misc{chenReplayFreeContinualLowRank2025,
  title = {Replay-{{Free Continual Low-Rank Adaptation}} with {{Dynamic Memory}}},
  author = {Chen, Huancheng and Li, Jingtao and Zhuang, Weiming and Chen, Chen and Lyu, Lingjuan},
  year = 2025,
  month = sep,
  number = {arXiv:2411.00623},
  eprint = {2411.00623},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2411.00623},
  urldate = {2025-10-08},
  abstract = {We revisit continual learning{\textasciitilde}(CL), which enables pre-trained vision transformers (ViTs) to sequentially fine-tune on new downstream tasks over time. However, as the scale of these models increases, catastrophic forgetting remains a more serious challenge. Recent studies highlight a crossover between CL techniques and parameter-efficient fine-tuning (PEFT), which focuses on fine-tuning only a small set of trainable parameters to adapt to downstream tasks, such as low-rank adaptation (LoRA). While LoRA achieves faster convergence and requires fewer trainable parameters, it has seldom been explored in the context of continual learning. To address this gap, we propose a novel PEFT-CL method called Dual Low-Rank Adaptation (DualLoRA), which introduces both an orthogonal LoRA adapter and a residual LoRA adapter parallel to pre-trained weights in each layer. These components are orchestrated by a dynamic memory mechanism to strike a balance between stability and plasticity. Additionally, we propose a scheme to predict task identity with confidence and calibrate the model's outputs accordingly. On ViT-based models, we demonstrate that DualLoRA offers significant advantages in accuracy, inference speed, and computation efficiency in training over existing CL methods across multiple benchmarks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/N67ELAX6/Chen et al. - 2025 - Replay-Free Continual Low-Rank Adaptation with Dynamic Memory.pdf;/Users/giulialanzillotta/Zotero/storage/SNJMBIKE/2411.html}
}

@misc{choExponentiallyIncreasingCapacitytoComputation2014,
  title = {Exponentially {{Increasing}} the {{Capacity-to-Computation Ratio}} for {{Conditional Computation}} in {{Deep Learning}}},
  author = {Cho, Kyunghyun and Bengio, Yoshua},
  year = 2014,
  month = jun,
  number = {arXiv:1406.7362},
  eprint = {1406.7362},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1406.7362},
  urldate = {2025-10-03},
  abstract = {Many state-of-the-art results obtained with deep networks are achieved with the largest models that could be trained, and if more computation power was available, we might be able to exploit much larger datasets in order to improve generalization ability. Whereas in learning algorithms such as decision trees the ratio of capacity (e.g., the number of parameters) to computation is very favorable (up to exponentially more parameters than computation), the ratio is essentially 1 for deep neural networks. Conditional computation has been proposed as a way to increase the capacity of a deep neural network without increasing the amount of computation required, by activating some parameters and computation "on-demand", on a per-example basis. In this note, we propose a novel parametrization of weight matrices in neural networks which has the potential to increase up to exponentially the ratio of the number of parameters to computation. The proposed approach is based on turning on some parameters (weight matrices) when specific bit patterns of hidden unit activations are obtained. In order to better control for the overfitting that might result, we propose a parametrization that is tree-structured, where each node of the tree corresponds to a prefix of a sequence of sign bits, or gating units, associated with hidden units.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/KEYPKV4Z/Cho and Bengio - 2014 - Exponentially Increasing the Capacity-to-Computation Ratio for Conditional Computation in Deep Learn.pdf;/Users/giulialanzillotta/Zotero/storage/64V7JEGD/1406.html}
}

@misc{choExponentiallyIncreasingCapacitytoComputation2014a,
  title = {Exponentially {{Increasing}} the {{Capacity-to-Computation Ratio}} for {{Conditional Computation}} in {{Deep Learning}}},
  author = {Cho, Kyunghyun and Bengio, Yoshua},
  year = 2014,
  month = jun,
  number = {arXiv:1406.7362},
  eprint = {1406.7362},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1406.7362},
  urldate = {2025-10-03},
  abstract = {Many state-of-the-art results obtained with deep networks are achieved with the largest models that could be trained, and if more computation power was available, we might be able to exploit much larger datasets in order to improve generalization ability. Whereas in learning algorithms such as decision trees the ratio of capacity (e.g., the number of parameters) to computation is very favorable (up to exponentially more parameters than computation), the ratio is essentially 1 for deep neural networks. Conditional computation has been proposed as a way to increase the capacity of a deep neural network without increasing the amount of computation required, by activating some parameters and computation "on-demand", on a per-example basis. In this note, we propose a novel parametrization of weight matrices in neural networks which has the potential to increase up to exponentially the ratio of the number of parameters to computation. The proposed approach is based on turning on some parameters (weight matrices) when specific bit patterns of hidden unit activations are obtained. In order to better control for the overfitting that might result, we propose a parametrization that is tree-structured, where each node of the tree corresponds to a prefix of a sequence of sign bits, or gating units, associated with hidden units.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/EB6CML3P/Cho and Bengio - 2014 - Exponentially Increasing the Capacity-to-Computation Ratio for Conditional Computation in Deep Learn.pdf;/Users/giulialanzillotta/Zotero/storage/9VRSNZVU/1406.html}
}

@article{chrysakisOnlineContinualLearning,
  title = {Online {{Continual Learning}} from {{Imbalanced Data}}},
  author = {Chrysakis, Aristotelis and Moens, Marie-Francine},
  abstract = {A well-documented weakness of neural networks is the fact that they suffer from catastrophic for getting when trained on data provided by a nonstationary distribution. Recent work in the field of continual learning attempts to understand and overcome this issue. Unfortunately, the majority of relevant work embraces the implicit assump tion that the distribution of observed data is per fectly balanced, despite the fact that, in the real world, humans and animals learn from observa tions that are temporally correlated and severely imbalanced. Motivated by this remark, we aim to evaluate memory population methods that are used in online continual learning, when dealing with highly imbalanced and temporally correlated streams of data. More importantly, we introduce a new memory population approach, which we call class-balancing reservoir sampling (CBRS). We demonstrate that CBRS outperforms the state-of the-art memory population algorithms in a consid erably challenging learning setting, over a range of different datasets, and for multiple architec tures.},
  langid = {english},
  file = {/Users/giulialanzillotta/Zotero/storage/B64IIL86/Chrysakis and Moens - Online Continual Learning from Imbalanced Data.pdf}
}

@misc{collierRoutingNetworksCotraining2020,
  title = {Routing {{Networks}} with {{Co-training}} for {{Continual Learning}}},
  author = {Collier, Mark and Kokiopoulou, Efi and Gesmundo, Andrea and Berent, Jesse},
  year = 2020,
  month = sep,
  number = {arXiv:2009.04381},
  eprint = {2009.04381},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2009.04381},
  urldate = {2025-10-01},
  abstract = {The core challenge with continual learning is catastrophic forgetting, the phenomenon that when neural networks are trained on a sequence of tasks they rapidly forget previously learned tasks. It has been observed that catastrophic forgetting is most severe when tasks are dissimilar to each other. We propose the use of sparse routing networks for continual learning. For each input, these network architectures activate a different path through a network of experts. Routing networks have been shown to learn to route similar tasks to overlapping sets of experts and dissimilar tasks to disjoint sets of experts. In the continual learning context this behaviour is desirable as it minimizes interference between dissimilar tasks while allowing positive transfer between related tasks. In practice, we find it is necessary to develop a new training method for routing networks, which we call co-training which avoids poorly initialized experts when new tasks are presented. When combined with a small episodic memory replay buffer, sparse routing networks with co-training outperform densely connected networks on the MNIST-Permutations and MNIST-Rotations benchmarks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/65QTSHZZ/Collier et al. - 2020 - Routing Networks with Co-training for Continual Learning.pdf;/Users/giulialanzillotta/Zotero/storage/CFHV6PGD/2009.html}
}

@misc{damianSelfStabilizationImplicitBias2023,
  title = {Self-{{Stabilization}}: {{The Implicit Bias}} of {{Gradient Descent}} at the {{Edge}} of {{Stability}}},
  shorttitle = {Self-{{Stabilization}}},
  author = {Damian, Alex and Nichani, Eshaan and Lee, Jason D.},
  year = 2023,
  month = apr,
  number = {arXiv:2209.15594},
  eprint = {2209.15594},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2209.15594},
  urldate = {2025-04-18},
  abstract = {Traditional analyses of gradient descent show that when the largest eigenvalue of the Hessian, also known as the sharpness \$S({\textbackslash}theta)\$, is bounded by \$2/{\textbackslash}eta\$, training is "stable" and the training loss decreases monotonically. Recent works, however, have observed that this assumption does not hold when training modern neural networks with full batch or large batch gradient descent. Most recently, Cohen et al. (2021) observed two important phenomena. The first, dubbed progressive sharpening, is that the sharpness steadily increases throughout training until it reaches the instability cutoff \$2/{\textbackslash}eta\$. The second, dubbed edge of stability, is that the sharpness hovers at \$2/{\textbackslash}eta\$ for the remainder of training while the loss continues decreasing, albeit non-monotonically. We demonstrate that, far from being chaotic, the dynamics of gradient descent at the edge of stability can be captured by a cubic Taylor expansion: as the iterates diverge in direction of the top eigenvector of the Hessian due to instability, the cubic term in the local Taylor expansion of the loss function causes the curvature to decrease until stability is restored. This property, which we call self-stabilization, is a general property of gradient descent and explains its behavior at the edge of stability. A key consequence of self-stabilization is that gradient descent at the edge of stability implicitly follows projected gradient descent (PGD) under the constraint \$S({\textbackslash}theta) {\textbackslash}le 2/{\textbackslash}eta\$. Our analysis provides precise predictions for the loss, sharpness, and deviation from the PGD trajectory throughout training, which we verify both empirically in a number of standard settings and theoretically under mild conditions. Our analysis uncovers the mechanism for gradient descent's implicit bias towards stability.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Information Theory,Computer Science - Machine Learning,Mathematics - Information Theory,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/7CI8I558/Damian et al. - 2023 - Self-Stabilization The Implicit Bias of Gradient Descent at the Edge of Stability.pdf;/Users/giulialanzillotta/Zotero/storage/PU6EWYE3/2209.html}
}

@misc{damianSelfStabilizationImplicitBias2023a,
  title = {Self-{{Stabilization}}: {{The Implicit Bias}} of {{Gradient Descent}} at the {{Edge}} of {{Stability}}},
  shorttitle = {Self-{{Stabilization}}},
  author = {Damian, Alex and Nichani, Eshaan and Lee, Jason D.},
  year = 2023,
  month = apr,
  number = {arXiv:2209.15594},
  eprint = {2209.15594},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2209.15594},
  urldate = {2025-04-28},
  abstract = {Traditional analyses of gradient descent show that when the largest eigenvalue of the Hessian, also known as the sharpness \$S({\textbackslash}theta)\$, is bounded by \$2/{\textbackslash}eta\$, training is "stable" and the training loss decreases monotonically. Recent works, however, have observed that this assumption does not hold when training modern neural networks with full batch or large batch gradient descent. Most recently, Cohen et al. (2021) observed two important phenomena. The first, dubbed progressive sharpening, is that the sharpness steadily increases throughout training until it reaches the instability cutoff \$2/{\textbackslash}eta\$. The second, dubbed edge of stability, is that the sharpness hovers at \$2/{\textbackslash}eta\$ for the remainder of training while the loss continues decreasing, albeit non-monotonically. We demonstrate that, far from being chaotic, the dynamics of gradient descent at the edge of stability can be captured by a cubic Taylor expansion: as the iterates diverge in direction of the top eigenvector of the Hessian due to instability, the cubic term in the local Taylor expansion of the loss function causes the curvature to decrease until stability is restored. This property, which we call self-stabilization, is a general property of gradient descent and explains its behavior at the edge of stability. A key consequence of self-stabilization is that gradient descent at the edge of stability implicitly follows projected gradient descent (PGD) under the constraint \$S({\textbackslash}theta) {\textbackslash}le 2/{\textbackslash}eta\$. Our analysis provides precise predictions for the loss, sharpness, and deviation from the PGD trajectory throughout training, which we verify both empirically in a number of standard settings and theoretically under mild conditions. Our analysis uncovers the mechanism for gradient descent's implicit bias towards stability.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Information Theory,Computer Science - Machine Learning,Mathematics - Information Theory,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/F52DIS7X/Damian et al. - 2023 - Self-Stabilization The Implicit Bias of Gradient Descent at the Edge of Stability.pdf;/Users/giulialanzillotta/Zotero/storage/SSUXD4GA/2209.html}
}

@misc{dangMemoryefficientContinualLearning2024,
  title = {Memory-Efficient {{Continual Learning}} with {{Neural Collapse Contrastive}}},
  author = {Dang, Trung-Anh and Nguyen, Vincent and Vu, Ngoc-Son and Vrain, Christel},
  year = 2024,
  month = dec,
  number = {arXiv:2412.02865},
  eprint = {2412.02865},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2412.02865},
  urldate = {2025-09-16},
  abstract = {Contrastive learning has significantly improved representation quality, enhancing knowledge transfer across tasks in continual learning (CL). However, catastrophic forgetting remains a key challenge, as contrastive based methods primarily focus on "soft relationships" or "softness" between samples, which shift with changing data distributions and lead to representation overlap across tasks. Recently, the newly identified Neural Collapse phenomenon has shown promise in CL by focusing on "hard relationships" or "hardness" between samples and fixed prototypes. However, this approach overlooks "softness", crucial for capturing intra-class variability, and this rigid focus can also pull old class representations toward current ones, increasing forgetting. Building on these insights, we propose Focal Neural Collapse Contrastive (FNC{\textasciicircum}2), a novel representation learning loss that effectively balances both soft and hard relationships. Additionally, we introduce the Hardness-Softness Distillation (HSD) loss to progressively preserve the knowledge gained from these relationships across tasks. Our method outperforms state-of-the-art approaches, particularly in minimizing memory reliance. Remarkably, even without the use of memory, our approach rivals rehearsal-based methods, offering a compelling solution for data privacy concerns.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/ZAX6UWRH/Dang et al. - 2024 - Memory-efficient Continual Learning with Neural Collapse Contrastive.pdf;/Users/giulialanzillotta/Zotero/storage/QX6FC7MS/2412.html}
}

@misc{dangNeuralCollapseDeep2023,
  title = {Neural {{Collapse}} in {{Deep Linear Networks}}: {{From Balanced}} to {{Imbalanced Data}}},
  shorttitle = {Neural {{Collapse}} in {{Deep Linear Networks}}},
  author = {Dang, Hien and Tran, Tho and Osher, Stanley and {Tran-The}, Hung and Ho, Nhat and Nguyen, Tan},
  year = 2023,
  month = jun,
  number = {arXiv:2301.00437},
  eprint = {2301.00437},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2301.00437},
  urldate = {2025-09-16},
  abstract = {Modern deep neural networks have achieved impressive performance on tasks from image classification to natural language processing. Surprisingly, these complex systems with massive amounts of parameters exhibit the same structural properties in their last-layer features and classifiers across canonical datasets when training until convergence. In particular, it has been observed that the last-layer features collapse to their class-means, and those class-means are the vertices of a simplex Equiangular Tight Frame (ETF). This phenomenon is known as Neural Collapse (NC). Recent papers have theoretically shown that NC emerges in the global minimizers of training problems with the simplified "unconstrained feature model". In this context, we take a step further and prove the NC occurrences in deep linear networks for the popular mean squared error (MSE) and cross entropy (CE) losses, showing that global solutions exhibit NC properties across the linear layers. Furthermore, we extend our study to imbalanced data for MSE loss and present the first geometric analysis of NC under bias-free setting. Our results demonstrate the convergence of the last-layer features and classifiers to a geometry consisting of orthogonal vectors, whose lengths depend on the amount of data in their corresponding classes. Finally, we empirically validate our theoretical analyses on synthetic and practical network architectures with both balanced and imbalanced scenarios.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/98XINYLI/Dang et al. - 2023 - Neural Collapse in Deep Linear Networks From Balanced to Imbalanced Data.pdf;/Users/giulialanzillotta/Zotero/storage/KQNHCCHD/2301.html}
}

@misc{dangNeuralCollapseDeep2023a,
  title = {Neural {{Collapse}} in {{Deep Linear Networks}}: {{From Balanced}} to {{Imbalanced Data}}},
  shorttitle = {Neural {{Collapse}} in {{Deep Linear Networks}}},
  author = {Dang, Hien and Tran, Tho and Osher, Stanley and {Tran-The}, Hung and Ho, Nhat and Nguyen, Tan},
  year = 2023,
  month = jun,
  number = {arXiv:2301.00437},
  eprint = {2301.00437},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2301.00437},
  urldate = {2025-09-16},
  abstract = {Modern deep neural networks have achieved impressive performance on tasks from image classification to natural language processing. Surprisingly, these complex systems with massive amounts of parameters exhibit the same structural properties in their last-layer features and classifiers across canonical datasets when training until convergence. In particular, it has been observed that the last-layer features collapse to their class-means, and those class-means are the vertices of a simplex Equiangular Tight Frame (ETF). This phenomenon is known as Neural Collapse (NC). Recent papers have theoretically shown that NC emerges in the global minimizers of training problems with the simplified "unconstrained feature model". In this context, we take a step further and prove the NC occurrences in deep linear networks for the popular mean squared error (MSE) and cross entropy (CE) losses, showing that global solutions exhibit NC properties across the linear layers. Furthermore, we extend our study to imbalanced data for MSE loss and present the first geometric analysis of NC under bias-free setting. Our results demonstrate the convergence of the last-layer features and classifiers to a geometry consisting of orthogonal vectors, whose lengths depend on the amount of data in their corresponding classes. Finally, we empirically validate our theoretical analyses on synthetic and practical network architectures with both balanced and imbalanced scenarios.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/DQGJJDDC/Dang et al. - 2023 - Neural Collapse in Deep Linear Networks From Balanced to Imbalanced Data.pdf;/Users/giulialanzillotta/Zotero/storage/HBFZCV7S/2301.html}
}

@inproceedings{davariProbingRepresentationForgetting2022,
  title = {Probing Representation Forgetting in Supervised and Unsupervised Continual Learning},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF}} Conference on Computer Vision and Pattern Recognition},
  author = {Davari, MohammadReza and Asadi, Nader and Mudur, Sudhir and Aljundi, Rahaf and Belilovsky, Eugene},
  year = 2022,
  pages = {16712--16721}
}

@misc{davariProbingRepresentationForgetting2022a,
  title = {Probing {{Representation Forgetting}} in {{Supervised}} and {{Unsupervised Continual Learning}}},
  author = {Davari, MohammadReza and Asadi, Nader and Mudur, Sudhir and Aljundi, Rahaf and Belilovsky, Eugene},
  year = 2022,
  month = apr,
  number = {arXiv:2203.13381},
  eprint = {2203.13381},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2203.13381},
  urldate = {2025-10-03},
  abstract = {Continual Learning research typically focuses on tackling the phenomenon of catastrophic forgetting in neural networks. Catastrophic forgetting is associated with an abrupt loss of knowledge previously learned by a model when the task, or more broadly the data distribution, being trained on changes. In supervised learning problems this forgetting, resulting from a change in the model's representation, is typically measured or observed by evaluating the decrease in old task performance. However, a model's representation can change without losing knowledge about prior tasks. In this work we consider the concept of representation forgetting, observed by using the difference in performance of an optimal linear classifier before and after a new task is introduced. Using this tool we revisit a number of standard continual learning benchmarks and observe that, through this lens, model representations trained without any explicit control for forgetting often experience small representation forgetting and can sometimes be comparable to methods which explicitly control for forgetting, especially in longer task sequences. We also show that representation forgetting can lead to new insights on the effect of model capacity and loss function used in continual learning. Based on our results, we show that a simple yet competitive approach is to learn representations continually with standard supervised contrastive learning while constructing prototypes of class samples when queried on old samples.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/3WW6X8QE/Davari et al. - 2022 - Probing Representation Forgetting in Supervised and Unsupervised Continual Learning.pdf;/Users/giulialanzillotta/Zotero/storage/4MZFC6YP/2203.html}
}

@article{daxbergerImprovingContinualLearning,
  title = {Improving {{Continual Learning}} by {{Accurate Gradient Reconstructions}} of the {{Past}}},
  author = {Daxberger, Erik and Swaroop, Siddharth and Osawa, Kazuki},
  abstract = {Weight-regularization and experience replay are two popular continual-learning strategies with complementary strengths: while weight-regularization requires less memory, replay can more accurately mimic batch training. How can we combine them to get better methods? Despite the simplicity of the question, little is known or done to optimally combine these approaches. In this paper, we present such a method by using a recently proposed principle of adaptation that relies on a faithful reconstruction of the gradients of the past data. Using this principle, we design a prior which combines two types of replay methods with a quadratic weight-regularizer and achieves better gradient reconstructions. The combination improves performance on standard task-incremental continual learning benchmarks such as Split-CIFAR, SplitTinyImageNet, and ImageNet-1000, achieving {$>$} 80\% of the batch performance by simply utilizing a memory of {$<$} 10\% of the past data. Our work shows that a good combination of the two strategies can be very effective in reducing forgetting.},
  langid = {english},
  file = {/Users/giulialanzillotta/Zotero/storage/GZHZESUD/Daxberger et al. - Improving Continual Learning by Accurate Gradient Reconstructions of the Past.pdf}
}

@article{daxbergerImprovingContinualLearning2023,
  title = {Improving {{Continual Learning}} by {{Accurate Gradient Reconstructions}} of the {{Past}}},
  author = {Daxberger, Erik and Swaroop, Siddharth and Osawa, Kazuki and Yokota, Rio and Turner, Richard E. and {Hern{\'a}ndez-Lobato}, Jos{\'e} Miguel and Khan, Mohammad Emtiyaz},
  year = 2023,
  month = jul,
  journal = {Transactions on Machine Learning Research},
  issn = {2835-8856},
  urldate = {2025-09-30},
  abstract = {Weight-regularization and experience replay are two popular continual-learning strategies with complementary strengths: while weight-regularization requires less memory, replay can more accurately mimic batch training. How can we combine them to get better methods? Despite the simplicity of the question, little is known or done to optimally combine these approaches. In this paper, we present such a method by using a recently proposed principle of adaptation that relies on a faithful reconstruction of the gradients of the past data. Using this principle, we design a prior which combines two types of replay methods with a quadratic weight-regularizer and achieves better gradient reconstructions. The combination improves performance on standard task-incremental continual learning benchmarks such as Split-CIFAR, SplitTinyImageNet, and ImageNet-1000, achieving \${$>\backslash$}!80{\textbackslash}\%\$ of the batch performance by simply utilizing a memory of \${$<\backslash$}!10{\textbackslash}\%\$ of the past data. Our work shows that a good combination of the two strategies can be very effective in reducing forgetting.},
  langid = {english},
  file = {/Users/giulialanzillotta/Zotero/storage/2QBFDE4U/Daxberger et al. - 2023 - Improving Continual Learning by Accurate Gradient Reconstructions of the Past.pdf}
}

@article{daxbergerImprovingContinualLearninga,
  title = {Improving {{Continual Learning}} by {{Accurate Gradient Reconstructions}} of the {{Past}}},
  author = {Daxberger, Erik and Swaroop, Siddharth and Osawa, Kazuki},
  abstract = {Weight-regularization and experience replay are two popular continual-learning strategies with complementary strengths: while weight-regularization requires less memory, replay can more accurately mimic batch training. How can we combine them to get better methods? Despite the simplicity of the question, little is known or done to optimally combine these approaches. In this paper, we present such a method by using a recently proposed principle of adaptation that relies on a faithful reconstruction of the gradients of the past data. Using this principle, we design a prior which combines two types of replay methods with a quadratic weight-regularizer and achieves better gradient reconstructions. The combination improves performance on standard task-incremental continual learning benchmarks such as Split-CIFAR, SplitTinyImageNet, and ImageNet-1000, achieving {$>$} 80\% of the batch performance by simply utilizing a memory of {$<$} 10\% of the past data. Our work shows that a good combination of the two strategies can be very effective in reducing forgetting.},
  langid = {english},
  file = {/Users/giulialanzillotta/Zotero/storage/3BSE4YTG/Daxberger et al. - Improving Continual Learning by Accurate Gradient Reconstructions of the Past.pdf}
}

@article{daxbergerImprovingContinualLearningb,
  title = {Improving {{Continual Learning}} by {{Accurate Gradient Reconstructions}} of the {{Past}}},
  author = {Daxberger, Erik and Swaroop, Siddharth and Osawa, Kazuki},
  abstract = {Weight-regularization and experience replay are two popular continual-learning strategies with complementary strengths: while weight-regularization requires less memory, replay can more accurately mimic batch training. How can we combine them to get better methods? Despite the simplicity of the question, little is known or done to optimally combine these approaches. In this paper, we present such a method by using a recently proposed principle of adaptation that relies on a faithful reconstruction of the gradients of the past data. Using this principle, we design a prior which combines two types of replay methods with a quadratic weight-regularizer and achieves better gradient reconstructions. The combination improves performance on standard task-incremental continual learning benchmarks such as Split-CIFAR, SplitTinyImageNet, and ImageNet-1000, achieving {$>$} 80\% of the batch performance by simply utilizing a memory of {$<$} 10\% of the past data. Our work shows that a good combination of the two strategies can be very effective in reducing forgetting.},
  langid = {english},
  file = {/Users/giulialanzillotta/Zotero/storage/IRZ4DSNQ/Daxberger et al. - Improving Continual Learning by Accurate Gradient Reconstructions of the Past.pdf}
}

@article{delangeContinualLearningSurvey2022,
  title = {A {{Continual Learning Survey}}: {{Defying Forgetting}} in {{Classification Tasks}}},
  shorttitle = {A {{Continual Learning Survey}}},
  author = {De Lange, Matthias and Aljundi, Rahaf and Masana, Marc and Parisot, Sarah and Jia, Xu and Leonardis, Ale{\v s} and Slabaugh, Gregory and Tuytelaars, Tinne},
  year = 2022,
  month = jul,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {44},
  number = {7},
  pages = {3366--3385},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2021.3057446},
  urldate = {2025-04-14},
  abstract = {Artificial neural networks thrive in solving the classification problem for a particular rigid task, acquiring knowledge through generalized learning behaviour from a distinct training phase. The resulting network resembles a static entity of knowledge, with endeavours to extend this knowledge without targeting the original task resulting in a catastrophic forgetting. Continual learning shifts this paradigm towards networks that can continually accumulate knowledge over different tasks without the need to retrain from scratch. We focus on task incremental classification, where tasks arrive sequentially and are delineated by clear boundaries. Our main contributions concern: (1) a taxonomy and extensive overview of the state-of-the-art; (2) a novel framework to continually determine the stability-plasticity trade-off of the continual learner; (3) a comprehensive experimental comparison of 11 state-of-the-art continual learning methods; and (4) baselines. We empirically scrutinize method strengths and weaknesses on three benchmarks, considering Tiny Imagenet and large-scale unbalanced iNaturalist and a sequence of recognition datasets. We study the influence of model capacity, weight decay and dropout regularization, and the order in which the tasks are presented, and qualitatively compare methods in terms of required memory, computation time, and storage.},
  keywords = {catastrophic forgetting,classification,Continual learning,Interference,Knowledge engineering,Learning systems,lifelong learning,neural networks,Neural networks,Task analysis,task incremental learning,Training,Training data},
  file = {/Users/giulialanzillotta/Zotero/storage/XDJ8UKMD/De Lange et al. - 2022 - A Continual Learning Survey Defying Forgetting in Classification Tasks.pdf}
}

@misc{dengFlatteningSharpnessDynamic2021,
  title = {Flattening {{Sharpness}} for {{Dynamic Gradient Projection Memory Benefits Continual Learning}}},
  author = {Deng, Danruo and Chen, Guangyong and Hao, Jianye and Wang, Qiong and Heng, Pheng-Ann},
  year = 2021,
  month = oct,
  number = {arXiv:2110.04593},
  eprint = {2110.04593},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2110.04593},
  urldate = {2025-10-01},
  abstract = {The backpropagation networks are notably susceptible to catastrophic forgetting, where networks tend to forget previously learned skills upon learning new ones. To address such the 'sensitivity-stability' dilemma, most previous efforts have been contributed to minimizing the empirical risk with different parameter regularization terms and episodic memory, but rarely exploring the usages of the weight loss landscape. In this paper, we investigate the relationship between the weight loss landscape and sensitivity-stability in the continual learning scenario, based on which, we propose a novel method, Flattening Sharpness for Dynamic Gradient Projection Memory (FS-DGPM). In particular, we introduce a soft weight to represent the importance of each basis representing past tasks in GPM, which can be adaptively learned during the learning process, so that less important bases can be dynamically released to improve the sensitivity of new skill learning. We further introduce Flattening Sharpness (FS) to reduce the generalization gap by explicitly regulating the flatness of the weight loss landscape of all seen tasks. As demonstrated empirically, our proposed method consistently outperforms baselines with the superior ability to learn new skills while alleviating forgetting effectively.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/2DLKLEXW/Deng et al. - 2021 - Flattening Sharpness for Dynamic Gradient Projection Memory Benefits Continual Learning.pdf;/Users/giulialanzillotta/Zotero/storage/IDATSG6M/2110.html}
}

@article{dohareContinualBackpropStochastic2021,
  title = {Continual Backprop: {{Stochastic}} Gradient Descent with Persistent Randomness},
  author = {Dohare, Shibhansh and Sutton, Richard S and Mahmood, A Rupam},
  year = 2021,
  journal = {arXiv preprint arXiv:2108.06325},
  eprint = {2108.06325},
  archiveprefix = {arXiv}
}

@article{dohareLossPlasticityDeep2024,
  title = {Loss of Plasticity in Deep Continual Learning},
  author = {Dohare, Shibhansh and {Hernandez-Garcia}, J. Fernando and Lan, Qingfeng and Rahman, Parash and Mahmood, A. Rupam and Sutton, Richard S.},
  year = 2024,
  month = aug,
  journal = {Nature},
  volume = {632},
  number = {8026},
  pages = {768--774},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/s41586-024-07711-7},
  urldate = {2025-10-13},
  abstract = {The pervasive problem of artificial neural networks losing plasticity in continual-learning settings is demonstrated and a simple solution called the continual\&nbsp;backpropagation algorithm is described to prevent this issue.},
  copyright = {2024 The Author(s)},
  langid = {english},
  file = {/Users/giulialanzillotta/Zotero/storage/J37L6Q34/Dohare et al. - 2024 - Loss of plasticity in deep continual learning.pdf}
}

@misc{douLoRAMoEAlleviateWorld2024,
  title = {{{LoRAMoE}}: {{Alleviate World Knowledge Forgetting}} in {{Large Language Models}} via {{MoE-Style Plugin}}},
  shorttitle = {{{LoRAMoE}}},
  author = {Dou, Shihan and Zhou, Enyu and Liu, Yan and Gao, Songyang and Zhao, Jun and Shen, Wei and Zhou, Yuhao and Xi, Zhiheng and Wang, Xiao and Fan, Xiaoran and Pu, Shiliang and Zhu, Jiang and Zheng, Rui and Gui, Tao and Zhang, Qi and Huang, Xuanjing},
  year = 2024,
  month = mar,
  number = {arXiv:2312.09979},
  eprint = {2312.09979},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2312.09979},
  urldate = {2025-10-01},
  abstract = {Supervised fine-tuning (SFT) is a crucial step for large language models (LLMs), enabling them to align with human instructions and enhance their capabilities in downstream tasks. Increasing instruction data substantially is a direct solution to align the model with a broader range of downstream tasks or notably improve its performance on a specific task. However, we find that large-scale increases in instruction data can damage the world knowledge previously stored in LLMs. To address this challenge, we propose LoRAMoE, a novelty framework that introduces several low-rank adapters (LoRA) and integrates them by using a router network, like a plugin version of Mixture of Experts (MoE). It freezes the backbone model and forces a portion of LoRAs to focus on leveraging world knowledge to solve downstream tasks, to alleviate world knowledge-edge forgetting. Experimental results show that, as the instruction data increases, LoRAMoE can significantly improve the ability to process downstream tasks, while maintaining the world knowledge stored in the LLM.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/giulialanzillotta/Zotero/storage/UFDYID9T/Dou et al. - 2024 - LoRAMoE Alleviate World Knowledge Forgetting in Large Language Models via MoE-Style Plugin.pdf;/Users/giulialanzillotta/Zotero/storage/GWTM54BN/2312.html}
}

@article{duAdaptingAuxiliaryLosses2018,
  title = {Adapting {{Auxiliary Losses Using Gradient Similarity}}},
  author = {Du, Yunshu and Czarnecki, Wojciech M. and Jayakumar, Siddhant M. and Pascanu, Razvan and Lakshminarayanan, Balaji},
  year = 2018,
  journal = {ArXiv},
  volume = {abs/1812.02224}
}

@misc{duGLaMEfficientScaling2022,
  title = {{{GLaM}}: {{Efficient Scaling}} of {{Language Models}} with {{Mixture-of-Experts}}},
  shorttitle = {{{GLaM}}},
  author = {Du, Nan and Huang, Yanping and Dai, Andrew M. and Tong, Simon and Lepikhin, Dmitry and Xu, Yuanzhong and Krikun, Maxim and Zhou, Yanqi and Yu, Adams Wei and Firat, Orhan and Zoph, Barret and Fedus, Liam and Bosma, Maarten and Zhou, Zongwei and Wang, Tao and Wang, Yu Emma and Webster, Kellie and Pellat, Marie and Robinson, Kevin and {Meier-Hellstern}, Kathleen and Duke, Toju and Dixon, Lucas and Zhang, Kun and Le, Quoc V. and Wu, Yonghui and Chen, Zhifeng and Cui, Claire},
  year = 2022,
  month = aug,
  number = {arXiv:2112.06905},
  eprint = {2112.06905},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2112.06905},
  urldate = {2025-10-01},
  abstract = {Scaling language models with more data, compute and parameters has driven significant progress in natural language processing. For example, thanks to scaling, GPT-3 was able to achieve strong results on in-context learning tasks. However, training these large dense models requires significant amounts of computing resources. In this paper, we propose and develop a family of language models named GLaM (Generalist Language Model), which uses a sparsely activated mixture-of-experts architecture to scale the model capacity while also incurring substantially less training cost compared to dense variants. The largest GLaM has 1.2 trillion parameters, which is approximately 7x larger than GPT-3. It consumes only 1/3 of the energy used to train GPT-3 and requires half of the computation flops for inference, while still achieving better overall zero-shot and one-shot performance across 29 NLP tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/giulialanzillotta/Zotero/storage/LNVWKBV6/Du et al. - 2022 - GLaM Efficient Scaling of Language Models with Mixture-of-Experts.pdf;/Users/giulialanzillotta/Zotero/storage/PYP6DG2C/2112.html}
}

@misc{ebrahimiAdversarialContinualLearning2020,
  title = {Adversarial {{Continual Learning}}},
  author = {Ebrahimi, Sayna and Meier, Franziska and Calandra, Roberto and Darrell, Trevor and Rohrbach, Marcus},
  year = 2020,
  month = jul,
  number = {arXiv:2003.09553},
  eprint = {2003.09553},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2003.09553},
  urldate = {2025-10-01},
  abstract = {Continual learning aims to learn new tasks without forgetting previously learned ones. We hypothesize that representations learned to solve each task in a sequence have a shared structure while containing some task-specific properties. We show that shared features are significantly less prone to forgetting and propose a novel hybrid continual learning framework that learns a disjoint representation for task-invariant and task-specific features required to solve a sequence of tasks. Our model combines architecture growth to prevent forgetting of task-specific skills and an experience replay approach to preserve shared skills. We demonstrate our hybrid approach is effective in avoiding forgetting and show it is superior to both architecture-based and memory-based approaches on class incrementally learning of a single dataset as well as a sequence of multiple datasets in image classification. Our code is available at {\textbackslash}url\{https://github.com/facebookresearch/Adversarial-Continual-Learning\}.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/LVXCRL5A/Ebrahimi et al. - 2020 - Adversarial Continual Learning.pdf;/Users/giulialanzillotta/Zotero/storage/HTLWFY8F/2003.html}
}

@article{elsayedAddressingLossPlasticity2022,
  title = {Addressing {{Loss}} of {{Plasticity}} and {{Catastrophic Forgetting}} in {{Continual Learning}}},
  author = {Elsayed, Mohamed and Mahmood, A. Rupam},
  year = 2022,
  journal = {arXiv preprint arXiv:2205.12995},
  eprint = {2205.12995},
  publisher = {University of Alberta, Alberta Machine Intelligence Institute (Amii)},
  archiveprefix = {arXiv}
}

@misc{elsayedAddressingLossPlasticity2024,
  title = {Addressing {{Loss}} of {{Plasticity}} and {{Catastrophic Forgetting}} in {{Continual Learning}}},
  author = {Elsayed, Mohamed and Mahmood, A. Rupam},
  year = 2024,
  month = apr,
  number = {arXiv:2404.00781},
  eprint = {2404.00781},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2404.00781},
  urldate = {2025-10-13},
  abstract = {Deep representation learning methods struggle with continual learning, suffering from both catastrophic forgetting of useful units and loss of plasticity, often due to rigid and unuseful units. While many methods address these two issues separately, only a few currently deal with both simultaneously. In this paper, we introduce Utility-based Perturbed Gradient Descent (UPGD) as a novel approach for the continual learning of representations. UPGD combines gradient updates with perturbations, where it applies smaller modifications to more useful units, protecting them from forgetting, and larger modifications to less useful units, rejuvenating their plasticity. We use a challenging streaming learning setup where continual learning problems have hundreds of non-stationarities and unknown task boundaries. We show that many existing methods suffer from at least one of the issues, predominantly manifested by their decreasing accuracy over tasks. On the other hand, UPGD continues to improve performance and surpasses or is competitive with all methods in all problems. Finally, in extended reinforcement learning experiments with PPO, we show that while Adam exhibits a performance drop after initial learning, UPGD avoids it by addressing both continual learning issues.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/2DYS72AD/Elsayed and Mahmood - 2024 - Addressing Loss of Plasticity and Catastrophic Forgetting in Continual Learning.pdf;/Users/giulialanzillotta/Zotero/storage/BZC6PAJW/2404.html}
}

@misc{ermisMemoryEfficientContinual2023,
  title = {Memory {{Efficient Continual Learning}} with {{Transformers}}},
  author = {Ermis, Beyza and Zappella, Giovanni and Wistuba, Martin and Rawal, Aditya and Archambeau, Cedric},
  year = 2023,
  month = jan,
  number = {arXiv:2203.04640},
  eprint = {2203.04640},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2203.04640},
  urldate = {2025-10-01},
  abstract = {In many real-world scenarios, data to train machine learning models becomes available over time. Unfortunately, these models struggle to continually learn new concepts without forgetting what has been learnt in the past. This phenomenon is known as catastrophic forgetting and it is difficult to prevent due to practical constraints. For instance, the amount of data that can be stored or the computational resources that can be used might be limited. Moreover, applications increasingly rely on large pre-trained neural networks, such as pre-trained Transformers, since the resources or data might not be available in sufficiently large quantities to practitioners to train the model from scratch. In this paper, we devise a method to incrementally train a model on a sequence of tasks using pre-trained Transformers and extending them with Adapters. Different than the existing approaches, our method is able to scale to a large number of tasks without significant overhead and allows sharing information across tasks. On both image and text classification tasks, we empirically demonstrate that our method maintains a good predictive performance without retraining the model or increasing the number of model parameters over time. The resulting model is also significantly faster at inference time compared to Adapter-based state-of-the-art methods.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Statistics - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/LCPC7AKJ/Ermis et al. - 2023 - Memory Efficient Continual Learning with Transformers.pdf;/Users/giulialanzillotta/Zotero/storage/3WL2NK6V/2203.html}
}

@inproceedings{evronHowCatastrophicCan2022,
  title = {How Catastrophic Can Catastrophic Forgetting Be in Linear Regression?},
  booktitle = {Conference on {{Learning Theory}}},
  author = {Evron, Itay and Moroshko, Edward and Ward, Rachel and Srebro, Nathan and Soudry, Daniel},
  year = 2022,
  pages = {4028--4079},
  publisher = {PMLR}
}

@article{evronJointEffectTask2024,
  title = {The {{Joint Effect}} of {{Task Similarity}} and {{Overparameterization}} on {{Catastrophic Forgetting}}--{{An Analytical Model}}},
  author = {Evron, Itay and Goldfarb, Daniel and Weinberger, Nir and Soudry, Daniel and Hand, Paul},
  year = 2024,
  journal = {arXiv preprint arXiv:2401.12617},
  eprint = {2401.12617},
  archiveprefix = {arXiv}
}

@article{fangExploringDeepNeural2021,
  title = {Exploring Deep Neural Networks via Layer-Peeled Model: {{Minority}} Collapse in Imbalanced Training},
  author = {Fang, Cong and He, Hangfeng and Long, Qi and Su, Weijie J},
  year = 2021,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {118},
  number = {43},
  pages = {e2103091118},
  publisher = {National Academy of Sciences}
}

@article{fangExploringDeepNeural2021a,
  title = {Exploring {{Deep Neural Networks}} via {{Layer-Peeled Model}}: {{Minority Collapse}} in {{Imbalanced Training}}},
  shorttitle = {Exploring {{Deep Neural Networks}} via {{Layer-Peeled Model}}},
  author = {Fang, Cong and He, Hangfeng and Long, Qi and Su, Weijie J.},
  year = 2021,
  month = oct,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {118},
  number = {43},
  eprint = {2101.12699},
  primaryclass = {cs},
  pages = {e2103091118},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.2103091118},
  urldate = {2025-09-16},
  abstract = {In this paper, we introduce the {\textbackslash}textit\{Layer-Peeled Model\}, a nonconvex yet analytically tractable optimization program, in a quest to better understand deep neural networks that are trained for a sufficiently long time. As the name suggests, this new model is derived by isolating the topmost layer from the remainder of the neural network, followed by imposing certain constraints separately on the two parts of the network. We demonstrate that the Layer-Peeled Model, albeit simple, inherits many characteristics of well-trained neural networks, thereby offering an effective tool for explaining and predicting common empirical patterns of deep learning training. First, when working on class-balanced datasets, we prove that any solution to this model forms a simplex equiangular tight frame, which in part explains the recently discovered phenomenon of neural collapse {\textbackslash}cite\{papyan2020prevalence\}. More importantly, when moving to the imbalanced case, our analysis of the Layer-Peeled Model reveals a hitherto unknown phenomenon that we term {\textbackslash}textit\{Minority Collapse\}, which fundamentally limits the performance of deep learning models on the minority classes. In addition, we use the Layer-Peeled Model to gain insights into how to mitigate Minority Collapse. Interestingly, this phenomenon is first predicted by the Layer-Peeled Model before being confirmed by our computational experiments.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/FBL3XSI5/Fang et al. - 2021 - Exploring Deep Neural Networks via Layer-Peeled Model Minority Collapse in Imbalanced Training.pdf;/Users/giulialanzillotta/Zotero/storage/ELKFGUJW/2101.html}
}

@article{fangExploringDeepNeural2021b,
  title = {Exploring {{Deep Neural Networks}} via {{Layer-Peeled Model}}: {{Minority Collapse}} in {{Imbalanced Training}}},
  shorttitle = {Exploring {{Deep Neural Networks}} via {{Layer-Peeled Model}}},
  author = {Fang, Cong and He, Hangfeng and Long, Qi and Su, Weijie J.},
  year = 2021,
  month = oct,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {118},
  number = {43},
  eprint = {2101.12699},
  primaryclass = {cs},
  pages = {e2103091118},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.2103091118},
  urldate = {2025-09-16},
  abstract = {In this paper, we introduce the {\textbackslash}textit\{Layer-Peeled Model\}, a nonconvex yet analytically tractable optimization program, in a quest to better understand deep neural networks that are trained for a sufficiently long time. As the name suggests, this new model is derived by isolating the topmost layer from the remainder of the neural network, followed by imposing certain constraints separately on the two parts of the network. We demonstrate that the Layer-Peeled Model, albeit simple, inherits many characteristics of well-trained neural networks, thereby offering an effective tool for explaining and predicting common empirical patterns of deep learning training. First, when working on class-balanced datasets, we prove that any solution to this model forms a simplex equiangular tight frame, which in part explains the recently discovered phenomenon of neural collapse {\textbackslash}cite\{papyan2020prevalence\}. More importantly, when moving to the imbalanced case, our analysis of the Layer-Peeled Model reveals a hitherto unknown phenomenon that we term {\textbackslash}textit\{Minority Collapse\}, which fundamentally limits the performance of deep learning models on the minority classes. In addition, we use the Layer-Peeled Model to gain insights into how to mitigate Minority Collapse. Interestingly, this phenomenon is first predicted by the Layer-Peeled Model before being confirmed by our computational experiments.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/QTB2FACG/Fang et al. - 2021 - Exploring Deep Neural Networks via Layer-Peeled Model Minority Collapse in Imbalanced Training.pdf;/Users/giulialanzillotta/Zotero/storage/45X58GLR/2101.html}
}

@inproceedings{farajtabarOrthogonalGradientDescent2020,
  title = {Orthogonal {{Gradient Descent}} for {{Continual Learning}}},
  booktitle = {Proceedings of the {{Twenty Third International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Farajtabar, Mehrdad and Azizan, Navid and Mott, Alex and Li, Ang},
  year = 2020,
  month = jun,
  pages = {3762--3773},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2025-09-30},
  abstract = {Neural networks are achieving state of the art and sometimes super-human performance on learning tasks across a variety of domains. Whenever these problems require learning in a continual or sequential manner, however, neural networks suffer from the problem of catastrophic forgetting; they forget how to solve previous tasks after being trained on a new task, despite having the essential capacity to solve both tasks if they were trained on both simultaneously. In this paper, we propose to address this issue from a parameter space perspective and study an approach to restrict the direction of the gradient updates to avoid forgetting previously-learned data. We present the Orthogonal Gradient Descent (OGD) method, which accomplishes this goal by projecting the gradients from new tasks onto a subspace in which the neural network output on previous task does not change and the projected gradient is still in a useful direction for learning the new task. Our approach utilizes the high capacity of a neural network more efficiently and does not require storing the previously learned data that might raise privacy concerns. Experiments on common benchmarks reveal the effectiveness of the proposed OGD method.},
  langid = {english},
  file = {/Users/giulialanzillotta/Zotero/storage/4CUM67KA/Farajtabar et al. - 2020 - Orthogonal Gradient Descent for Continual Learning.pdf}
}

@misc{farquharUnifyingBayesianView2019,
  title = {A {{Unifying Bayesian View}} of {{Continual Learning}}},
  author = {Farquhar, Sebastian and Gal, Yarin},
  year = 2019,
  month = feb,
  number = {arXiv:1902.06494},
  eprint = {1902.06494},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1902.06494},
  urldate = {2025-04-14},
  abstract = {Some machine learning applications require continual learning - where data comes in a sequence of datasets, each is used for training and then permanently discarded. From a Bayesian perspective, continual learning seems straightforward: Given the model posterior one would simply use this as the prior for the next task. However, exact posterior evaluation is intractable with many models, especially with Bayesian neural networks (BNNs). Instead, posterior approximations are often sought. Unfortunately, when posterior approximations are used, prior-focused approaches do not succeed in evaluations designed to capture properties of realistic continual learning use cases. As an alternative to prior-focused methods, we introduce a new approximate Bayesian derivation of the continual learning loss. Our loss does not rely on the posterior from earlier tasks, and instead adapts the model itself by changing the likelihood term. We call these approaches likelihood-focused. We then combine prior- and likelihood-focused methods into one objective, tying the two views together under a single unifying framework of approximate Bayesian continual learning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/SNJCV5X4/Farquhar and Gal - 2019 - A Unifying Bayesian View of Continual Learning.pdf;/Users/giulialanzillotta/Zotero/storage/RVEWNJXQ/1902.html}
}

@inproceedings{fedusRevisitingFundamentalsExperience2020,
  title = {Revisiting {{Fundamentals}} of {{Experience Replay}}},
  booktitle = {Proceedings of the 37th {{International Conference}} on {{Machine Learning}}},
  author = {Fedus, William and Ramachandran, Prajit and Agarwal, Rishabh and Bengio, Yoshua and Larochelle, Hugo and Rowland, Mark and Dabney, Will},
  year = 2020,
  month = nov,
  pages = {3061--3071},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2025-10-07},
  abstract = {Experience replay is central to off-policy algorithms in deep reinforcement learning (RL), but there remain significant gaps in our understanding. We therefore present a systematic and extensive analysis of experience replay in Q-learning methods, focusing on two fundamental properties: the replay capacity and the ratio of learning updates to experience collected (replay ratio). Our additive and ablative studies upend conventional wisdom around experience replay \{---\} greater capacity is found to substantially increase the performance of certain algorithms, while leaving others unaffected. Counterintuitively we show that theoretically ungrounded, uncorrected n-step returns are uniquely beneficial while other techniques confer limited benefit for sifting through larger memory. Separately, by directly controlling the replay ratio we contextualize previous observations in the literature and empirically measure its importance across a variety of deep RL algorithms. Finally, we conclude by testing a set of hypotheses on the nature of these performance benefits.},
  langid = {english},
  file = {/Users/giulialanzillotta/Zotero/storage/JDXIQ3QE/Fedus et al. - 2020 - Revisiting Fundamentals of Experience Replay.pdf}
}

@misc{fedusSwitchTransformersScaling2022,
  title = {Switch {{Transformers}}: {{Scaling}} to {{Trillion Parameter Models}} with {{Simple}} and {{Efficient Sparsity}}},
  shorttitle = {Switch {{Transformers}}},
  author = {Fedus, William and Zoph, Barret and Shazeer, Noam},
  year = 2022,
  month = jun,
  number = {arXiv:2101.03961},
  eprint = {2101.03961},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2101.03961},
  urldate = {2025-10-01},
  abstract = {In deep learning, models typically reuse the same parameters for all inputs. Mixture of Experts (MoE) defies this and instead selects different parameters for each incoming example. The result is a sparsely-activated model -- with outrageous numbers of parameters -- but a constant computational cost. However, despite several notable successes of MoE, widespread adoption has been hindered by complexity, communication costs and training instability -- we address these with the Switch Transformer. We simplify the MoE routing algorithm and design intuitive improved models with reduced communication and computational costs. Our proposed training techniques help wrangle the instabilities and we show large sparse models may be trained, for the first time, with lower precision (bfloat16) formats. We design models based off T5-Base and T5-Large to obtain up to 7x increases in pre-training speed with the same computational resources. These improvements extend into multilingual settings where we measure gains over the mT5-Base version across all 101 languages. Finally, we advance the current scale of language models by pre-training up to trillion parameter models on the "Colossal Clean Crawled Corpus" and achieve a 4x speedup over the T5-XXL model.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/2I9NTFQB/Fedus et al. - 2022 - Switch Transformers Scaling to Trillion Parameter Models with Simple and Efficient Sparsity.pdf;/Users/giulialanzillotta/Zotero/storage/BDTX5J9B/2101.html}
}

@article{fengInverseVarianceFlatness2021,
  title = {The Inverse Variance--Flatness Relation in Stochastic Gradient Descent Is Critical for Finding Flat Minima},
  author = {Feng, Yu and Tu, Yuhai},
  year = 2021,
  month = mar,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {118},
  number = {9},
  pages = {e2015617118},
  publisher = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.2015617118},
  urldate = {2025-07-05},
  abstract = {Despite tremendous success of the stochastic gradient descent (SGD) algorithm in deep learning, little is known about how SGD finds generalizable solutions at flat minima of the loss function in high-dimensional weight space. Here, we investigate the connection between SGD learning dynamics and the loss function landscape. A principal component analysis (PCA) shows that SGD dynamics follow a low-dimensional drift--diffusion motion in the weight space. Around a solution found by SGD, the loss function landscape can be characterized by its flatness in each PCA direction. Remarkably, our study reveals a robust inverse relation between the weight variance and the landscape flatness in all PCA directions, which is the opposite to the fluctuation--response relation (aka Einstein relation) in equilibrium statistical physics. To understand the inverse variance--flatness relation, we develop a phenomenological theory of SGD based on statistical properties of the ensemble of minibatch loss functions. We find that both the anisotropic SGD noise strength (temperature) and its correlation time depend inversely on the landscape flatness in each PCA direction. Our results suggest that SGD serves as a landscape-dependent annealing algorithm. The effective temperature decreases with the landscape flatness so the system seeks out (prefers) flat minima over sharp ones. Based on these insights, an algorithm with landscape-dependent constraints is developed to mitigate catastrophic forgetting efficiently when learning multiple tasks sequentially. In general, our work provides a theoretical framework to understand learning dynamics, which may eventually lead to better algorithms for different learning tasks.},
  file = {/Users/giulialanzillotta/Zotero/storage/QTCMTZDY/Feng and Tu - 2021 - The inverse varianceâ€“flatness relation in stochastic gradient descent is critical for finding flat m.pdf}
}

@misc{fernandoPathNetEvolutionChannels2017,
  title = {{{PathNet}}: {{Evolution Channels Gradient Descent}} in {{Super Neural Networks}}},
  shorttitle = {{{PathNet}}},
  author = {Fernando, Chrisantha and Banarse, Dylan and Blundell, Charles and Zwols, Yori and Ha, David and Rusu, Andrei A. and Pritzel, Alexander and Wierstra, Daan},
  year = 2017,
  month = jan,
  number = {arXiv:1701.08734},
  eprint = {1701.08734},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1701.08734},
  urldate = {2025-10-01},
  abstract = {For artificial general intelligence (AGI) it would be efficient if multiple users trained the same giant neural network, permitting parameter reuse, without catastrophic forgetting. PathNet is a first step in this direction. It is a neural network algorithm that uses agents embedded in the neural network whose task is to discover which parts of the network to re-use for new tasks. Agents are pathways (views) through the network which determine the subset of parameters that are used and updated by the forwards and backwards passes of the backpropogation algorithm. During learning, a tournament selection genetic algorithm is used to select pathways through the neural network for replication and mutation. Pathway fitness is the performance of that pathway measured according to a cost function. We demonstrate successful transfer learning; fixing the parameters along a path learned on task A and re-evolving a new population of paths for task B, allows task B to be learned faster than it could be learned from scratch or after fine-tuning. Paths evolved on task B re-use parts of the optimal path evolved on task A. Positive transfer was demonstrated for binary MNIST, CIFAR, and SVHN supervised learning classification tasks, and a set of Atari and Labyrinth reinforcement learning tasks, suggesting PathNets have general applicability for neural network training. Finally, PathNet also significantly improves the robustness to hyperparameter choices of a parallel asynchronous reinforcement learning algorithm (A3C).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/giulialanzillotta/Zotero/storage/UMZC23LD/Fernando et al. - 2017 - PathNet Evolution Channels Gradient Descent in Super Neural Networks.pdf;/Users/giulialanzillotta/Zotero/storage/P547LG3K/1701.html}
}

@inproceedings{finiSelfsupervisedModelsAre2022,
  title = {Self-Supervised Models Are Continual Learners},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF}} Conference on Computer Vision and Pattern Recognition},
  author = {Fini, Enrico and Da Costa, Victor G Turrisi and {Alameda-Pineda}, Xavier and Ricci, Elisa and Alahari, Karteek and Mairal, Julien},
  year = 2022,
  pages = {9621--9630}
}

@misc{finiSelfSupervisedModelsAre2022a,
  title = {Self-{{Supervised Models}} Are {{Continual Learners}}},
  author = {Fini, Enrico and da Costa, Victor G. Turrisi and {Alameda-Pineda}, Xavier and Ricci, Elisa and Alahari, Karteek and Mairal, Julien},
  year = 2022,
  month = apr,
  number = {arXiv:2112.04215},
  eprint = {2112.04215},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2112.04215},
  urldate = {2025-10-03},
  abstract = {Self-supervised models have been shown to produce comparable or better visual representations than their supervised counterparts when trained offline on unlabeled data at scale. However, their efficacy is catastrophically reduced in a Continual Learning (CL) scenario where data is presented to the model sequentially. In this paper, we show that self-supervised loss functions can be seamlessly converted into distillation mechanisms for CL by adding a predictor network that maps the current state of the representations to their past state. This enables us to devise a framework for Continual self-supervised visual representation Learning that (i) significantly improves the quality of the learned representations, (ii) is compatible with several state-of-the-art self-supervised objectives, and (iii) needs little to no hyperparameter tuning. We demonstrate the effectiveness of our approach empirically by training six popular self-supervised models in various CL settings.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/WP97YPWY/Fini et al. - 2022 - Self-Supervised Models are Continual Learners.pdf;/Users/giulialanzillotta/Zotero/storage/ZA33W3HT/2112.html}
}

@article{fleschContinualTaskLearning2023,
  title = {Continual Task Learning in Natural and Artificial Agents},
  author = {Flesch, Timo and Saxe, Andrew and Summerfield, Christopher},
  year = 2023,
  month = mar,
  journal = {Trends in Neurosciences},
  volume = {46},
  number = {3},
  pages = {199--210},
  issn = {0166-2236},
  doi = {10.1016/j.tins.2022.12.006},
  urldate = {2025-07-01},
  abstract = {How do humans and other animals learn new tasks? A wave of brain recording studies has investigated how neural representations change during task learning, with a focus on how tasks can be acquired and coded in ways that minimise mutual interference. We review recent work that has explored the geometry and dimensionality of neural task representations in neocortex, and computational models that have exploited these findings to understand how the brain may partition knowledge between tasks. We discuss how ideas from machine learning, including those that combine supervised and unsupervised learning, are helping neuroscientists understand how natural tasks are learned and coded in biological brains.},
  keywords = {Hebbian gating,machine learning,neural networks,neuroimaging,representational geometry},
  file = {/Users/giulialanzillotta/Zotero/storage/HM63T7WK/Flesch et al. - 2023 - Continual task learning in natural and artificial agents.pdf;/Users/giulialanzillotta/Zotero/storage/FSZQ4GKF/S0166223622002600.html}
}

@article{fleschContinualTaskLearning2023a,
  title = {Continual Task Learning in Natural and Artificial Agents},
  author = {Flesch, Timo and Saxe, Andrew and Summerfield, Christopher},
  year = 2023,
  month = mar,
  journal = {Trends in Neurosciences},
  volume = {46},
  number = {3},
  pages = {199--210},
  issn = {01662236},
  doi = {10.1016/j.tins.2022.12.006},
  urldate = {2025-07-01},
  langid = {english},
  file = {/Users/giulialanzillotta/Zotero/storage/CSNH3U7F/Flesch et al. - 2023 - Continual task learning in natural and artificial agents.pdf}
}

@misc{ForgettingOrderContinual,
  title = {Forgetting {{Order}} of {{Continual Learning}}: {{Examples That}} Are {{Learned First}} Are {{Forgotten Last}}},
  urldate = {2025-07-29},
  howpublished = {https://arxiv.org/html/2406.09935v1},
  file = {/Users/giulialanzillotta/Zotero/storage/SURAXKZD/2406.html}
}

@misc{fortDeepLearningKernel2020,
  title = {Deep Learning versus Kernel Learning: An Empirical Study of Loss Landscape Geometry and the Time Evolution of the {{Neural Tangent Kernel}}},
  shorttitle = {Deep Learning versus Kernel Learning},
  author = {Fort, Stanislav and Dziugaite, Gintare Karolina and Paul, Mansheej and Kharaghani, Sepideh and Roy, Daniel M. and Ganguli, Surya},
  year = 2020,
  month = oct,
  number = {arXiv:2010.15110},
  eprint = {2010.15110},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2010.15110},
  urldate = {2025-07-28},
  abstract = {In suitably initialized wide networks, small learning rates transform deep neural networks (DNNs) into neural tangent kernel (NTK) machines, whose training dynamics is well-approximated by a linear weight expansion of the network at initialization. Standard training, however, diverges from its linearization in ways that are poorly understood. We study the relationship between the training dynamics of nonlinear deep networks, the geometry of the loss landscape, and the time evolution of a data-dependent NTK. We do so through a large-scale phenomenological analysis of training, synthesizing diverse measures characterizing loss landscape geometry and NTK dynamics. In multiple neural architectures and datasets, we find these diverse measures evolve in a highly correlated manner, revealing a universal picture of the deep learning process. In this picture, deep network training exhibits a highly chaotic rapid initial transient that within 2 to 3 epochs determines the final linearly connected basin of low loss containing the end point of training. During this chaotic transient, the NTK changes rapidly, learning useful features from the training data that enables it to outperform the standard initial NTK by a factor of 3 in less than 3 to 4 epochs. After this rapid chaotic transient, the NTK changes at constant velocity, and its performance matches that of full network training in 15\% to 45\% of training time. Overall, our analysis reveals a striking correlation between a diverse set of metrics over training time, governed by a rapid chaotic to stable transition in the first few epochs, that together poses challenges and opportunities for the development of more accurate theories of deep learning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/A5BZG7UP/Fort et al. - 2020 - Deep learning versus kernel learning an empirical study of loss landscape geometry and the time evo.pdf}
}

@article{frenchCatastrophicForgettingConnectionist1999,
  title = {Catastrophic Forgetting in Connectionist Networks},
  author = {French, Robert M.},
  year = 1999,
  journal = {Trends in Cognitive Sciences},
  volume = {3},
  number = {4},
  pages = {128--135},
  issn = {13646613},
  doi = {10.1016/S1364-6613(99)01294-2},
  isbn = {13646613},
  pmid = {10322466},
  keywords = {Classic}
}

@misc{fujimotoEquivalenceLossFunctions2020,
  title = {An {{Equivalence}} between {{Loss Functions}} and {{Non-Uniform Sampling}} in {{Experience Replay}}},
  author = {Fujimoto, Scott and Meger, David and Precup, Doina},
  year = 2020,
  month = oct,
  number = {arXiv:2007.06049},
  eprint = {2007.06049},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2007.06049},
  urldate = {2025-10-14},
  abstract = {Prioritized Experience Replay (PER) is a deep reinforcement learning technique in which agents learn from transitions sampled with non-uniform probability proportionate to their temporal-difference error. We show that any loss function evaluated with non-uniformly sampled data can be transformed into another uniformly sampled loss function with the same expected gradient. Surprisingly, we find in some environments PER can be replaced entirely by this new loss function without impact to empirical performance. Furthermore, this relationship suggests a new branch of improvements to PER by correcting its uniformly sampled loss function equivalent. We demonstrate the effectiveness of our proposed modifications to PER and the equivalent loss function in several MuJoCo and Atari environments.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/PZZPNPX2/Fujimoto et al. - 2020 - An Equivalence between Loss Functions and Non-Uniform Sampling in Experience Replay.pdf;/Users/giulialanzillotta/Zotero/storage/BRRFPYAM/2007.html}
}

@article{galantiRoleNeuralCollapse2021,
  title = {On the Role of Neural Collapse in Transfer Learning},
  author = {Galanti, Tomer and Gy{\"o}rgy, Andr{\'a}s and Hutter, Marcus},
  year = 2021,
  journal = {arXiv preprint arXiv:2112.15121},
  eprint = {2112.15121},
  archiveprefix = {arXiv}
}

@inproceedings{gaoDDGRContinualLearning2023,
  title = {{{DDGR}}: {{Continual Learning}} with {{Deep Diffusion-based Generative Replay}}},
  shorttitle = {{{DDGR}}},
  booktitle = {Proceedings of the 40th {{International Conference}} on {{Machine Learning}}},
  author = {Gao, Rui and Liu, Weiwei},
  year = 2023,
  month = jul,
  pages = {10744--10763},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2025-10-14},
  abstract = {Popular deep-learning models in the field of image classification suffer from catastrophic forgetting---models will forget previously acquired skills when learning new ones. Generative replay (GR), which typically consists of a generator and a classifier, is an efficient way to mitigate catastrophic forgetting. However, conventional GR methods only focus on a single instruction relationship (generator-to-classifier), where the generator synthesizes samples for previous tasks to instruct the training of the classifier, while ignoring the ways in which the classifier can benefit the generator. In addition, most generative replay methods typically reuse the generated samples to update the generator, which causes the samples regenerated by the generator deviating from the distribution of previous tasks. To overcome these two issues, we propose a novel approach, called deep diffusion-based generative replay (DDGR), which adopts a diffusion model as the generator and calculates an instruction-operator through the classifier to instruct the generation of samples. Extensive experiments in class incremental (CI) and class incremental with repetition (CIR) settings demonstrate the advantages of DDGR. Our code is available at https://github.com/xiaocangshengGR/DDGR.},
  langid = {english},
  file = {/Users/giulialanzillotta/Zotero/storage/XCYTAHD9/Gao and Liu - 2023 - DDGR Continual Learning with Deep Diffusion-based Generative Replay.pdf}
}

@misc{goldwaserUnderstandingSparseFeature2023,
  title = {Understanding {{Sparse Feature Updates}} in {{Deep Networks}} Using {{Iterative Linearisation}}},
  author = {Goldwaser, Adrian and Ge, Hong},
  year = 2023,
  month = oct,
  number = {arXiv:2211.12345},
  eprint = {2211.12345},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2211.12345},
  urldate = {2025-07-28},
  abstract = {Larger and deeper networks generalise well despite their increased capacity to overfit. Understanding why this happens is theoretically and practically important. One recent approach looks at the infinitely wide limits of such networks and their corresponding kernels. However, these theoretical tools cannot fully explain finite networks as the empirical kernel changes significantly during gradient-descent-based training in contrast to infinite networks. In this work, we derive an iterative linearised training method as a novel empirical tool to further investigate this distinction, allowing us to control for sparse (i.e. infrequent) feature updates and quantify the frequency of feature learning needed to achieve comparable performance. We justify iterative linearisation as an interpolation between a finite analog of the infinite width regime, which does not learn features, and standard gradient descent training, which does. Informally, we also show that it is analogous to a damped version of the Gauss-Newton algorithm -- a second-order method. We show that in a variety of cases, iterative linearised training surprisingly performs on par with standard training, noting in particular how much less frequent feature learning is required to achieve comparable performance. We also show that feature learning is essential for good performance. Since such feature learning inevitably causes changes in the NTK kernel, we provide direct negative evidence for the NTK theory, which states the NTK kernel remains constant during training.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/ZZ9S7SZ8/Goldwaser and Ge - 2023 - Understanding Sparse Feature Updates in Deep Networks using Iterative Linearisation.pdf;/Users/giulialanzillotta/Zotero/storage/LN53PTJU/2211.html}
}

@misc{golkarContinualLearningNeural2019,
  title = {Continual {{Learning}} via {{Neural Pruning}}},
  author = {Golkar, Siavash and Kagan, Michael and Cho, Kyunghyun},
  year = 2019,
  month = mar,
  number = {arXiv:1903.04476},
  eprint = {1903.04476},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1903.04476},
  urldate = {2025-10-01},
  abstract = {We introduce Continual Learning via Neural Pruning (CLNP), a new method aimed at lifelong learning in fixed capacity models based on neuronal model sparsification. In this method, subsequent tasks are trained using the inactive neurons and filters of the sparsified network and cause zero deterioration to the performance of previous tasks. In order to deal with the possible compromise between model sparsity and performance, we formalize and incorporate the concept of graceful forgetting: the idea that it is preferable to suffer a small amount of forgetting in a controlled manner if it helps regain network capacity and prevents uncontrolled loss of performance during the training of future tasks. CLNP also provides simple continual learning diagnostic tools in terms of the number of free neurons left for the training of future tasks as well as the number of neurons that are being reused. In particular, we see in experiments that CLNP verifies and automatically takes advantage of the fact that the features of earlier layers are more transferable. We show empirically that CLNP leads to significantly improved results over current weight elasticity based methods.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Quantitative Biology - Neurons and Cognition,Statistics - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/5UYHWRWV/Golkar et al. - 2019 - Continual Learning via Neural Pruning.pdf;/Users/giulialanzillotta/Zotero/storage/EGAVHK3U/1903.html}
}

@misc{graldiImportanceBeingLazy2025,
  title = {The {{Importance}} of {{Being Lazy}}: {{Scaling Limits}} of {{Continual Learning}}},
  shorttitle = {The {{Importance}} of {{Being Lazy}}},
  author = {Graldi, Jacopo and Breccia, Alessandro and Lanzillotta, Giulia and Hofmann, Thomas and Noci, Lorenzo},
  year = 2025,
  month = jun,
  number = {arXiv:2506.16884},
  eprint = {2506.16884},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2506.16884},
  urldate = {2025-07-01},
  abstract = {Despite recent efforts, neural networks still struggle to learn in non-stationary environments, and our understanding of catastrophic forgetting (CF) is far from complete. In this work, we perform a systematic study on the impact of model scale and the degree of feature learning in continual learning. We reconcile existing contradictory observations on scale in the literature, by differentiating between lazy and rich training regimes through a variable parameterization of the architecture. We show that increasing model width is only beneficial when it reduces the amount of feature learning, yielding more laziness. Using the framework of dynamical mean field theory, we then study the infinite width dynamics of the model in the feature learning regime and characterize CF, extending prior theoretical results limited to the lazy regime. We study the intricate relationship between feature learning, task non-stationarity, and forgetting, finding that high feature learning is only beneficial with highly similar tasks. We identify a transition modulated by task similarity where the model exits an effectively lazy regime with low forgetting to enter a rich regime with significant forgetting. Finally, our findings reveal that neural networks achieve optimal performance at a critical level of feature learning, which depends on task non-stationarity and transfers across model scales. This work provides a unified perspective on the role of scale and feature learning in continual learning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/PFDW8UH9/Graldi et al. - 2025 - The Importance of Being Lazy Scaling Limits of Continual Learning.pdf;/Users/giulialanzillotta/Zotero/storage/KPDMFRAS/2506.html}
}

@misc{graldiImportanceBeingLazy2025a,
  title = {The {{Importance}} of {{Being Lazy}}: {{Scaling Limits}} of {{Continual Learning}}},
  shorttitle = {The {{Importance}} of {{Being Lazy}}},
  author = {Graldi, Jacopo and Breccia, Alessandro and Lanzillotta, Giulia and Hofmann, Thomas and Noci, Lorenzo},
  year = 2025,
  month = aug,
  number = {arXiv:2506.16884},
  eprint = {2506.16884},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2506.16884},
  urldate = {2025-10-03},
  abstract = {Despite recent efforts, neural networks still struggle to learn in non-stationary environments, and our understanding of catastrophic forgetting (CF) is far from complete. In this work, we perform a systematic study on the impact of model scale and the degree of feature learning in continual learning. We reconcile existing contradictory observations on scale in the literature, by differentiating between lazy and rich training regimes through a variable parameterization of the architecture. We show that increasing model width is only beneficial when it reduces the amount of feature learning, yielding more laziness. Using the framework of dynamical mean field theory, we then study the infinite width dynamics of the model in the feature learning regime and characterize CF, extending prior theoretical results limited to the lazy regime. We study the intricate relationship between feature learning, task non-stationarity, and forgetting, finding that high feature learning is only beneficial with highly similar tasks. We identify a transition modulated by task similarity where the model exits an effectively lazy regime with low forgetting to enter a rich regime with significant forgetting. Finally, our findings reveal that neural networks achieve optimal performance at a critical level of feature learning, which depends on task non-stationarity and transfers across model scales. This work provides a unified perspective on the role of scale and feature learning in continual learning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/42RT8UWM/Graldi et al. - 2025 - The Importance of Being Lazy Scaling Limits of Continual Learning.pdf;/Users/giulialanzillotta/Zotero/storage/5ZJ64I53/2506.html}
}

@misc{graldiImportanceBeingLazy2025b,
  title = {The {{Importance}} of {{Being Lazy}}: {{Scaling Limits}} of {{Continual Learning}}},
  shorttitle = {The {{Importance}} of {{Being Lazy}}},
  author = {Graldi, Jacopo and Breccia, Alessandro and Lanzillotta, Giulia and Hofmann, Thomas and Noci, Lorenzo},
  year = 2025,
  month = aug,
  number = {arXiv:2506.16884},
  eprint = {2506.16884},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2506.16884},
  urldate = {2025-10-03},
  abstract = {Despite recent efforts, neural networks still struggle to learn in non-stationary environments, and our understanding of catastrophic forgetting (CF) is far from complete. In this work, we perform a systematic study on the impact of model scale and the degree of feature learning in continual learning. We reconcile existing contradictory observations on scale in the literature, by differentiating between lazy and rich training regimes through a variable parameterization of the architecture. We show that increasing model width is only beneficial when it reduces the amount of feature learning, yielding more laziness. Using the framework of dynamical mean field theory, we then study the infinite width dynamics of the model in the feature learning regime and characterize CF, extending prior theoretical results limited to the lazy regime. We study the intricate relationship between feature learning, task non-stationarity, and forgetting, finding that high feature learning is only beneficial with highly similar tasks. We identify a transition modulated by task similarity where the model exits an effectively lazy regime with low forgetting to enter a rich regime with significant forgetting. Finally, our findings reveal that neural networks achieve optimal performance at a critical level of feature learning, which depends on task non-stationarity and transfers across model scales. This work provides a unified perspective on the role of scale and feature learning in continual learning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/GW5ZI76A/Graldi et al. - 2025 - The Importance of Being Lazy Scaling Limits of Continual Learning.pdf;/Users/giulialanzillotta/Zotero/storage/9JFACMD9/2506.html}
}

@article{grunwaldMinimumDescriptionLength2005,
  title = {Minimum Description Length Tutorial},
  author = {Gr{\"u}nwald, Peter},
  year = 2005,
  journal = {Advances in minimum description length: Theory and applications},
  volume = {5},
  pages = {1--80},
  publisher = {MIT press 5 Cambridge Center, Cambridge, MA 02412}
}

@inproceedings{gunasekarCharacterizingImplicitBias2018,
  title = {Characterizing Implicit Bias in Terms of Optimization Geometry},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Gunasekar, Suriya and Lee, Jason and Soudry, Daniel and Srebro, Nathan},
  year = 2018,
  pages = {1832--1841},
  publisher = {PMLR}
}

@misc{guoComprehensiveSurveyContinual2025,
  title = {A {{Comprehensive Survey}} on {{Continual Learning}} in {{Generative Models}}},
  author = {Guo, Haiyang and Zeng, Fanhu and Zhu, Fei and Wang, Jiayi and Wang, Xukai and Zhou, Jingang and Zhao, Hongbo and Liu, Wenzhuo and Ma, Shijie and Wang, Da-Han and Zhang, Xu-Yao and Liu, Cheng-Lin},
  year = 2025,
  month = jun,
  number = {arXiv:2506.13045},
  eprint = {2506.13045},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2506.13045},
  urldate = {2025-10-14},
  abstract = {The rapid advancement of generative models has enabled modern AI systems to comprehend and produce highly sophisticated content, even achieving human-level performance in specific domains. However, these models remain fundamentally constrained by catastrophic forgetting - a persistent challenge where adapting to new tasks typically leads to significant degradation in performance on previously learned tasks. To address this practical limitation, numerous approaches have been proposed to enhance the adaptability and scalability of generative models in real-world applications. In this work, we present a comprehensive survey of continual learning methods for mainstream generative models, including large language models, multimodal large language models, vision language action models, and diffusion models. Drawing inspiration from the memory mechanisms of the human brain, we systematically categorize these approaches into three paradigms: architecture-based, regularization-based, and replay-based methods, while elucidating their underlying methodologies and motivations. We further analyze continual learning setups for different generative models, including training objectives, benchmarks, and core backbones, offering deeper insights into the field. The project page of this paper is available at https://github.com/Ghy0501/Awesome-Continual-Learning-in-Generative-Models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/J7GGAN9N/Guo et al. - 2025 - A Comprehensive Survey on Continual Learning in Generative Models.pdf;/Users/giulialanzillotta/Zotero/storage/JB5EXXGU/2506.html}
}

@misc{guoEfficientContinualPretraining2024,
  title = {Efficient {{Continual Pre-training}} by {{Mitigating}} the {{Stability Gap}}},
  author = {Guo, Yiduo and Fu, Jie and Zhang, Huishuai and Zhao, Dongyan and Shen, Yikang},
  year = 2024,
  month = jun,
  number = {arXiv:2406.14833},
  eprint = {2406.14833},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2406.14833},
  urldate = {2025-09-16},
  abstract = {Continual pre-training has increasingly become the predominant approach for adapting Large Language Models (LLMs) to new domains. This process involves updating the pre-trained LLM with a corpus from a new domain, resulting in a shift in the training distribution. To study the behavior of LLMs during this shift, we measured the model's performance throughout the continual pre-training process. we observed a temporary performance drop at the beginning, followed by a recovery phase, a phenomenon known as the "stability gap," previously noted in vision models classifying new classes. To address this issue and enhance LLM performance within a fixed compute budget, we propose three effective strategies: (1) Continually pre-training the LLM on a subset with a proper size for multiple epochs, resulting in faster performance recovery than pre-training the LLM on a large corpus in a single epoch; (2) Pre-training the LLM only on high-quality sub-corpus, which rapidly boosts domain performance; and (3) Using a data mixture similar to the pre-training data to reduce distribution gap. We conduct various experiments on Llama-family models to validate the effectiveness of our strategies in both medical continual pre-training and instruction tuning. For example, our strategies improve the average medical task performance of the OpenLlama-3B model from 36.2\% to 40.7\% with only 40\% of the original training budget and enhance the average general task performance without causing forgetting. Furthermore, we apply our strategies to the Llama-3-8B model. The resulting model, Llama-3-Physician, achieves the best medical performance among current open-source models, and performs comparably to or even better than GPT-4 on several medical benchmarks. We release our models at {\textbackslash}url\{https://huggingface.co/YiDuo1999/Llama-3-Physician-8B-Instruct\}.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/giulialanzillotta/Zotero/storage/MDVK4VDB/Guo et al. - 2024 - Efficient Continual Pre-training by Mitigating the Stability Gap.pdf;/Users/giulialanzillotta/Zotero/storage/CUW4QN69/2406.html}
}

@misc{gur-ariGradientDescentHappens2018,
  title = {Gradient {{Descent Happens}} in a {{Tiny Subspace}}},
  author = {{Gur-Ari}, Guy and Roberts, Daniel A. and Dyer, Ethan},
  year = 2018,
  month = dec,
  number = {arXiv:1812.04754},
  eprint = {1812.04754},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1812.04754},
  urldate = {2025-04-18},
  abstract = {We show that in a variety of large-scale deep learning scenarios the gradient dynamically converges to a very small subspace after a short period of training. The subspace is spanned by a few top eigenvectors of the Hessian (equal to the number of classes in the dataset), and is mostly preserved over long periods of training. A simple argument then suggests that gradient descent may happen mostly in this subspace. We give an example of this effect in a solvable model of classification, and we comment on possible implications for optimization and learning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/Y5KYFLX6/Gur-Ari et al. - 2018 - Gradient Descent Happens in a Tiny Subspace.pdf;/Users/giulialanzillotta/Zotero/storage/SL5JW6CS/1812.html}
}

@misc{gururanganDEMixLayersDisentangling2021,
  title = {{{DEMix Layers}}: {{Disentangling Domains}} for {{Modular Language Modeling}}},
  shorttitle = {{{DEMix Layers}}},
  author = {Gururangan, Suchin and Lewis, Mike and Holtzman, Ari and Smith, Noah A. and Zettlemoyer, Luke},
  year = 2021,
  month = aug,
  number = {arXiv:2108.05036},
  eprint = {2108.05036},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2108.05036},
  urldate = {2025-10-01},
  abstract = {We introduce a new domain expert mixture (DEMix) layer that enables conditioning a language model (LM) on the domain of the input text. A DEMix layer is a collection of expert feedforward networks, each specialized to a domain, that makes the LM modular: experts can be mixed, added or removed after initial training. Extensive experiments with autoregressive transformer LMs (up to 1.3B parameters) show that DEMix layers reduce test-time perplexity, increase training efficiency, and enable rapid adaptation with little overhead. We show that mixing experts during inference, using a parameter-free weighted ensemble, allows the model to better generalize to heterogeneous or unseen domains. We also show that experts can be added to iteratively incorporate new domains without forgetting older ones, and that experts can be removed to restrict access to unwanted domains, without additional training. Overall, these results demonstrate benefits of explicitly conditioning on textual domains during language modeling.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/giulialanzillotta/Zotero/storage/L77FP9VH/Gururangan et al. - 2021 - DEMix Layers Disentangling Domains for Modular Language Modeling.pdf;/Users/giulialanzillotta/Zotero/storage/33KUTQFB/2108.html}
}

@misc{haasLinkingNeuralCollapse2023,
  title = {Linking {{Neural Collapse}} and {{L2 Normalization}} with {{Improved Out-of-Distribution Detection}} in {{Deep Neural Networks}}},
  author = {Haas, Jarrod and Yolland, William and Rabus, Bernhard},
  year = 2023,
  month = jan,
  number = {arXiv:2209.08378},
  eprint = {2209.08378},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2209.08378},
  urldate = {2025-09-16},
  abstract = {We propose a simple modification to standard ResNet architectures--L2 normalization over feature space--that substantially improves out-of-distribution (OoD) performance on the previously proposed Deep Deterministic Uncertainty (DDU) benchmark. We show that this change also induces early Neural Collapse (NC), an effect linked to better OoD performance. Our method achieves comparable or superior OoD detection scores and classification accuracy in a small fraction of the training time of the benchmark. Additionally, it substantially improves worst case OoD performance over multiple, randomly initialized models. Though we do not suggest that NC is the sole mechanism or a comprehensive explanation for OoD behaviour in deep neural networks (DNN), we believe NC's simple mathematical and geometric structure can provide a framework for analysis of this complex phenomenon in future work.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/C5GP9HLF/Haas et al. - 2023 - Linking Neural Collapse and L2 Normalization with Improved Out-of-Distribution Detection in Deep Neu.pdf;/Users/giulialanzillotta/Zotero/storage/RBKIISYL/2209.html}
}

@article{hacohenForgettingOrderContinual2024,
  title = {Forgetting {{Order}} of {{Continual Learning}}: {{Examples That}} Are {{Learned First}} Are {{Forgotten Last}}},
  author = {Hacohen, Guy and Tuytelaars, Tinne},
  year = 2024,
  journal = {arXiv preprint arXiv:2406.09935},
  eprint = {2406.09935},
  archiveprefix = {arXiv}
}

@article{hadsellEmbracingChangeContinual2020,
  title = {Embracing {{Change}}: {{Continual Learning}} in {{Deep Neural Networks}}},
  author = {Hadsell, Raia and Rao, Dushyant and Rusu, Andrei A and Pascanu, Razvan},
  year = 2020,
  journal = {Trends in Cognitive Sciences},
  volume = {24},
  number = {12},
  pages = {1028--1040},
  publisher = {Elsevier},
  doi = {10.1016/j.tics.2020.09.004}
}

@article{hanNeuralCollapseMse2021,
  title = {Neural Collapse under Mse Loss: {{Proximity}} to and Dynamics on the Central Path},
  author = {Han, {\relax XY} and Papyan, Vardan and Donoho, David L},
  year = 2021,
  journal = {arXiv preprint arXiv:2106.02073},
  eprint = {2106.02073},
  archiveprefix = {arXiv}
}

@article{harunWhatVariablesAffect,
  title = {What {{Variables Affect Out-of-Distribution Generalization}} in {{Pretrained Models}}?},
  author = {Harun, Yousuf and Lee, Kyungbok and Gallardo, Jhair and Krishnan, Giri and Kanan, Christopher},
  abstract = {Embeddings produced by pre-trained deep neural networks (DNNs) are widely used; however, their efficacy for downstream tasks can vary widely. We study the factors influencing transferability and out-of-distribution (OOD) generalization of pre-trained DNN embeddings through the lens of the tunnel effect hypothesis, which is closely related to intermediate neural collapse. This hypothesis suggests that deeper DNN layers compress representations and hinder OOD generalization. Contrary to earlier work, our experiments show this is not a universal phenomenon. We comprehensively investigate the impact of DNN architecture, training data, image resolution, and augmentations on transferability. We identify that training with high-resolution datasets containing many classes greatly reduces representation compression and improves transferability. Our results emphasize the danger of generalizing findings from toy datasets to broader contexts.},
  langid = {english},
  file = {/Users/giulialanzillotta/Zotero/storage/9ZEZUG5G/Harun et al. - What Variables Affect Out-of-Distribution Generalization in Pretrained Models.pdf}
}

@article{hayesReplayDeepLearning2021,
  title = {Replay in {{Deep Learning}}: {{Current Approaches}} and {{Missing Biological Elements}}},
  shorttitle = {Replay in {{Deep Learning}}},
  author = {Hayes, Tyler L. and Krishnan, Giri P. and Bazhenov, Maxim and Siegelmann, Hava T. and Sejnowski, Terrence J. and Kanan, Christopher},
  year = 2021,
  month = oct,
  journal = {Neural computation},
  volume = {33},
  number = {11},
  pages = {2908--2950},
  issn = {0899-7667},
  doi = {10.1162/neco_a_01433},
  urldate = {2025-09-30},
  abstract = {Replay is the reactivation of one or more neural patterns, which are similar to the activation patterns experienced during past waking experiences. Replay was first observed in biological neural networks during sleep, and it is now thought to play a critical role in memory formation, retrieval, and consolidation. Replay-like mechanisms have been incorporated into deep artificial neural networks that learn over time to avoid catastrophic forgetting of previous knowledge. Replay algorithms have been successfully used in a wide range of deep learning methods within supervised, unsupervised, and reinforcement learning paradigms. In this paper, we provide the first comprehensive comparison between replay in the mammalian brain and replay in artificial neural networks. We identify multiple aspects of biological replay that are missing in deep learning systems and hypothesize how they could be utilized to improve artificial neural networks.},
  pmcid = {PMC9074752},
  pmid = {34474476},
  file = {/Users/giulialanzillotta/Zotero/storage/NWDLVGDJ/Hayes et al. - 2021 - Replay in Deep Learning Current Approaches and Missing Biological Elements.pdf}
}

@article{hazanIntroductionOnlineConvex2016,
  title = {Introduction to Online Convex Optimization},
  author = {Hazan, Elad and others},
  year = 2016,
  journal = {Foundations and Trends{\textregistered} in Optimization},
  volume = {2},
  number = {3-4},
  pages = {157--325},
  publisher = {Now Publishers, Inc.}
}

@misc{hazanIntroductionOnlineConvex2023,
  title = {Introduction to {{Online Convex Optimization}}},
  author = {Hazan, Elad},
  year = 2023,
  month = aug,
  number = {arXiv:1909.05207},
  eprint = {1909.05207},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1909.05207},
  urldate = {2025-04-18},
  abstract = {This manuscript portrays optimization as a process. In many practical applications the environment is so complex that it is infeasible to lay out a comprehensive theoretical model and use classical algorithmic theory and mathematical optimization. It is necessary as well as beneficial to take a robust approach, by applying an optimization method that learns as one goes along, learning from experience as more aspects of the problem are observed. This view of optimization as a process has become prominent in varied fields and has led to some spectacular success in modeling and systems that are now part of our daily lives.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/RGBTLLLE/Hazan - 2023 - Introduction to Online Convex Optimization.pdf;/Users/giulialanzillotta/Zotero/storage/23D4ZTW5/1909.html}
}

@article{heDAKDDifficultyAwareKnowledge,
  title = {{{DA-KD}}: {{Difficulty-Aware Knowledge Distillation}} for  {{Efficient Large Language Models}}},
  author = {He, Changyi and Ding, Yifu and Guo, Jinyang and Gong, Ruihao and Qin, Haotong and Liu, Xianglong},
  abstract = {Although knowledge distillation (KD) is an effective approach to improve the performance of a smaller LLM (i.e., the student model) by transferring knowledge from a large LLM (i.e., the teacher model), it still suffers from high training cost. Existing LLM distillation methods ignore the difficulty difference among different samples, making the distillation of easy samples unnecessary. This leads to high distillation cost. In this paper, we propose difficulty-aware knowledge distillation (DA-KD) framework for efficient knowledge distillation, in which we dynamically adjust the distillation dataset based on the difficulty of samples. We further observe existing KD loss cannot perform well when most of samples are difficult in the distillation dataset because of unstable optimization and the neglect of hard samples. Therefore, we also propose a new KD loss called bidirectional discrepancy loss (BDL) for effective KD. Extensive experiments demonstrate that our DA-KD framework is effective and efficient. Without bells and whistles, DA-KD can outperform existing state-of-the-art KD methods by 2\% with half training cost and even surpass the teacher model with 4.7{\texttimes} compression.},
  langid = {english},
  file = {/Users/giulialanzillotta/Zotero/storage/I5QCCZ48/He et al. - DA-KD Difficulty-Aware Knowledge Distillation for  Efficient Large Language Models.pdf}
}

@inproceedings{heDAKDDifficultyAwareKnowledge2025,
  title = {{{DA-KD}}: {{Difficulty-Aware Knowledge Distillation}} for {{Efficient Large Language Models}}},
  shorttitle = {{{DA-KD}}},
  booktitle = {Forty-Second {{International Conference}} on {{Machine Learning}}},
  author = {He, Changyi and Ding, Yifu and Guo, Jinyang and Gong, Ruihao and Qin, Haotong and Liu, Xianglong},
  year = 2025,
  month = jun,
  urldate = {2025-10-13},
  abstract = {Although knowledge distillation (KD) is an effective approach to improve the performance of a smaller LLM (i.e., the student model) by transferring knowledge from a large LLM (i.e., the teacher model), it still suffers from high training cost. Existing LLM distillation methods ignore the difficulty difference among different samples, making the distillation of easy samples unnecessary. This leads to high distillation cost. In this paper, we propose difficulty-aware knowledge distillation (DA-KD) framework for efficient knowledge distillation, in which we dynamically adjust the distillation dataset based on the difficulty of samples. We further observe existing KD loss cannot perform well when most of samples are difficult in the distillation dataset because of unstable optimization and the neglect of hard samples. Therefore, we also propose a new KD loss called bidirectional discrepancy loss (BDL) for effective KD. Extensive experiments demonstrate that our DA-KD framework is effective and efficient. Without bells and whistles, DA-KD can outperform existing state-of-the-art KD methods by 2{\textbackslash}\% with half training cost and even surpass the teacher model with 4.7\${\textbackslash}times\$ compression.},
  langid = {english},
  file = {/Users/giulialanzillotta/Zotero/storage/HGUAEPNU/He et al. - 2025 - DA-KD Difficulty-Aware Knowledge Distillation for Efficient Large Language Models.pdf}
}

@misc{heDeepResidualLearning2015,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  year = 2015
}

@misc{hendrycksBaselineDetectingMisclassified2018,
  title = {A {{Baseline}} for {{Detecting Misclassified}} and {{Out-of-Distribution Examples}} in {{Neural Networks}}},
  author = {Hendrycks, Dan and Gimpel, Kevin},
  year = 2018,
  month = oct,
  number = {arXiv:1610.02136},
  eprint = {1610.02136},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1610.02136},
  urldate = {2025-09-16},
  abstract = {We consider the two related problems of detecting if an example is misclassified or out-of-distribution. We present a simple baseline that utilizes probabilities from softmax distributions. Correctly classified examples tend to have greater maximum softmax probabilities than erroneously classified and out-of-distribution examples, allowing for their detection. We assess performance by defining several tasks in computer vision, natural language processing, and automatic speech recognition, showing the effectiveness of this baseline across all. We then show the baseline can sometimes be surpassed, demonstrating the room for future research on these underexplored detection tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/giulialanzillotta/Zotero/storage/ZAIHAEEF/Hendrycks and Gimpel - 2018 - A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks.pdf;/Users/giulialanzillotta/Zotero/storage/F5PZSGQU/1610.html}
}

@inproceedings{herbsterTrackingBestRegressor1998,
  title = {Tracking the Best Regressor},
  booktitle = {Proceedings of the Eleventh Annual Conference on {{Computational}} Learning Theory},
  author = {Herbster, Mark and Warmuth, Manfred K},
  year = 1998,
  pages = {24--31}
}

@article{hessKnowledgeAccumulationContinually2023,
  title = {Knowledge Accumulation in Continually Learned Representations and the Issue of Feature Forgetting},
  author = {Hess, Timm and Verwimp, Eli and {van de Ven}, Gido M and Tuytelaars, Tinne},
  year = 2023,
  journal = {arXiv preprint arXiv:2304.00933},
  eprint = {2304.00933},
  archiveprefix = {arXiv}
}

@misc{hessKnowledgeAccumulationContinually2024,
  title = {Knowledge {{Accumulation}} in {{Continually Learned Representations}} and the {{Issue}} of {{Feature Forgetting}}},
  author = {Hess, Timm and Verwimp, Eli and van de Ven, Gido M. and Tuytelaars, Tinne},
  year = 2024,
  month = jun,
  number = {arXiv:2304.00933},
  eprint = {2304.00933},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2304.00933},
  urldate = {2025-10-01},
  abstract = {Continual learning research has shown that neural networks suffer from catastrophic forgetting "at the output level", but it is debated whether this is also the case at the level of learned representations. Multiple recent studies ascribe representations a certain level of innate robustness against forgetting -- that they only forget minimally in comparison with forgetting at the output level. We revisit and expand upon the experiments that revealed this difference in forgetting and illustrate the coexistence of two phenomena that affect the quality of continually learned representations: knowledge accumulation and feature forgetting. Taking both aspects into account, we show that, even though forgetting in the representation (i.e. feature forgetting) can be small in absolute terms, when measuring relative to how much was learned during a task, forgetting in the representation tends to be just as catastrophic as forgetting at the output level. Next we show that this feature forgetting is problematic as it substantially slows down the incremental learning of good general representations (i.e. knowledge accumulation). Finally, we study how feature forgetting and knowledge accumulation are affected by different types of continual learning methods.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/ILH3S3NE/Hess et al. - 2024 - Knowledge Accumulation in Continually Learned Representations and the Issue of Feature Forgetting.pdf;/Users/giulialanzillotta/Zotero/storage/QM9UQG7T/2304.html}
}

@article{heTaskAgnosticContinual2019,
  title = {Task {{Agnostic Continual Learning}} via {{Meta Learning}}},
  author = {He, Xu and Sygnowski, Jakub and Galashov, Alexandre and Rusu, Andrei A. and Teh, Yee Whye and Pascanu, Razvan},
  year = 2019,
  journal = {ArXiv},
  volume = {abs/1906.05201},
  keywords = {"Continual-Meta Learning"}
}

@misc{heUnifiedViewParameterEfficient2022,
  title = {Towards a {{Unified View}} of {{Parameter-Efficient Transfer Learning}}},
  author = {He, Junxian and Zhou, Chunting and Ma, Xuezhe and {Berg-Kirkpatrick}, Taylor and Neubig, Graham},
  year = 2022,
  month = feb,
  number = {arXiv:2110.04366},
  eprint = {2110.04366},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2110.04366},
  urldate = {2025-10-03},
  abstract = {Fine-tuning large pre-trained language models on downstream tasks has become the de-facto learning paradigm in NLP. However, conventional approaches fine-tune all the parameters of the pre-trained model, which becomes prohibitive as the model size and the number of tasks grow. Recent work has proposed a variety of parameter-efficient transfer learning methods that only fine-tune a small number of (extra) parameters to attain strong performance. While effective, the critical ingredients for success and the connections among the various methods are poorly understood. In this paper, we break down the design of state-of-the-art parameter-efficient transfer learning methods and present a unified framework that establishes connections between them. Specifically, we re-frame them as modifications to specific hidden states in pre-trained models, and define a set of design dimensions along which different methods vary, such as the function to compute the modification and the position to apply the modification. Through comprehensive empirical studies across machine translation, text summarization, language understanding, and text classification benchmarks, we utilize the unified view to identify important design choices in previous methods. Furthermore, our unified framework enables the transfer of design elements across different approaches, and as a result we are able to instantiate new parameter-efficient fine-tuning methods that tune less parameters than previous methods while being more effective, achieving comparable results to fine-tuning all parameters on all four tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/HLIWA67G/He et al. - 2022 - Towards a Unified View of Parameter-Efficient Transfer Learning.pdf;/Users/giulialanzillotta/Zotero/storage/B7TKDTX9/2110.html}
}

@misc{hickokScalableStrategiesContinual2025,
  title = {Scalable {{Strategies}} for {{Continual Learning}} with {{Replay}}},
  author = {Hickok, Truman},
  year = 2025,
  month = may,
  number = {arXiv:2505.12512},
  eprint = {2505.12512},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2505.12512},
  urldate = {2025-09-30},
  abstract = {Future deep learning models will be distinguished by systems that perpetually learn through interaction, imagination, and cooperation, blurring the line between training and inference. This makes continual learning a critical challenge, as methods that efficiently maximize bidirectional transfer across learning trajectories will be essential. Replay is on track to play a foundational role in continual learning, allowing models to directly reconcile new information with past knowledge. In practice, however, replay is quite unscalable, doubling the cost of continual learning when applied naively. Moreover, the continual learning literature has not fully synchronized with the multi-task fine-tuning literature, having not fully integrated highly scalable techniques like model merging and low rank adaptation into a replay-enabled toolset that can produce a unified model in the face of many sequential tasks. In this paper, we begin by applying and analyzing low rank adaptation in a continual learning setting. Next, we introduce consolidation, a phasic approach to replay which leads to up to 55{\textbackslash}\% less replay samples being needed for a given performance target. Then, we propose sequential merging, an offshoot of task arithmetic which is tailored to the continual learning setting and is shown to work well in combination with replay. Finally, we demonstrate that the developed strategies can operate synergistically, resulting in a highly scalable toolset that outperforms standalone variants.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/NC2BHS9C/Hickok - 2025 - Scalable Strategies for Continual Learning with Replay.pdf;/Users/giulialanzillotta/Zotero/storage/ZHMPV77Q/2505.html}
}

@misc{hintonDistillingKnowledgeNeural2015,
  title = {Distilling the {{Knowledge}} in a {{Neural Network}}},
  author = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
  year = 2015
}

@misc{hoiOnlineLearningComprehensive2018,
  title = {Online {{Learning}}: {{A Comprehensive Survey}}},
  author = {Hoi, Steven C. H. and Sahoo, Doyen and Lu, Jing and Zhao, Peilin},
  year = 2018
}

@misc{hoiOnlineLearningComprehensive2018a,
  title = {Online {{Learning}}: {{A Comprehensive Survey}}},
  author = {Hoi, Steven C. H. and Sahoo, Doyen and Lu, Jing and Zhao, Peilin},
  year = 2018
}

@article{hongNeuralCollapseUnconstrained2023,
  title = {Neural Collapse for Unconstrained Feature Model under Cross-Entropy Loss with Imbalanced Data},
  author = {Hong, Wanli and Ling, Shuyang},
  year = 2023,
  journal = {arXiv preprint arXiv:2309.09725},
  eprint = {2309.09725},
  archiveprefix = {arXiv}
}

@misc{hongNeuralCollapseUnconstrained2023a,
  title = {Neural {{Collapse}} for {{Unconstrained Feature Model}} under {{Cross-entropy Loss}} with {{Imbalanced Data}}},
  author = {Hong, Wanli and Ling, Shuyang},
  year = 2023,
  month = oct,
  number = {arXiv:2309.09725},
  eprint = {2309.09725},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2309.09725},
  urldate = {2025-09-16},
  abstract = {Recent years have witnessed the huge success of deep neural networks (DNNs) in various tasks of computer vision and text processing. Interestingly, these DNNs with massive number of parameters share similar structural properties on their feature representation and last-layer classifier at terminal phase of training (TPT). Specifically, if the training data are balanced (each class shares the same number of samples), it is observed that the feature vectors of samples from the same class converge to their corresponding in-class mean features and their pairwise angles are the same. This fascinating phenomenon is known as Neural Collapse (N C), first termed by Papyan, Han, and Donoho in 2019. Many recent works manage to theoretically explain this phenomenon by adopting so-called unconstrained feature model (UFM). In this paper, we study the extension of N C phenomenon to the imbalanced data under cross-entropy loss function in the context of unconstrained feature model. Our contribution is multi-fold compared with the state-of-the-art results: (a) we show that the feature vectors exhibit collapse phenomenon, i.e., the features within the same class collapse to the same mean vector; (b) the mean feature vectors no longer form an equiangular tight frame. Instead, their pairwise angles depend on the sample size; (c) we also precisely characterize the sharp threshold on which the minority collapse (the feature vectors of the minority groups collapse to one single vector) will take place; (d) finally, we argue that the effect of the imbalance in datasize diminishes as the sample size grows. Our results provide a complete picture of the N C under the cross-entropy loss for the imbalanced data. Numerical experiments confirm our theoretical analysis.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/FZC3DAD3/Hong and Ling - 2023 - Neural Collapse for Unconstrained Feature Model under Cross-entropy Loss with Imbalanced Data.pdf;/Users/giulialanzillotta/Zotero/storage/JE3JI5E5/2309.html}
}

@misc{hongNeuralCollapseUnconstrained2023b,
  title = {Neural {{Collapse}} for {{Unconstrained Feature Model}} under {{Cross-entropy Loss}} with {{Imbalanced Data}}},
  author = {Hong, Wanli and Ling, Shuyang},
  year = 2023,
  month = oct,
  number = {arXiv:2309.09725},
  eprint = {2309.09725},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2309.09725},
  urldate = {2025-09-16},
  abstract = {Recent years have witnessed the huge success of deep neural networks (DNNs) in various tasks of computer vision and text processing. Interestingly, these DNNs with massive number of parameters share similar structural properties on their feature representation and last-layer classifier at terminal phase of training (TPT). Specifically, if the training data are balanced (each class shares the same number of samples), it is observed that the feature vectors of samples from the same class converge to their corresponding in-class mean features and their pairwise angles are the same. This fascinating phenomenon is known as Neural Collapse (N C), first termed by Papyan, Han, and Donoho in 2019. Many recent works manage to theoretically explain this phenomenon by adopting so-called unconstrained feature model (UFM). In this paper, we study the extension of N C phenomenon to the imbalanced data under cross-entropy loss function in the context of unconstrained feature model. Our contribution is multi-fold compared with the state-of-the-art results: (a) we show that the feature vectors exhibit collapse phenomenon, i.e., the features within the same class collapse to the same mean vector; (b) the mean feature vectors no longer form an equiangular tight frame. Instead, their pairwise angles depend on the sample size; (c) we also precisely characterize the sharp threshold on which the minority collapse (the feature vectors of the minority groups collapse to one single vector) will take place; (d) finally, we argue that the effect of the imbalance in datasize diminishes as the sample size grows. Our results provide a complete picture of the N C under the cross-entropy loss for the imbalanced data. Numerical experiments confirm our theoretical analysis.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/UHH3NCFE/Hong and Ling - 2023 - Neural Collapse for Unconstrained Feature Model under Cross-entropy Loss with Imbalanced Data.pdf;/Users/giulialanzillotta/Zotero/storage/3WJIGMME/2309.html}
}

@inproceedings{houLearningUnifiedClassifier2019,
  title = {Learning a Unified Classifier Incrementally via Rebalancing},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF}} Conference on Computer Vision and Pattern Recognition},
  author = {Hou, Saihui and Pan, Xinyu and Loy, Chen Change and Wang, Zilei and Lin, Dahua},
  year = 2019,
  pages = {831--839}
}

@inproceedings{houLearningUnifiedClassifier2019a,
  title = {Learning a {{Unified Classifier Incrementally}} via {{Rebalancing}}},
  booktitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Hou, Saihui and Pan, Xinyu and Loy, Chen Change and Wang, Zilei and Lin, Dahua},
  year = 2019,
  month = jun,
  pages = {831--839},
  publisher = {IEEE},
  address = {Long Beach, CA, USA},
  doi = {10.1109/CVPR.2019.00092},
  urldate = {2025-09-30},
  abstract = {Conventionally, deep neural networks are trained offline, relying on a large dataset prepared in advance. This paradigm is often challenged in real-world applications, e.g. online services that involve continuous streams of incoming data. Recently, incremental learning receives increasing attention, and is considered as a promising solution to the practical challenges mentioned above. However, it has been observed that incremental learning is subject to a fundamental difficulty -- catastrophic forgetting, namely adapting a model to new data often results in severe performance degradation on previous tasks or classes. Our study reveals that the imbalance between previous and new data is a crucial cause to this problem. In this work, we develop a new framework for incrementally learning a unified classifier, i.e. a classifier that treats both old and new classes uniformly. Specifically, we incorporate three components, cosine normalization, less-forget constraint, and inter-class separation, to mitigate the adverse effects of the imbalance. Experiments show that the proposed method can effectively rebalance the training process, thus obtaining superior performance compared to the existing methods. On CIFAR100 and ImageNet, our method can reduce the classification errors by more than 6\% and 13\% respectively, under the incremental setting of 10 phases.},
  copyright = {https://doi.org/10.15223/policy-029},
  isbn = {978-1-7281-3293-8},
  langid = {english},
  file = {/Users/giulialanzillotta/Zotero/storage/528NXIQJ/Hou et al. - 2019 - Learning a Unified Classifier Incrementally via Rebalancing.pdf}
}

@inproceedings{houLearningUnifiedClassifier2019b,
  title = {Learning a {{Unified Classifier Incrementally}} via {{Rebalancing}}},
  booktitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Hou, Saihui and Pan, Xinyu and Loy, Chen Change and Wang, Zilei and Lin, Dahua},
  year = 2019,
  month = jun,
  pages = {831--839},
  publisher = {IEEE},
  address = {Long Beach, CA, USA},
  doi = {10.1109/CVPR.2019.00092},
  urldate = {2025-10-01},
  abstract = {Conventionally, deep neural networks are trained offline, relying on a large dataset prepared in advance. This paradigm is often challenged in real-world applications, e.g. online services that involve continuous streams of incoming data. Recently, incremental learning receives increasing attention, and is considered as a promising solution to the practical challenges mentioned above. However, it has been observed that incremental learning is subject to a fundamental difficulty -- catastrophic forgetting, namely adapting a model to new data often results in severe performance degradation on previous tasks or classes. Our study reveals that the imbalance between previous and new data is a crucial cause to this problem. In this work, we develop a new framework for incrementally learning a unified classifier, i.e. a classifier that treats both old and new classes uniformly. Specifically, we incorporate three components, cosine normalization, less-forget constraint, and inter-class separation, to mitigate the adverse effects of the imbalance. Experiments show that the proposed method can effectively rebalance the training process, thus obtaining superior performance compared to the existing methods. On CIFAR100 and ImageNet, our method can reduce the classification errors by more than 6\% and 13\% respectively, under the incremental setting of 10 phases.},
  copyright = {https://doi.org/10.15223/policy-029},
  isbn = {978-1-7281-3293-8},
  langid = {english},
  file = {/Users/giulialanzillotta/Zotero/storage/PR6KIWBM/Hou et al. - 2019 - Learning a Unified Classifier Incrementally via Rebalancing.pdf}
}

@misc{houlsbyParameterEfficientTransferLearning2019,
  title = {Parameter-{{Efficient Transfer Learning}} for {{NLP}}},
  author = {Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrone, Bruna and de Laroussilhe, Quentin and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain},
  year = 2019,
  month = jun,
  number = {arXiv:1902.00751},
  eprint = {1902.00751},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1902.00751},
  urldate = {2025-10-01},
  abstract = {Fine-tuning large pre-trained models is an effective transfer mechanism in NLP. However, in the presence of many downstream tasks, fine-tuning is parameter inefficient: an entire new model is required for every task. As an alternative, we propose transfer with adapter modules. Adapter modules yield a compact and extensible model; they add only a few trainable parameters per task, and new tasks can be added without revisiting previous ones. The parameters of the original network remain fixed, yielding a high degree of parameter sharing. To demonstrate adapter's effectiveness, we transfer the recently proposed BERT Transformer model to 26 diverse text classification tasks, including the GLUE benchmark. Adapters attain near state-of-the-art performance, whilst adding only a few parameters per task. On GLUE, we attain within 0.4\% of the performance of full fine-tuning, adding only 3.6\% parameters per task. By contrast, fine-tuning trains 100\% of the parameters per task.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@misc{houlsbyParameterEfficientTransferLearning2019a,
  title = {Parameter-{{Efficient Transfer Learning}} for {{NLP}}},
  author = {Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrone, Bruna and de Laroussilhe, Quentin and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain},
  year = 2019,
  month = jun,
  number = {arXiv:1902.00751},
  eprint = {1902.00751},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1902.00751},
  urldate = {2025-10-01},
  abstract = {Fine-tuning large pre-trained models is an effective transfer mechanism in NLP. However, in the presence of many downstream tasks, fine-tuning is parameter inefficient: an entire new model is required for every task. As an alternative, we propose transfer with adapter modules. Adapter modules yield a compact and extensible model; they add only a few trainable parameters per task, and new tasks can be added without revisiting previous ones. The parameters of the original network remain fixed, yielding a high degree of parameter sharing. To demonstrate adapter's effectiveness, we transfer the recently proposed BERT Transformer model to 26 diverse text classification tasks, including the GLUE benchmark. Adapters attain near state-of-the-art performance, whilst adding only a few parameters per task. On GLUE, we attain within 0.4\% of the performance of full fine-tuning, adding only 3.6\% parameters per task. By contrast, fine-tuning trains 100\% of the parameters per task.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/2GJAKJJM/Houlsby et al. - 2019 - Parameter-Efficient Transfer Learning for NLP.pdf;/Users/giulialanzillotta/Zotero/storage/Q9TQY6QC/1902.html}
}

@misc{huangLanguageNotAll2023,
  title = {Language {{Is Not All You Need}}: {{Aligning Perception}} with {{Language Models}}},
  shorttitle = {Language {{Is Not All You Need}}},
  author = {Huang, Shaohan and Dong, Li and Wang, Wenhui and Hao, Yaru and Singhal, Saksham and Ma, Shuming and Lv, Tengchao and Cui, Lei and Mohammed, Owais Khan and Patra, Barun and Liu, Qiang and Aggarwal, Kriti and Chi, Zewen and Bjorck, Johan and Chaudhary, Vishrav and Som, Subhojit and Song, Xia and Wei, Furu},
  year = 2023,
  month = mar,
  number = {arXiv:2302.14045},
  eprint = {2302.14045},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2302.14045},
  urldate = {2025-10-13},
  abstract = {A big convergence of language, multimodal perception, action, and world modeling is a key step toward artificial general intelligence. In this work, we introduce Kosmos-1, a Multimodal Large Language Model (MLLM) that can perceive general modalities, learn in context (i.e., few-shot), and follow instructions (i.e., zero-shot). Specifically, we train Kosmos-1 from scratch on web-scale multimodal corpora, including arbitrarily interleaved text and images, image-caption pairs, and text data. We evaluate various settings, including zero-shot, few-shot, and multimodal chain-of-thought prompting, on a wide range of tasks without any gradient updates or finetuning. Experimental results show that Kosmos-1 achieves impressive performance on (i) language understanding, generation, and even OCR-free NLP (directly fed with document images), (ii) perception-language tasks, including multimodal dialogue, image captioning, visual question answering, and (iii) vision tasks, such as image recognition with descriptions (specifying classification via text instructions). We also show that MLLMs can benefit from cross-modal transfer, i.e., transfer knowledge from language to multimodal, and from multimodal to language. In addition, we introduce a dataset of Raven IQ test, which diagnoses the nonverbal reasoning capability of MLLMs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/giulialanzillotta/Zotero/storage/JDA59YB9/Huang et al. - 2023 - Language Is Not All You Need Aligning Perception with Language Models.pdf;/Users/giulialanzillotta/Zotero/storage/LQ5NFXMP/2302.html}
}

@misc{huHowWellDoes2022,
  title = {How {{Well Does Self-Supervised Pre-Training Perform}} with {{Streaming Data}}?},
  author = {Hu, Dapeng and Yan, Shipeng and Lu, Qizhengqiu and Hong, Lanqing and Hu, Hailin and Zhang, Yifan and Li, Zhenguo and Wang, Xinchao and Feng, Jiashi},
  year = 2022,
  month = jul,
  number = {arXiv:2104.12081},
  eprint = {2104.12081},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2104.12081},
  urldate = {2025-10-03},
  abstract = {Prior works on self-supervised pre-training focus on the joint training scenario, where massive unlabeled data are assumed to be given as input all at once, and only then is a learner trained. Unfortunately, such a problem setting is often impractical if not infeasible since many real-world tasks rely on sequential learning, e.g., data are decentralized or collected in a streaming fashion. In this paper, we conduct the first thorough and dedicated investigation on self-supervised pre-training with streaming data, aiming to shed light on the model behavior under this overlooked setup. Specifically, we pre-train over 500 models on four categories of pre-training streaming data from ImageNet and DomainNet and evaluate them on three types of downstream tasks and 12 different downstream datasets. Our studies show that, somehow beyond our expectation, with simple data replay or parameter regularization, sequential self-supervised pre-training turns out to be an efficient alternative for joint pre-training, as the performances of the former are mostly on par with those of the latter. Moreover, catastrophic forgetting, a common issue in sequential supervised learning, is much alleviated in sequential self-supervised learning (SSL), which is well justified through our comprehensive empirical analysis on representations and the sharpness of minima in the loss landscape. Our findings, therefore, suggest that, in practice, for SSL, the cumbersome joint training can be replaced mainly by sequential learning, which in turn enables a much broader spectrum of potential application scenarios.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/7L7SMTP9/Hu et al. - 2022 - How Well Does Self-Supervised Pre-Training Perform with Streaming Data.pdf;/Users/giulialanzillotta/Zotero/storage/RM8GKFJ7/2104.html}
}

@article{huiLimitationsNeuralCollapse2022,
  title = {Limitations of Neural Collapse for Understanding Generalization in Deep Learning},
  author = {Hui, Like and Belkin, Mikhail and Nakkiran, Preetum},
  year = 2022,
  journal = {arXiv preprint arXiv:2202.08384},
  eprint = {2202.08384},
  archiveprefix = {arXiv}
}

@misc{hungCompactingPickingGrowing2019,
  title = {Compacting, {{Picking}} and {{Growing}} for {{Unforgetting Continual Learning}}},
  author = {Hung, Steven C. Y. and Tu, Cheng-Hao and Wu, Cheng-En and Chen, Chien-Hung and Chan, Yi-Ming and Chen, Chu-Song},
  year = 2019,
  month = oct,
  number = {arXiv:1910.06562},
  eprint = {1910.06562},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1910.06562},
  urldate = {2025-10-01},
  abstract = {Continual lifelong learning is essential to many applications. In this paper, we propose a simple but effective approach to continual deep learning. Our approach leverages the principles of deep model compression, critical weights selection, and progressive networks expansion. By enforcing their integration in an iterative manner, we introduce an incremental learning method that is scalable to the number of sequential tasks in a continual learning process. Our approach is easy to implement and owns several favorable characteristics. First, it can avoid forgetting (i.e., learn new tasks while remembering all previous tasks). Second, it allows model expansion but can maintain the model compactness when handling sequential tasks. Besides, through our compaction and selection/expansion mechanism, we show that the knowledge accumulated through learning previous tasks is helpful to build a better model for the new tasks compared to training the models independently with tasks. Experimental results show that our approach can incrementally learn a deep model tackling multiple tasks without forgetting, while the model compactness is maintained with the performance more satisfiable than individual task training.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/QQHLEJID/Hung et al. - 2019 - Compacting, Picking and Growing for Unforgetting Continual Learning.pdf;/Users/giulialanzillotta/Zotero/storage/V8KE6J2N/1910.html}
}

@misc{hurtadoOptimizingReusableKnowledge2021,
  title = {Optimizing {{Reusable Knowledge}} for {{Continual Learning}} via {{Metalearning}}},
  author = {Hurtado, Julio and {Raymond-Saez}, Alain and Soto, Alvaro},
  year = 2021,
  month = nov,
  number = {arXiv:2106.05390},
  eprint = {2106.05390},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2106.05390},
  urldate = {2025-10-01},
  abstract = {When learning tasks over time, artificial neural networks suffer from a problem known as Catastrophic Forgetting (CF). This happens when the weights of a network are overwritten during the training of a new task causing forgetting of old information. To address this issue, we propose MetA Reusable Knowledge or MARK, a new method that fosters weight reusability instead of overwriting when learning a new task. Specifically, MARK keeps a set of shared weights among tasks. We envision these shared weights as a common Knowledge Base (KB) that is not only used to learn new tasks, but also enriched with new knowledge as the model learns new tasks. Key components behind MARK are two-fold. On the one hand, a metalearning approach provides the key mechanism to incrementally enrich the KB with new knowledge and to foster weight reusability among tasks. On the other hand, a set of trainable masks provides the key mechanism to selectively choose from the KB relevant weights to solve each task. By using MARK, we achieve state of the art results in several popular benchmarks, surpassing the best performing methods in terms of average accuracy by over 10\% on the 20-Split-MiniImageNet dataset, while achieving almost zero forgetfulness using 55\% of the number of parameters. Furthermore, an ablation study provides evidence that, indeed, MARK is learning reusable knowledge that is selectively used by each task.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/5BCME2VN/Hurtado et al. - 2021 - Optimizing Reusable Knowledge for Continual Learning via Metalearning.pdf;/Users/giulialanzillotta/Zotero/storage/L8SIDR8T/2106.html}
}

@misc{ibrahimSimpleScalableStrategies2024,
  title = {Simple and {{Scalable Strategies}} to {{Continually Pre-train Large Language Models}}},
  author = {Ibrahim, Adam and Th{\'e}rien, Benjamin and Gupta, Kshitij and Richter, Mats L. and Anthony, Quentin and Lesort, Timoth{\'e}e and Belilovsky, Eugene and Rish, Irina},
  year = 2024,
  month = sep,
  number = {arXiv:2403.08763},
  eprint = {2403.08763},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2403.08763},
  urldate = {2025-09-30},
  abstract = {Large language models (LLMs) are routinely pre-trained on billions of tokens, only to start the process over again once new data becomes available. A much more efficient solution is to continually pre-train these models, saving significant compute compared to re-training. However, the distribution shift induced by new data typically results in degraded performance on previous data or poor adaptation to the new data. In this work, we show that a simple and scalable combination of learning rate (LR) re-warming, LR re-decaying, and replay of previous data is sufficient to match the performance of fully re-training from scratch on all available data, as measured by the final loss and the average score on several language model (LM) evaluation benchmarks. Specifically, we show this for a weak but realistic distribution shift between two commonly used LLM pre-training datasets (English\${\textbackslash}rightarrow\$English) and a stronger distribution shift (English\${\textbackslash}rightarrow\$German) at the \$405\$M parameter model scale with large dataset sizes (hundreds of billions of tokens). Selecting the weak but realistic shift for larger-scale experiments, we also find that our continual learning strategies match the re-training baseline for a 10B parameter LLM. Our results demonstrate that LLMs can be successfully updated via simple and scalable continual learning strategies, matching the re-training baseline using only a fraction of the compute. Finally, inspired by previous work, we propose alternatives to the cosine learning rate schedule that help circumvent forgetting induced by LR re-warming and that are not bound to a fixed token budget.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/X5JHR8SD/Ibrahim et al. - 2024 - Simple and Scalable Strategies to Continually Pre-train Large Language Models.pdf;/Users/giulialanzillotta/Zotero/storage/WQIBF3JA/2403.html}
}

@misc{InverseVarianceFlatness,
  title = {The Inverse Variance--Flatness Relation in Stochastic Gradient Descent Is Critical for Finding Flat Minima},
  doi = {10.1073/pnas.2015617118},
  urldate = {2025-07-05},
  howpublished = {https://www.pnas.org/doi/10.1073/pnas.2015617118},
  langid = {english},
  file = {/Users/giulialanzillotta/Zotero/storage/VZNU3QPV/The inverse varianceâ€“flatness relation in stochastic gradient descent is critical for finding flat m.pdf;/Users/giulialanzillotta/Zotero/storage/Q9PR8HH4/pnas.html}
}

@misc{ioffeBatchNormalizationAccelerating2015,
  title = {Batch {{Normalization}}: {{Accelerating Deep Network Training}} by {{Reducing Internal Covariate Shift}}},
  author = {Ioffe, Sergey and Szegedy, Christian},
  year = 2015
}

@misc{iscenMemoryEfficientIncrementalLearning2020,
  title = {Memory-{{Efficient Incremental Learning Through Feature Adaptation}}},
  author = {Iscen, Ahmet and Zhang, Jeffrey and Lazebnik, Svetlana and Schmid, Cordelia},
  year = 2020,
  month = aug,
  number = {arXiv:2004.00713},
  eprint = {2004.00713},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2004.00713},
  urldate = {2025-09-30},
  abstract = {We introduce an approach for incremental learning that preserves feature descriptors of training images from previously learned classes, instead of the images themselves, unlike most existing work. Keeping the much lower-dimensional feature embeddings of images reduces the memory footprint significantly. We assume that the model is updated incrementally for new classes as new data becomes available sequentially.This requires adapting the previously stored feature vectors to the updated feature space without having access to the corresponding original training images. Feature adaptation is learned with a multi-layer perceptron, which is trained on feature pairs corresponding to the outputs of the original and updated network on a training image. We validate experimentally that such a transformation generalizes well to the features of the previous set of classes, and maps features to a discriminative subspace in the feature space. As a result, the classifier is optimized jointly over new and old classes without requiring old class images. Experimental results show that our method achieves state-of-the-art classification accuracy in incremental learning benchmarks, while having at least an order of magnitude lower memory footprint compared to image-preserving strategies.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/giulialanzillotta/Zotero/storage/4XNSSQBE/Iscen et al. - 2020 - Memory-Efficient Incremental Learning Through Feature Adaptation.pdf;/Users/giulialanzillotta/Zotero/storage/SJDN923M/2004.html}
}

@inproceedings{jacotImprovingUnderstandingVariational2019,
  title = {Improving and {{Understanding Variational Continual Learning}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Jacot, Arthur and Gabriel, Franck and Hongler, Clement and He, Xu and Sygnowski, Jakub and Galashov, Alexandre and Rusu, Andrei A. and Teh, Yee Whye and Pascanu, Razvan and Swaroop, Siddharth and Nguyen, Cuong V. and Bui, Thang D. and Turner, Richard E.},
  editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and {Cesa-Bianchi}, N. and Garnett, R.},
  year = 2019,
  volume = {abs/1905.02099},
  publisher = {Curran Associates, Inc.},
  keywords = {Regularization}
}

@inproceedings{jacotNeuralTangentKernel2018,
  title = {Neural {{Tangent Kernel}}: {{Convergence}} and {{Generalization}} in {{Neural Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Jacot, Arthur and Gabriel, Franck and Hongler, Clement},
  editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and {Cesa-Bianchi}, N. and Garnett, R.},
  year = 2018,
  volume = {31},
  publisher = {Curran Associates, Inc.}
}

@misc{jacotWideNeuralNetworks2024,
  title = {Wide {{Neural Networks Trained}} with {{Weight Decay Provably Exhibit Neural Collapse}}},
  author = {Jacot, Arthur and S{\'u}ken{\'i}k, Peter and Wang, Zihan and Mondelli, Marco},
  year = 2024,
  month = oct,
  number = {arXiv:2410.04887},
  eprint = {2410.04887},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2410.04887},
  urldate = {2025-09-16},
  abstract = {Deep neural networks (DNNs) at convergence consistently represent the training data in the last layer via a highly symmetric geometric structure referred to as neural collapse. This empirical evidence has spurred a line of theoretical research aimed at proving the emergence of neural collapse, mostly focusing on the unconstrained features model. Here, the features of the penultimate layer are free variables, which makes the model data-agnostic and, hence, puts into question its ability to capture DNN training. Our work addresses the issue, moving away from unconstrained features and studying DNNs that end with at least two linear layers. We first prove generic guarantees on neural collapse that assume (i) low training error and balancedness of the linear layers (for within-class variability collapse), and (ii) bounded conditioning of the features before the linear part (for orthogonality of class-means, as well as their alignment with weight matrices). We then show that such assumptions hold for gradient descent training with weight decay: (i) for networks with a wide first layer, we prove low training error and balancedness, and (ii) for solutions that are either nearly optimal or stable under large learning rates, we additionally prove the bounded conditioning. Taken together, our results are the first to show neural collapse in the end-to-end training of DNNs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/ADGNHU4A/Jacot et al. - 2024 - Wide Neural Networks Trained with Weight Decay Provably Exhibit Neural Collapse.pdf;/Users/giulialanzillotta/Zotero/storage/YP3QWJZ3/2410.html}
}

@inproceedings{jiangGeneralizedNeuralCollapse2024,
  title = {Generalized {{Neural Collapse}} for a {{Large Number}} of {{Classes}}},
  booktitle = {Proceedings of the 41st {{International Conference}} on {{Machine Learning}}},
  author = {Jiang, Jiachen and Zhou, Jinxin and Wang, Peng and Qu, Qing and Mixon, Dustin G. and You, Chong and Zhu, Zhihui},
  year = 2024,
  month = jul,
  pages = {22010--22041},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2025-09-16},
  abstract = {Neural collapse provides an elegant mathematical characterization of learned last layer representations (a.k.a. features) and classifier weights in deep classification models. Such results not only provide insights but also motivate new techniques for improving practical deep models. However, most of the existing empirical and theoretical studies in neural collapse focus on the case that the number of classes is small relative to the dimension of the feature space. This paper extends neural collapse to cases where the number of classes are much larger than the dimension of feature space, which broadly occur for language models, retrieval systems, and face recognition applications. We show that the features and classifier exhibit a generalized neural collapse phenomenon, where the minimum one-vs-rest margins is maximized. We provide empirical study to verify the occurrence of generalized neural collapse in practical deep neural networks. Moreover, we provide theoretical study to show that the generalized neural collapse provably occurs under unconstrained feature model with spherical constraint, under certain technical conditions on feature dimension and number of classes.},
  langid = {english}
}

@misc{josephEnergybasedLatentAligner2022,
  title = {Energy-Based {{Latent Aligner}} for {{Incremental Learning}}},
  author = {Joseph, K. J. and Khan, Salman and Khan, Fahad Shahbaz and Anwer, Rao Muhammad and Balasubramanian, Vineeth N.},
  year = 2022,
  month = mar,
  number = {arXiv:2203.14952},
  eprint = {2203.14952},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2203.14952},
  urldate = {2025-10-03},
  abstract = {Deep learning models tend to forget their earlier knowledge while incrementally learning new tasks. This behavior emerges because the parameter updates optimized for the new tasks may not align well with the updates suitable for older tasks. The resulting latent representation mismatch causes forgetting. In this work, we propose ELI: Energy-based Latent Aligner for Incremental Learning, which first learns an energy manifold for the latent representations such that previous task latents will have low energy and the current task latents have high energy values. This learned manifold is used to counter the representational shift that happens during incremental learning. The implicit regularization that is offered by our proposed methodology can be used as a plug-and-play module in existing incremental learning methodologies. We validate this through extensive evaluation on CIFAR-100, ImageNet subset, ImageNet 1k and Pascal VOC datasets. We observe consistent improvement when ELI is added to three prominent methodologies in class-incremental learning, across multiple incremental settings. Further, when added to the state-of-the-art incremental object detector, ELI provides over 5\% improvement in detection accuracy, corroborating its effectiveness and complementary advantage to existing art.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning}
}

@misc{kangDeepNeuralNetworks2024,
  title = {Deep {{Neural Networks Tend To Extrapolate Predictably}}},
  author = {Kang, Katie and Setlur, Amrith and Tomlin, Claire and Levine, Sergey},
  year = 2024,
  month = mar,
  number = {arXiv:2310.00873},
  eprint = {2310.00873},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2310.00873},
  urldate = {2025-09-15},
  abstract = {Conventional wisdom suggests that neural network predictions tend to be unpredictable and overconfident when faced with out-of-distribution (OOD) inputs. Our work reassesses this assumption for neural networks with high-dimensional inputs. Rather than extrapolating in arbitrary ways, we observe that neural network predictions often tend towards a constant value as input data becomes increasingly OOD. Moreover, we find that this value often closely approximates the optimal constant solution (OCS), i.e., the prediction that minimizes the average loss over the training data without observing the input. We present results showing this phenomenon across 8 datasets with different distributional shifts (including CIFAR10-C and ImageNet-R, S), different loss functions (cross entropy, MSE, and Gaussian NLL), and different architectures (CNNs and transformers). Furthermore, we present an explanation for this behavior, which we first validate empirically and then study theoretically in a simplified setting involving deep homogeneous networks with ReLU activations. Finally, we show how one can leverage our insights in practice to enable risk-sensitive decision-making in the presence of OOD inputs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/X5LS853E/Kang et al. - 2024 - Deep Neural Networks Tend To Extrapolate Predictably.pdf;/Users/giulialanzillotta/Zotero/storage/42LG8KW4/2310.html}
}

@misc{kangDeepNeuralNetworks2024a,
  title = {Deep {{Neural Networks Tend To Extrapolate Predictably}}},
  author = {Kang, Katie and Setlur, Amrith and Tomlin, Claire and Levine, Sergey},
  year = 2024,
  month = mar,
  number = {arXiv:2310.00873},
  eprint = {2310.00873},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2310.00873},
  urldate = {2025-09-16},
  abstract = {Conventional wisdom suggests that neural network predictions tend to be unpredictable and overconfident when faced with out-of-distribution (OOD) inputs. Our work reassesses this assumption for neural networks with high-dimensional inputs. Rather than extrapolating in arbitrary ways, we observe that neural network predictions often tend towards a constant value as input data becomes increasingly OOD. Moreover, we find that this value often closely approximates the optimal constant solution (OCS), i.e., the prediction that minimizes the average loss over the training data without observing the input. We present results showing this phenomenon across 8 datasets with different distributional shifts (including CIFAR10-C and ImageNet-R, S), different loss functions (cross entropy, MSE, and Gaussian NLL), and different architectures (CNNs and transformers). Furthermore, we present an explanation for this behavior, which we first validate empirically and then study theoretically in a simplified setting involving deep homogeneous networks with ReLU activations. Finally, we show how one can leverage our insights in practice to enable risk-sensitive decision-making in the presence of OOD inputs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/MS3S4LX7/Kang et al. - 2024 - Deep Neural Networks Tend To Extrapolate Predictably.pdf;/Users/giulialanzillotta/Zotero/storage/Y35IHPAA/2310.html}
}

@inproceedings{kangForgetfreeContinualLearning2022,
  title = {Forget-Free {{Continual Learning}} with {{Winning Subnetworks}}},
  booktitle = {Proceedings of the 39th {{International Conference}} on {{Machine Learning}}},
  author = {Kang, Haeyong and Mina, Rusty John Lloyd and Madjid, Sultan Rizky Hikmawan and Yoon, Jaehong and {Hasegawa-Johnson}, Mark and Hwang, Sung Ju and Yoo, Chang D.},
  year = 2022,
  month = jun,
  pages = {10734--10750},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2025-10-01},
  abstract = {Inspired by Lottery Ticket Hypothesis that competitive subnetworks exist within a dense network, we propose a continual learning method referred to as Winning SubNetworks (WSN), which sequentially learns and selects an optimal subnetwork for each task. Specifically, WSN jointly learns the model weights and task-adaptive binary masks pertaining to subnetworks associated with each task whilst attempting to select a small set of weights to be activated (winning ticket) by reusing weights of the prior subnetworks. The proposed method is inherently immune to catastrophic forgetting as each selected subnetwork model does not infringe upon other subnetworks. Binary masks spawned per winning ticket are encoded into one N-bit binary digit mask, then compressed using Huffman coding for a sub-linear increase in network capacity with respect to the number of tasks.},
  langid = {english},
  file = {/Users/giulialanzillotta/Zotero/storage/YFP8X2V5/Kang et al. - 2022 - Forget-free Continual Learning with Winning Subnetworks.pdf}
}

@article{kapoorVariationalAutoRegressiveGaussian,
  title = {Variational {{Auto-Regressive Gaussian Processes}} for {{Continual Learning}}},
  author = {Kapoor, Sanyam and Karaletsos, Theofanis and Bui, Thang D},
  abstract = {Through sequential construction of posteriors on observing data online, Bayes' theorem provides a natural framework for continual learning. We develop Variational Auto-Regressive Gaussian Processes (VAR-GPs), a principled posterior updating mechanism to solve sequential tasks in continual learning. By relying on sparse inducing point approximations for scalable posteriors, we propose a novel auto-regressive variational distribution which reveals two fruitful connections to existing results in Bayesian inference, expectation propagation and orthogonal inducing points. Mean predictive entropy estimates show VAR-GPs prevent catastrophic forgetting, which is empirically supported by strong performance on modern continual learning benchmarks against competitive baselines. A thorough ablation study demonstrates the efficacy of our modeling choices.},
  langid = {english},
  file = {/Users/giulialanzillotta/Zotero/storage/NPQAAYXY/Kapoor et al. - Variational Auto-Regressive Gaussian Processes for Continual Learning.pdf}
}

@article{kearnsNearoptimalReinforcementLearning2002,
  title = {Near-Optimal Reinforcement Learning in Polynomial Time},
  author = {Kearns, Michael and Singh, Satinder},
  year = 2002,
  journal = {Machine learning},
  volume = {49},
  pages = {209--232},
  publisher = {Springer}
}

@misc{khanBayesianLearningRule2024,
  title = {The {{Bayesian Learning Rule}}},
  author = {Khan, Mohammad Emtiyaz and Rue, H{\aa}vard},
  year = 2024,
  month = jun,
  number = {arXiv:2107.04562},
  eprint = {2107.04562},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2107.04562},
  urldate = {2025-06-30},
  abstract = {We show that many machine-learning algorithms are specific instances of a single algorithm called the {\textbackslash}emph\{Bayesian learning rule\}. The rule, derived from Bayesian principles, yields a wide-range of algorithms from fields such as optimization, deep learning, and graphical models. This includes classical algorithms such as ridge regression, Newton's method, and Kalman filter, as well as modern deep-learning algorithms such as stochastic-gradient descent, RMSprop, and Dropout. The key idea in deriving such algorithms is to approximate the posterior using candidate distributions estimated by using natural gradients. Different candidate distributions result in different algorithms and further approximations to natural gradients give rise to variants of those algorithms. Our work not only unifies, generalizes, and improves existing algorithms, but also helps us design new ones.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/6H9ZJBIT/Khan and Rue - 2024 - The Bayesian Learning Rule.pdf;/Users/giulialanzillotta/Zotero/storage/X352L2LC/2107.html}
}

@misc{khanKnowledgeAdaptationPosterior2025,
  title = {Knowledge {{Adaptation}} as {{Posterior Correction}}},
  author = {Khan, Mohammad Emtiyaz},
  year = 2025,
  month = jun,
  number = {arXiv:2506.14262},
  eprint = {2506.14262},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2506.14262},
  urldate = {2025-06-30},
  abstract = {Adaptation is the holy grail of intelligence, but even the best AI models (like GPT) lack the adaptivity of toddlers. So the question remains: how can machines adapt quickly? Despite a lot of progress on model adaptation to facilitate continual and federated learning, as well as model merging, editing, unlearning, etc., little is known about the mechanisms by which machines can naturally learn to adapt in a similar way as humans and animals. Here, we show that all such adaptation methods can be seen as different ways of `correcting' the approximate posteriors. More accurate posteriors lead to smaller corrections, which in turn imply quicker adaptation. The result is obtained by using a dual-perspective of the Bayesian Learning Rule of Khan and Rue (2023) where interference created during adaptation is characterized by the natural-gradient mismatch over the past data. We present many examples to demonstrate the use of posterior-correction as a natural mechanism for the machines to learn to adapt quickly.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/I4BB2K7M/Khan - 2025 - Knowledge Adaptation as Posterior Correction.pdf;/Users/giulialanzillotta/Zotero/storage/2URJ7MFG/2506.html}
}

@misc{khanKnowledgeAdaptationPriors2021,
  title = {Knowledge-{{Adaptation Priors}}},
  author = {Khan, Mohammad Emtiyaz and Swaroop, Siddharth},
  year = 2021,
  month = oct,
  number = {arXiv:2106.08769},
  eprint = {2106.08769},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2106.08769},
  urldate = {2025-09-30},
  abstract = {Humans and animals have a natural ability to quickly adapt to their surroundings, but machine-learning models, when subjected to changes, often require a complete retraining from scratch. We present Knowledge-adaptation priors (K-priors) to reduce the cost of retraining by enabling quick and accurate adaptation for a wide-variety of tasks and models. This is made possible by a combination of weight and function-space priors to reconstruct the gradients of the past, which recovers and generalizes many existing, but seemingly-unrelated, adaptation strategies. Training with simple first-order gradient methods can often recover the exact retrained model to an arbitrary accuracy by choosing a sufficiently large memory of the past data. Empirical results show that adaptation with K-priors achieves performance similar to full retraining, but only requires training on a handful of past examples.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/NRIHMVXT/Khan and Swaroop - 2021 - Knowledge-Adaptation Priors.pdf;/Users/giulialanzillotta/Zotero/storage/GCSPBQXB/2106.html}
}

@article{khetarpalContinualReinforcementLearning2022,
  title = {Towards Continual Reinforcement Learning: {{A}} Review and Perspectives},
  author = {Khetarpal, Khimya and Riemer, Matthew and Rish, Irina and Precup, Doina},
  year = 2022,
  journal = {Journal of Artificial Intelligence Research},
  volume = {75},
  pages = {1401--1476}
}

@article{khodakAdaptiveGradientbasedMetalearning2019,
  title = {Adaptive Gradient-Based Meta-Learning Methods},
  author = {Khodak, Mikhail and Balcan, Maria-Florina F and Talwalkar, Ameet S},
  year = 2019,
  journal = {Advances in Neural Information Processing Systems},
  volume = {32}
}

@misc{kimDiffusionMeetsFewshot2025,
  title = {Diffusion {{Meets Few-shot Class Incremental Learning}}},
  author = {Kim, Junsu and Ku, Yunhoe and Han, Dongyoon and Baek, Seungryul},
  year = 2025,
  month = mar,
  number = {arXiv:2503.23402},
  eprint = {2503.23402},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2503.23402},
  urldate = {2025-10-14},
  abstract = {Few-shot class-incremental learning (FSCIL) is challenging due to extremely limited training data; while aiming to reduce catastrophic forgetting and learn new information. We propose Diffusion-FSCIL, a novel approach that employs a text-to-image diffusion model as a frozen backbone. Our conjecture is that FSCIL can be tackled using a large generative model's capabilities benefiting from 1) generation ability via large-scale pre-training; 2) multi-scale representation; 3) representational flexibility through the text encoder. To maximize the representation capability, we propose to extract multiple complementary diffusion features to play roles as latent replay with slight support from feature distillation for preventing generative biases. Our framework realizes efficiency through 1) using a frozen backbone; 2) minimal trainable components; 3) batch processing of multiple feature extractions. Extensive experiments on CUB-200, miniImageNet, and CIFAR-100 show that Diffusion-FSCIL surpasses state-of-the-art methods, preserving performance on previously learned classes and adapting effectively to new ones.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/giulialanzillotta/Zotero/storage/YHV3BANM/Kim et al. - 2025 - Diffusion Meets Few-shot Class Incremental Learning.pdf;/Users/giulialanzillotta/Zotero/storage/CLCK9ZJJ/2503.html}
}

@article{kirkpatrickOvercomingCatastrophicForgetting2017,
  title = {Overcoming Catastrophic Forgetting in Neural Networks},
  author = {Kirkpatrick, James and Pascanu, Razvan and Rabinowitz, Neil and Veness, Joel and Desjardins, Guillaume and Rusu, Andrei A. and Milan, Kieran and Quan, John and Ramalho, Tiago and {Grabska-Barwinska}, Agnieszka and others},
  year = 2017,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {114},
  number = {13},
  pages = {3521--3526},
  publisher = {National Acad Sciences}
}

@article{kothapalliNeuralCollapseReview2022,
  title = {Neural Collapse: {{A}} Review on Modelling Principles and Generalization},
  author = {Kothapalli, Vignesh},
  year = 2022,
  journal = {arXiv preprint arXiv:2206.04041},
  eprint = {2206.04041},
  archiveprefix = {arXiv}
}

@inproceedings{krause4thIEEEWorkshop2013,
  title = {4th {{IEEE Workshop}} on {{3D Representation}} and {{Recognition}}, at {{ICCV}} 2013 ({{3dRR-13}})},
  booktitle = {Proceedings of the {{IEEE International Conference}} on {{Computer Vision Workshops}} ({{ICCVW}})},
  author = {Krause, Jonathan and Stark, Michael and Deng, Jia and {Fei-Fei}, Li},
  year = 2013,
  month = dec,
  address = {Sydney, Australia}
}

@inproceedings{krauseDescribingTexturesWild2014,
  title = {Describing {{Textures}} in the {{Wild}}},
  booktitle = {Proceedings of the {{IEEE Conf}}. on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Krause, Jonathan and Stark, Michael and Deng, Jia and {Fei-Fei}, Li and Elsayed, Mohamed and Mahmood, A. Rupam and Maltoni, D. and Lomonaco, V. and Zenke, F. and Poole, B. and Ganguli, S. and Rusu, A. A. and Rabinowitz, N. C. and Desjardins, G. and Soyer, H. and Kirkpatrick, J. and Kavukcuoglu, K. and Pascanu, R. and Hadsell, R. and Zhou, G. and Sohn, K. and Lee, H. and Shin, H. and Lee, J. K. and Kim, J. and Kim, J. and Robins, A. V. and Maji, Subhransu and Rahtu, Esa and Kannala, Juho and Blaschko, Matthew B. and Vedaldi, Andrea and Cimpoi, M. and Maji, S. and Kokkinos, I. and Mohamed, S. and Vedaldi, {and} A.},
  year = 2014,
  month = dec,
  volume = {abs/1306.5151},
  eprint = {1306.5151},
  pages = {123--146},
  publisher = {University of Alberta, Alberta Machine Intelligence Institute (Amii)},
  address = {Long Beach, CA},
  archiveprefix = {arXiv}
}

@misc{krizhevskyLearningMultipleLayers2009,
  title = {Learning {{Multiple Layers}} of {{Features}} from {{Tiny Images}}},
  author = {Krizhevsky, Alex and Hinton, Geoffrey},
  year = 2009,
  publisher = {University of Toronto}
}

@inproceedings{kumarBayesianStructuralAdaptation2021,
  title = {Bayesian {{Structural Adaptation}} for {{Continual Learning}}},
  booktitle = {Proceedings of the 38th {{International Conference}} on {{Machine Learning}}},
  author = {Kumar, Abhishek and Chatterjee, Sunabha and Rai, Piyush},
  year = 2021,
  month = jul,
  pages = {5850--5860},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2025-10-01},
  abstract = {Continual Learning is a learning paradigm where learning systems are trained on a sequence of tasks. The goal here is to perform well on the current task without suffering from a performance drop on the previous tasks. Two notable directions among the recent advances in continual learning with neural networks are (1) variational Bayes based regularization by learning priors from previous tasks, and, (2) learning the structure of deep networks to adapt to new tasks. So far, these two approaches have been largely orthogonal. We present a novel Bayesian framework based on continually learning the structure of deep neural networks, to unify these distinct yet complementary approaches. The proposed framework learns the deep structure for each task by learning which weights to be used, and supports inter-task transfer through the overlapping of different sparse subsets of weights learned by different tasks. An appealing aspect of our proposed continual learning framework is that it is applicable to both discriminative (supervised) and generative (unsupervised) settings. Experimental results on supervised and unsupervised benchmarks demonstrate that our approach performs comparably or better than recent advances in continual learning.},
  langid = {english},
  file = {/Users/giulialanzillotta/Zotero/storage/RAUK45FU/Kumar et al. - 2021 - Bayesian Structural Adaptation for Continual Learning.pdf}
}

@article{kumarContinualLearningComputationally2023,
  title = {Continual Learning as Computationally Constrained Reinforcement Learning},
  author = {Kumar, Saurabh and Marklund, Henrik and Rao, Ashish and Zhu, Yifan and Jeon, Hong Jun and Liu, Yueyang and Van Roy, Benjamin},
  year = 2023,
  journal = {arXiv preprint arXiv:2307.04345},
  eprint = {2307.04345},
  archiveprefix = {arXiv}
}

@misc{kumarContinualLearningComputationally2025,
  title = {Continual {{Learning}} as {{Computationally Constrained Reinforcement Learning}}},
  author = {Kumar, Saurabh and Marklund, Henrik and Rao, Ashish and Zhu, Yifan and Jeon, Hong Jun and Liu, Yueyang and Roy, Benjamin Van},
  year = 2025,
  month = jun,
  number = {arXiv:2307.04345},
  eprint = {2307.04345},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2307.04345},
  urldate = {2025-10-06},
  abstract = {An agent that efficiently accumulates knowledge to develop increasingly sophisticated skills over a long lifetime could advance the frontier of artificial intelligence capabilities. The design of such agents, which remains a long-standing challenge of artificial intelligence, is addressed by the subject of continual learning. This monograph clarifies and formalizes concepts of continual learning, introducing a framework and set of tools to stimulate further research.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/PMBHCLJ6/Kumar et al. - 2025 - Continual Learning as Computationally Constrained Reinforcement Learning.pdf;/Users/giulialanzillotta/Zotero/storage/ZENWSTPR/2307.html}
}

@article{kurleCONTINUALLEARNINGBAYESIAN2020,
  title = {{{CONTINUAL LEARNING WITH BAYESIAN NEURAL NETWORKS FOR NON-STATIONARY DATA}}},
  author = {Kurle, Richard and Cseke, Botond and Klushyn, Alexej},
  year = 2020,
  abstract = {This work addresses continual learning for non-stationary data, using Bayesian neural networks and memory-based online variational Bayes. We represent the posterior approximation of the network weights by a diagonal Gaussian distribution and a complementary memory of raw data. This raw data corresponds to likelihood terms that cannot be well approximated by the Gaussian. We introduce a novel method for sequentially updating both components of the posterior approximation. Furthermore, we propose Bayesian forgetting and a Gaussian diffusion process for adapting to non-stationary data. The experimental results show that our update method improves on existing approaches for streaming data. Additionally, the adaptation methods lead to better predictive performance for non-stationary data.},
  langid = {english},
  file = {/Users/giulialanzillotta/Zotero/storage/RWAI86CS/Kurle et al. - 2020 - CONTINUAL LEARNING WITH BAYESIAN NEURAL NETWORKS FOR NON-STATIONARY DATA.pdf}
}

@article{lanzillottaLocalVsGlobal2024,
  title = {Local vs {{Global}} Continual Learning},
  author = {Lanzillotta, Giulia and Singh, Sidak Pal and Grewe, Benjamin F and Hofmann, Thomas},
  year = 2024,
  journal = {arXiv preprint arXiv:2407.16611},
  eprint = {2407.16611},
  archiveprefix = {arXiv}
}

@misc{lanzillottaLocalVsGlobal2024a,
  title = {Local vs {{Global}} Continual Learning},
  author = {Lanzillotta, Giulia and Singh, Sidak Pal and Grewe, Benjamin F. and Hofmann, Thomas},
  year = 2024,
  month = jul,
  number = {arXiv:2407.16611},
  eprint = {2407.16611},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2407.16611},
  urldate = {2025-09-30},
  abstract = {Continual learning is the problem of integrating new information in a model while retaining the knowledge acquired in the past. Despite the tangible improvements achieved in recent years, the problem of continual learning is still an open one. A better understanding of the mechanisms behind the successes and failures of existing continual learning algorithms can unlock the development of new successful strategies. In this work, we view continual learning from the perspective of the multi-task loss approximation, and we compare two alternative strategies, namely local and global approximations. We classify existing continual learning algorithms based on the approximation used, and we assess the practical effects of this distinction in common continual learning settings.Additionally, we study optimal continual learning objectives in the case of local polynomial approximations and we provide examples of existing algorithms implementing the optimal objectives},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/LBMAJFRY/Lanzillotta et al. - 2024 - Local vs Global continual learning.pdf;/Users/giulialanzillotta/Zotero/storage/32MNBIC7/2407.html}
}

@misc{lapaczExploringStabilityGap2024,
  title = {Exploring the {{Stability Gap}} in {{Continual Learning}}: {{The Role}} of the {{Classification Head}}},
  shorttitle = {Exploring the {{Stability Gap}} in {{Continual Learning}}},
  author = {{\L}apacz, Wojciech and Marczak, Daniel and Szatkowski, Filip and Trzci{\'n}ski, Tomasz},
  year = 2024,
  month = nov,
  number = {arXiv:2411.04723},
  eprint = {2411.04723},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2411.04723},
  urldate = {2025-09-16},
  abstract = {Continual learning (CL) has emerged as a critical area in machine learning, enabling neural networks to learn from evolving data distributions while mitigating catastrophic forgetting. However, recent research has identified the stability gap -- a phenomenon where models initially lose performance on previously learned tasks before partially recovering during training. Such learning dynamics are contradictory to the intuitive understanding of stability in continual learning where one would expect the performance to degrade gradually instead of rapidly decreasing and then partially recovering later. To better understand and alleviate the stability gap, we investigate it at different levels of the neural network architecture, particularly focusing on the role of the classification head. We introduce the nearest-mean classifier (NMC) as a tool to attribute the influence of the backbone and the classification head on the stability gap. Our experiments demonstrate that NMC not only improves final performance, but also significantly enhances training stability across various continual learning benchmarks, including CIFAR100, ImageNet100, CUB-200, and FGVC Aircrafts. Moreover, we find that NMC also reduces task-recency bias. Our analysis provides new insights into the stability gap and suggests that the primary contributor to this phenomenon is the linear head, rather than the insufficient representation learning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/QMPNDEQH/Åapacz et al. - 2024 - Exploring the Stability Gap in Continual Learning The Role of the Classification Head.pdf;/Users/giulialanzillotta/Zotero/storage/BIE68WIG/2411.html}
}

@misc{lauLocalLearningCoefficient2024,
  title = {The {{Local Learning Coefficient}}: {{A Singularity-Aware Complexity Measure}}},
  shorttitle = {The {{Local Learning Coefficient}}},
  author = {Lau, Edmund and Furman, Zach and Wang, George and Murfet, Daniel and Wei, Susan},
  year = 2024,
  month = sep,
  number = {arXiv:2308.12108},
  eprint = {2308.12108},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2308.12108},
  urldate = {2025-04-30},
  abstract = {The Local Learning Coefficient (LLC) is introduced as a novel complexity measure for deep neural networks (DNNs). Recognizing the limitations of traditional complexity measures, the LLC leverages Singular Learning Theory (SLT), which has long recognized the significance of singularities in the loss landscape geometry. This paper provides an extensive exploration of the LLC's theoretical underpinnings, offering both a clear definition and intuitive insights into its application. Moreover, we propose a new scalable estimator for the LLC, which is then effectively applied across diverse architectures including deep linear networks up to 100M parameters, ResNet image models, and transformer language models. Empirical evidence suggests that the LLC provides valuable insights into how training heuristics might influence the effective complexity of DNNs. Ultimately, the LLC emerges as a crucial tool for reconciling the apparent contradiction between deep learning's complexity and the principle of parsimony.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/PJ393B8B/Lau et al. - 2024 - The Local Learning Coefficient A Singularity-Aware Complexity Measure.pdf;/Users/giulialanzillotta/Zotero/storage/QEMS4R3M/2308.html}
}

@inproceedings{lecarpentierLipschitzLifelongReinforcement2021,
  title = {Lipschitz Lifelong Reinforcement Learning},
  booktitle = {Proceedings of the {{AAAI Conference}} on {{Artificial Intelligence}}},
  author = {Lecarpentier, Erwan and Abel, David and Asadi, Kavosh and Jinnai, Yuu and Rachelson, Emmanuel and Littman, Michael L},
  year = 2021,
  volume = {35},
  pages = {8270--8278}
}

@article{lecarpentierNonstationaryMarkovDecision2019,
  title = {Non-Stationary {{Markov}} Decision Processes, a Worst-Case Approach Using Model-Based Reinforcement Learning},
  author = {Lecarpentier, Erwan and Rachelson, Emmanuel},
  year = 2019,
  journal = {Advances in neural information processing systems},
  volume = {32}
}

@phdthesis{lecarpentierReinforcementLearningNonstationary2020,
  title = {Reinforcement Learning in Non-Stationary Environments},
  author = {Lecarpentier, Erwan},
  year = 2020,
  school = {Toulouse, ISAE}
}

@incollection{lecunEfficientBackProp1998,
  title = {Efficient {{BackProp}}},
  booktitle = {Neural {{Networks}}: {{Tricks}} of the {{Trade}}},
  author = {LeCun, Yann and Bottou, Leon and Orr, Genevieve B. and M{\"u}ller, Klaus -Robert},
  editor = {Orr, Genevieve B. and M{\"u}ller, Klaus-Robert},
  year = 1998,
  pages = {9--50},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/3-540-49430-8_2},
  urldate = {2025-04-22},
  abstract = {The convergence of back-propagation learning is analyzed so as to explain common phenomenon observedb y practitioners. Many undesirable behaviors of backprop can be avoided with tricks that are rarely exposedin serious technical publications. This paper gives some of those tricks, ando.ers explanations of why they work. Many authors have suggested that second-order optimization methods are advantageous for neural net training. It is shown that most ``classical'' second-order methods are impractical for large neural networks. A few methods are proposed that do not have these limitations.},
  isbn = {978-3-540-49430-0},
  langid = {english},
  keywords = {Conjugate Gradient,Handwritten Digit,Learning Rate,Neural Information Processing System,Newton Algorithm},
  file = {/Users/giulialanzillotta/Zotero/storage/G45G5BDG/LeCun et al. - 1998 - Efficient BackProp.pdf}
}

@article{leeWideNeuralNetworks2020,
  title = {Wide {{Neural Networks}} of {{Any Depth Evolve}} as {{Linear Models Under Gradient Descent}}},
  author = {Lee, Jaehoon and Xiao, Lechao and Schoenholz, Samuel S. and Bahri, Yasaman and Novak, Roman and {Sohl-Dickstein}, Jascha and Pennington, Jeffrey},
  year = 2020,
  month = dec,
  journal = {Journal of Statistical Mechanics: Theory and Experiment},
  volume = {2020},
  number = {12},
  eprint = {1902.06720},
  primaryclass = {stat},
  pages = {124002},
  issn = {1742-5468},
  doi = {10.1088/1742-5468/abc62b},
  urldate = {2025-07-28},
  abstract = {A longstanding goal in deep learning research has been to precisely characterize training and generalization. However, the often complex loss landscapes of neural networks have made a theory of learning dynamics elusive. In this work, we show that for wide neural networks the learning dynamics simplify considerably and that, in the infinite width limit, they are governed by a linear model obtained from the first-order Taylor expansion of the network around its initial parameters. Furthermore, mirroring the correspondence between wide Bayesian neural networks and Gaussian processes, gradient-based training of wide neural networks with a squared loss produces test set predictions drawn from a Gaussian process with a particular compositional kernel. While these theoretical results are only exact in the infinite width limit, we nevertheless find excellent empirical agreement between the predictions of the original network and those of the linearized version even for finite practically-sized networks. This agreement is robust across different architectures, optimization methods, and loss functions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/XPTCRBBP/Lee et al. - 2020 - Wide Neural Networks of Any Depth Evolve as Linear Models Under Gradient Descent.pdf;/Users/giulialanzillotta/Zotero/storage/26AML9EB/1902.html}
}

@inproceedings{lesortRethinkingContinualLearning2022,
  title = {Rethinking {{Continual Learning}} from a {{Lifelong Learning Perspective}}},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR}})},
  author = {Lesort, Timoth{\'e}e and Delange, Maxime and Twardowski, Bartlomiej and Gepperth, Alexander and Stojanov, Slobodan},
  year = 2022
}

@misc{liLearningForgetting2017,
  title = {Learning without {{Forgetting}}},
  author = {Li, Zhizhong and Hoiem, Derek},
  year = 2017
}

@article{liLearningForgetting2017a,
  title = {Learning without Forgetting},
  author = {Li, Zhizhong and Hoiem, Derek},
  year = 2017,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  publisher = {IEEE},
  keywords = {Distillation}
}

@misc{linNotForgettingContinualLearning2022,
  title = {Beyond {{Not-Forgetting}}: {{Continual Learning}} with {{Backward Knowledge Transfer}}},
  shorttitle = {Beyond {{Not-Forgetting}}},
  author = {Lin, Sen and Yang, Li and Fan, Deliang and Zhang, Junshan},
  year = 2022,
  month = nov,
  number = {arXiv:2211.00789},
  eprint = {2211.00789},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2211.00789},
  urldate = {2025-10-01},
  abstract = {By learning a sequence of tasks continually, an agent in continual learning (CL) can improve the learning performance of both a new task and `old' tasks by leveraging the forward knowledge transfer and the backward knowledge transfer, respectively. However, most existing CL methods focus on addressing catastrophic forgetting in neural networks by minimizing the modification of the learnt model for old tasks. This inevitably limits the backward knowledge transfer from the new task to the old tasks, because judicious model updates could possibly improve the learning performance of the old tasks as well. To tackle this problem, we first theoretically analyze the conditions under which updating the learnt model of old tasks could be beneficial for CL and also lead to backward knowledge transfer, based on the gradient projection onto the input subspaces of old tasks. Building on the theoretical analysis, we next develop a ContinUal learning method with Backward knowlEdge tRansfer (CUBER), for a fixed capacity neural network without data replay. In particular, CUBER first characterizes the task correlation to identify the positively correlated old tasks in a layer-wise manner, and then selectively modifies the learnt model of the old tasks when learning the new task. Experimental studies show that CUBER can even achieve positive backward knowledge transfer on several existing CL benchmarks for the first time without data replay, where the related baselines still suffer from catastrophic forgetting (negative backward knowledge transfer). The superior performance of CUBER on the backward knowledge transfer also leads to higher accuracy accordingly.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/P7DXLJR2/Lin et al. - 2022 - Beyond Not-Forgetting Continual Learning with Backward Knowledge Transfer.pdf;/Users/giulialanzillotta/Zotero/storage/ED4NELAA/2211.html}
}

@article{linSelfimprovingReactiveAgents1992,
  title = {Self-Improving Reactive Agents Based on Reinforcement Learning, Planning and Teaching},
  author = {Lin, Long-Ji},
  year = 1992,
  journal = {Machine learning},
  volume = {8},
  pages = {293--321},
  publisher = {Kluwer Academic Publishers}
}

@article{linTheoryForgettingGeneralization,
  title = {Theory on {{Forgetting}} and {{Generalization}} of {{Continual Learning}}},
  author = {Lin, Sen and Ju, Peizhong and Liang, Yingbin and Shroff, Ness},
  abstract = {Continual learning (CL), which aims to learn a sequence of tasks, has attracted significant recent attention. However, most work has focused on the experimental performance of CL, and theoretical studies of CL are still limited. In particular, there is a lack of understanding on what factors are important and how they affect ``catastrophic forgetting'' and generalization performance. To fill this gap, our theoretical analysis, under overparameterized linear models, provides the first-known explicit form of the expected forgetting and generalization error for a general CL setup with an arbitrary number of tasks. Further analysis of such a key result yields a number of theoretical explanations about how overparameterization, task similarity, and task ordering affect both forgetting and generalization error of CL. More interestingly, by conducting experiments on real datasets using deep neural networks (DNNs), we show that some of these insights even go beyond the linear models and can be carried over to practical setups. In particular, we use concrete examples to show that our results not only explain some interesting empirical observations in recent studies, but also motivate better practical algorithm designs of CL.},
  langid = {english},
  file = {/Users/giulialanzillotta/Zotero/storage/K9GB2B4G/Lin et al. - Theory on Forgetting and Generalization of Continual Learning.pdf}
}

@inproceedings{linTheoryForgettingGeneralization2023,
  title = {Theory on Forgetting and Generalization of Continual Learning},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Lin, Sen and Ju, Peizhong and Liang, Yingbin and Shroff, Ness},
  year = 2023,
  pages = {21078--21100},
  publisher = {PMLR}
}

@misc{linTRGPTrustRegion2022,
  title = {{{TRGP}}: {{Trust Region Gradient Projection}} for {{Continual Learning}}},
  shorttitle = {{{TRGP}}},
  author = {Lin, Sen and Yang, Li and Fan, Deliang and Zhang, Junshan},
  year = 2022,
  month = feb,
  number = {arXiv:2202.02931},
  eprint = {2202.02931},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2202.02931},
  urldate = {2025-10-01},
  abstract = {Catastrophic forgetting is one of the major challenges in continual learning. To address this issue, some existing methods put restrictive constraints on the optimization space of the new task for minimizing the interference to old tasks. However, this may lead to unsatisfactory performance for the new task, especially when the new task is strongly correlated with old tasks. To tackle this challenge, we propose Trust Region Gradient Projection (TRGP) for continual learning to facilitate the forward knowledge transfer based on an efficient characterization of task correlation. Particularly, we introduce a notion of `trust region' to select the most related old tasks for the new task in a layer-wise and single-shot manner, using the norm of gradient projection onto the subspace spanned by task inputs. Then, a scaled weight projection is proposed to cleverly reuse the frozen weights of the selected old tasks in the trust region through a layer-wise scaling matrix. By jointly optimizing the scaling matrices and the model, where the model is updated along the directions orthogonal to the subspaces of old tasks, TRGP can effectively prompt knowledge transfer without forgetting. Extensive experiments show that our approach achieves significant improvement over related state-of-the-art methods.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/WSRZQPQ9/Lin et al. - 2022 - TRGP Trust Region Gradient Projection for Continual Learning.pdf;/Users/giulialanzillotta/Zotero/storage/VFE9VY26/2202.html}
}

@misc{liuGeneralizingDecouplingNeural2023,
  title = {Generalizing and {{Decoupling Neural Collapse}} via {{Hyperspherical Uniformity Gap}}},
  author = {Liu, Weiyang and Yu, Longhui and Weller, Adrian and Sch{\"o}lkopf, Bernhard},
  year = 2023,
  month = apr,
  number = {arXiv:2303.06484},
  eprint = {2303.06484},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2303.06484},
  urldate = {2025-09-16},
  abstract = {The neural collapse (NC) phenomenon describes an underlying geometric symmetry for deep neural networks, where both deeply learned features and classifiers converge to a simplex equiangular tight frame. It has been shown that both cross-entropy loss and mean square error can provably lead to NC. We remove NC's key assumption on the feature dimension and the number of classes, and then present a generalized neural collapse (GNC) hypothesis that effectively subsumes the original NC. Inspired by how NC characterizes the training target of neural networks, we decouple GNC into two objectives: minimal intra-class variability and maximal inter-class separability. We then use hyperspherical uniformity (which characterizes the degree of uniformity on the unit hypersphere) as a unified framework to quantify these two objectives. Finally, we propose a general objective -- hyperspherical uniformity gap (HUG), which is defined by the difference between inter-class and intra-class hyperspherical uniformity. HUG not only provably converges to GNC, but also decouples GNC into two separate objectives. Unlike cross-entropy loss that couples intra-class compactness and inter-class separability, HUG enjoys more flexibility and serves as a good alternative loss function. Empirical results show that HUG works well in terms of generalization and robustness.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/5U8VYSJ3/Liu et al. - 2023 - Generalizing and Decoupling Neural Collapse via Hyperspherical Uniformity Gap.pdf}
}

@misc{liuGeneralizingDecouplingNeural2023a,
  title = {Generalizing and {{Decoupling Neural Collapse}} via {{Hyperspherical Uniformity Gap}}},
  author = {Liu, Weiyang and Yu, Longhui and Weller, Adrian and Sch{\"o}lkopf, Bernhard},
  year = 2023,
  month = apr,
  number = {arXiv:2303.06484},
  eprint = {2303.06484},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2303.06484},
  urldate = {2025-09-16},
  abstract = {The neural collapse (NC) phenomenon describes an underlying geometric symmetry for deep neural networks, where both deeply learned features and classifiers converge to a simplex equiangular tight frame. It has been shown that both cross-entropy loss and mean square error can provably lead to NC. We remove NC's key assumption on the feature dimension and the number of classes, and then present a generalized neural collapse (GNC) hypothesis that effectively subsumes the original NC. Inspired by how NC characterizes the training target of neural networks, we decouple GNC into two objectives: minimal intra-class variability and maximal inter-class separability. We then use hyperspherical uniformity (which characterizes the degree of uniformity on the unit hypersphere) as a unified framework to quantify these two objectives. Finally, we propose a general objective -- hyperspherical uniformity gap (HUG), which is defined by the difference between inter-class and intra-class hyperspherical uniformity. HUG not only provably converges to GNC, but also decouples GNC into two separate objectives. Unlike cross-entropy loss that couples intra-class compactness and inter-class separability, HUG enjoys more flexibility and serves as a good alternative loss function. Empirical results show that HUG works well in terms of generalization and robustness.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/KE3R4C4Y/Liu et al. - 2023 - Generalizing and Decoupling Neural Collapse via Hyperspherical Uniformity Gap.pdf;/Users/giulialanzillotta/Zotero/storage/SLYU5452/2303.html}
}

@misc{liuGenerativeFeatureReplay2020,
  title = {Generative {{Feature Replay For Class-Incremental Learning}}},
  author = {Liu, Xialei and Wu, Chenshen and Menta, Mikel and Herranz, Luis and Raducanu, Bogdan and Bagdanov, Andrew D. and Jui, Shangling and van de Weijer, Joost},
  year = 2020,
  month = apr,
  number = {arXiv:2004.09199},
  eprint = {2004.09199},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2004.09199},
  urldate = {2025-10-14},
  abstract = {Humans are capable of learning new tasks without forgetting previous ones, while neural networks fail due to catastrophic forgetting between new and previously-learned tasks. We consider a class-incremental setting which means that the task-ID is unknown at inference time. The imbalance between old and new classes typically results in a bias of the network towards the newest ones. This imbalance problem can either be addressed by storing exemplars from previous tasks, or by using image replay methods. However, the latter can only be applied to toy datasets since image generation for complex datasets is a hard problem. We propose a solution to the imbalance problem based on generative feature replay which does not require any exemplars. To do this, we split the network into two parts: a feature extractor and a classifier. To prevent forgetting, we combine generative feature replay in the classifier with feature distillation in the feature extractor. Through feature generation, our method reduces the complexity of generative replay and prevents the imbalance problem. Our approach is computationally efficient and scalable to large datasets. Experiments confirm that our approach achieves state-of-the-art results on CIFAR-100 and ImageNet, while requiring only a fraction of the storage needed for exemplar-based continual learning. Code available at {\textbackslash}url\{https://github.com/xialeiliu/GFR-IL\}.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/L562VIDM/Liu et al. - 2020 - Generative Feature Replay For Class-Incremental Learning.pdf;/Users/giulialanzillotta/Zotero/storage/4DKVJAN9/2004.html}
}

@inproceedings{liuInducingNeuralCollapse2023,
  title = {Inducing Neural Collapse in Deep Long-Tailed Learning},
  booktitle = {International Conference on Artificial Intelligence and Statistics},
  author = {Liu, Xuantong and Zhang, Jianfeng and Hu, Tianyang and Cao, He and Yao, Yuan and Pan, Lujia},
  year = 2023,
  pages = {11534--11544},
  publisher = {PMLR}
}

@inproceedings{liuNonstationaryBanditLearning2023,
  title = {Nonstationary Bandit Learning via Predictive Sampling},
  booktitle = {International {{Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Liu, Yueyang and Van Roy, Benjamin and Xu, Kuang},
  year = 2023,
  pages = {6215--6244},
  publisher = {PMLR}
}

@article{liuTheoreticalPerspectivesKnowledge2025,
  title = {Theoretical {{Perspectives}} on {{Knowledge Distillation}}: {{A Review}}},
  shorttitle = {Theoretical {{Perspectives}} on {{Knowledge Distillation}}},
  author = {Liu, Chuanhui and Yin, Haoyun and Wang, Xiao},
  year = 2025,
  journal = {WIREs Computational Statistics},
  volume = {17},
  number = {4},
  pages = {e70049},
  issn = {1939-0068},
  doi = {10.1002/wics.70049},
  urldate = {2025-10-13},
  abstract = {Knowledge distillation (KD) is a widely used technique for transferring predictive behavior from a high-capacity teacher model to a compact student model, providing a scalable strategy to compress and adapt foundation models to downstream tasks while allowing the distillation process to be tailored toward the target application. Its success spans both computer vision and natural language processing domains, where KD enables faster inference and greater accessibility without requiring costly retraining of large models. Despite its empirical prominence, the body of work addressing its theoretical justification remains relatively sparse. In this work, we present a systematic overview of the theoretical foundations of knowledge distillation. Specifically, we examine perspectives that frame KD as smoothing label distributions, regularizing empirical risk, and approximating mutual information, aiming to bridge the gap between practical utility and theoretical insight. We evaluate the impact of each theoretical perspective through image classification experiments on CIFAR-10, examining how these interpretations manifest in practical distillation outcomes. This article is categorized under: Statistical Learning and Exploratory Methods of the Data Sciences {$>$} Modeling Methods Statistical Learning and Exploratory Methods of the Data Sciences {$>$} Neural Networks Statistical Models {$>$} Classification Models},
  copyright = {{\copyright} 2025 The Author(s). WIREs Computational Statistics published by Wiley Periodicals LLC.},
  langid = {english},
  keywords = {classification,knowledge distillation,learning theory,model compression},
  file = {/Users/giulialanzillotta/Zotero/storage/IT4UUPEW/Liu et al. - 2025 - Theoretical Perspectives on Knowledge Distillation A Review.pdf}
}

@article{liuTheoreticalPerspectivesKnowledge2025a,
  title = {Theoretical {{Perspectives}} on {{Knowledge Distillation}}: {{A Review}}},
  shorttitle = {Theoretical {{Perspectives}} on {{Knowledge Distillation}}},
  author = {Liu, Chuanhui and Yin, Haoyun and Wang, Xiao},
  year = 2025,
  journal = {WIREs Computational Statistics},
  volume = {17},
  number = {4},
  pages = {e70049},
  issn = {1939-0068},
  doi = {10.1002/wics.70049},
  urldate = {2025-10-17},
  abstract = {Knowledge distillation (KD) is a widely used technique for transferring predictive behavior from a high-capacity teacher model to a compact student model, providing a scalable strategy to compress and adapt foundation models to downstream tasks while allowing the distillation process to be tailored toward the target application. Its success spans both computer vision and natural language processing domains, where KD enables faster inference and greater accessibility without requiring costly retraining of large models. Despite its empirical prominence, the body of work addressing its theoretical justification remains relatively sparse. In this work, we present a systematic overview of the theoretical foundations of knowledge distillation. Specifically, we examine perspectives that frame KD as smoothing label distributions, regularizing empirical risk, and approximating mutual information, aiming to bridge the gap between practical utility and theoretical insight. We evaluate the impact of each theoretical perspective through image classification experiments on CIFAR-10, examining how these interpretations manifest in practical distillation outcomes. This article is categorized under: Statistical Learning and Exploratory Methods of the Data Sciences {$>$} Modeling Methods Statistical Learning and Exploratory Methods of the Data Sciences {$>$} Neural Networks Statistical Models {$>$} Classification Models},
  langid = {english},
  keywords = {classification,knowledge distillation,learning theory,model compression},
  file = {/Users/giulialanzillotta/Zotero/storage/4K6LZMN8/Liu et al. - 2025 - Theoretical Perspectives on Knowledge Distillation A Review.pdf;/Users/giulialanzillotta/Zotero/storage/9KHKH4HU/wics.html}
}

@misc{looGeneralizedVariationalContinual2020,
  title = {Generalized {{Variational Continual Learning}}},
  author = {Loo, Noel and Swaroop, Siddharth and Turner, Richard E.},
  year = 2020,
  month = nov,
  number = {arXiv:2011.12328},
  eprint = {2011.12328},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2011.12328},
  urldate = {2025-06-30},
  abstract = {Continual learning deals with training models on new tasks and datasets in an online fashion. One strand of research has used probabilistic regularization for continual learning, with two of the main approaches in this vein being Online Elastic Weight Consolidation (Online EWC) and Variational Continual Learning (VCL). VCL employs variational inference, which in other settings has been improved empirically by applying likelihood-tempering. We show that applying this modification to VCL recovers Online EWC as a limiting case, allowing for interpolation between the two approaches. We term the general algorithm Generalized VCL (GVCL). In order to mitigate the observed overpruning effect of VI, we take inspiration from a common multi-task architecture, neural networks with task-specific FiLM layers, and find that this addition leads to significant performance gains, specifically for variational methods. In the small-data regime, GVCL strongly outperforms existing baselines. In larger datasets, GVCL with FiLM layers outperforms or is competitive with existing baselines in terms of accuracy, whilst also providing significantly better calibration.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/IF6AZWLB/Loo et al. - 2020 - Generalized Variational Continual Learning.pdf;/Users/giulialanzillotta/Zotero/storage/TKK5NSM8/2011.html}
}

@article{lopez-pazGradientEpisodicMemory2017,
  title = {Gradient Episodic Memory for Continual Learning},
  author = {{Lopez-Paz}, David and Ranzato, Marc'Aurelio},
  year = 2017,
  journal = {Advances in neural information processing systems},
  volume = {30}
}

@misc{LossPlasticitySutton,
  title = {Loss of Plasticity Sutton - {{Google Search}}},
  urldate = {2025-10-13},
  howpublished = {https://www.nature.com/articles/s41586-024-07711-7}
}

@article{luNeuralCollapseCrossentropy2022,
  title = {Neural Collapse under Cross-Entropy Loss},
  author = {Lu, Jianfeng and Steinerberger, Stefan},
  year = 2022,
  journal = {Applied and Computational Harmonic Analysis},
  volume = {59},
  pages = {224--241},
  publisher = {Elsevier}
}

@misc{MacTeXTeXUsers,
  title = {{{MacTeX}} - {{TeX Users Group}}},
  urldate = {2025-05-06},
  howpublished = {https://tug.org/mactex/mactex-download.html},
  file = {/Users/giulialanzillotta/Zotero/storage/SJDTS6TV/mactex-download.html}
}

@misc{madaanRepresentationalContinuityUnsupervised2022,
  title = {Representational {{Continuity}} for {{Unsupervised Continual Learning}}},
  author = {Madaan, Divyam and Yoon, Jaehong and Li, Yuanchun and Liu, Yunxin and Hwang, Sung Ju},
  year = 2022,
  month = apr,
  number = {arXiv:2110.06976},
  eprint = {2110.06976},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2110.06976},
  urldate = {2025-10-03},
  abstract = {Continual learning (CL) aims to learn a sequence of tasks without forgetting the previously acquired knowledge. However, recent CL advances are restricted to supervised continual learning (SCL) scenarios. Consequently, they are not scalable to real-world applications where the data distribution is often biased and unannotated. In this work, we focus on unsupervised continual learning (UCL), where we learn the feature representations on an unlabelled sequence of tasks and show that reliance on annotated data is not necessary for continual learning. We conduct a systematic study analyzing the learned feature representations and show that unsupervised visual representations are surprisingly more robust to catastrophic forgetting, consistently achieve better performance, and generalize better to out-of-distribution tasks than SCL. Furthermore, we find that UCL achieves a smoother loss landscape through qualitative analysis of the learned representations and learns meaningful feature representations. Additionally, we propose Lifelong Unsupervised Mixup (LUMP), a simple yet effective technique that interpolates between the current task and previous tasks' instances to alleviate catastrophic forgetting for unsupervised representations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/RBJLYA96/Madaan et al. - 2022 - Representational Continuity for Unsupervised Continual Learning.pdf;/Users/giulialanzillotta/Zotero/storage/M44Z99ZZ/2110.html}
}

@misc{madaanRepresentationalContinuityUnsupervised2022a,
  title = {Representational {{Continuity}} for {{Unsupervised Continual Learning}}},
  author = {Madaan, Divyam and Yoon, Jaehong and Li, Yuanchun and Liu, Yunxin and Hwang, Sung Ju},
  year = 2022,
  month = apr,
  number = {arXiv:2110.06976},
  eprint = {2110.06976},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2110.06976},
  urldate = {2025-10-03},
  abstract = {Continual learning (CL) aims to learn a sequence of tasks without forgetting the previously acquired knowledge. However, recent CL advances are restricted to supervised continual learning (SCL) scenarios. Consequently, they are not scalable to real-world applications where the data distribution is often biased and unannotated. In this work, we focus on unsupervised continual learning (UCL), where we learn the feature representations on an unlabelled sequence of tasks and show that reliance on annotated data is not necessary for continual learning. We conduct a systematic study analyzing the learned feature representations and show that unsupervised visual representations are surprisingly more robust to catastrophic forgetting, consistently achieve better performance, and generalize better to out-of-distribution tasks than SCL. Furthermore, we find that UCL achieves a smoother loss landscape through qualitative analysis of the learned representations and learns meaningful feature representations. Additionally, we propose Lifelong Unsupervised Mixup (LUMP), a simple yet effective technique that interpolates between the current task and previous tasks' instances to alleviate catastrophic forgetting for unsupervised representations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/7QNNHQ95/Madaan et al. - 2022 - Representational Continuity for Unsupervised Continual Learning.pdf;/Users/giulialanzillotta/Zotero/storage/JBMJ8D6W/2110.html}
}

@misc{maddoxConditioningSparseVariational2021,
  title = {Conditioning {{Sparse Variational Gaussian Processes}} for {{Online Decision-making}}},
  author = {Maddox, Wesley J. and Stanton, Samuel and Wilson, Andrew Gordon},
  year = 2021,
  month = oct,
  number = {arXiv:2110.15172},
  eprint = {2110.15172},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2110.15172},
  urldate = {2025-10-14},
  abstract = {With a principled representation of uncertainty and closed form posterior updates, Gaussian processes (GPs) are a natural choice for online decision making. However, Gaussian processes typically require at least \${\textbackslash}mathcal\{O\}(n{\textasciicircum}2)\$ computations for \$n\$ training points, limiting their general applicability. Stochastic variational Gaussian processes (SVGPs) can provide scalable inference for a dataset of fixed size, but are difficult to efficiently condition on new data. We propose online variational conditioning (OVC), a procedure for efficiently conditioning SVGPs in an online setting that does not require re-training through the evidence lower bound with the addition of new data. OVC enables the pairing of SVGPs with advanced look-ahead acquisition functions for black-box optimization, even with non-Gaussian likelihoods. We show OVC provides compelling performance in a range of applications including active learning of malaria incidence, and reinforcement learning on MuJoCo simulated robotic control tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/P6E9IAV3/Maddox et al. - 2021 - Conditioning Sparse Variational Gaussian Processes for Online Decision-making.pdf;/Users/giulialanzillotta/Zotero/storage/HS4K8C3I/2110.html}
}

@misc{maddoxConditioningSparseVariational2021a,
  title = {Conditioning {{Sparse Variational Gaussian Processes}} for {{Online Decision-making}}},
  author = {Maddox, Wesley J. and Stanton, Samuel and Wilson, Andrew Gordon},
  year = 2021,
  month = oct,
  number = {arXiv:2110.15172},
  eprint = {2110.15172},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2110.15172},
  urldate = {2025-10-14},
  abstract = {With a principled representation of uncertainty and closed form posterior updates, Gaussian processes (GPs) are a natural choice for online decision making. However, Gaussian processes typically require at least \${\textbackslash}mathcal\{O\}(n{\textasciicircum}2)\$ computations for \$n\$ training points, limiting their general applicability. Stochastic variational Gaussian processes (SVGPs) can provide scalable inference for a dataset of fixed size, but are difficult to efficiently condition on new data. We propose online variational conditioning (OVC), a procedure for efficiently conditioning SVGPs in an online setting that does not require re-training through the evidence lower bound with the addition of new data. OVC enables the pairing of SVGPs with advanced look-ahead acquisition functions for black-box optimization, even with non-Gaussian likelihoods. We show OVC provides compelling performance in a range of applications including active learning of malaria incidence, and reinforcement learning on MuJoCo simulated robotic control tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/536BLSXQ/Maddox et al. - 2021 - Conditioning Sparse Variational Gaussian Processes for Online Decision-making.pdf;/Users/giulialanzillotta/Zotero/storage/YM85QX5A/2110.html}
}

@article{majiFineGrainedVisualClassification2013,
  title = {Fine-{{Grained Visual Classification}} of {{Aircraft}}},
  author = {Maji, Subhransu and Rahtu, Esa and Kannala, Juho and Blaschko, Matthew B. and Vedaldi, Andrea},
  year = 2013,
  journal = {CoRR},
  volume = {abs/1306.5151},
  eprint = {1306.5151},
  archiveprefix = {arXiv}
}

@inproceedings{mallyaPacknetAddingMultiple2018,
  title = {Packnet: {{Adding}} Multiple Tasks to a Single Network by Iterative Pruning},
  booktitle = {Proceedings of the {{IEEE}} Conference on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Mallya, Arun and Lazebnik, Svetlana},
  year = 2018,
  pages = {7765--7773}
}

@misc{mallyaPackNetAddingMultiple2018a,
  title = {{{PackNet}}: {{Adding Multiple Tasks}} to a {{Single Network}} by {{Iterative Pruning}}},
  shorttitle = {{{PackNet}}},
  author = {Mallya, Arun and Lazebnik, Svetlana},
  year = 2018,
  month = may,
  number = {arXiv:1711.05769},
  eprint = {1711.05769},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1711.05769},
  urldate = {2025-10-01},
  abstract = {This paper presents a method for adding multiple tasks to a single deep neural network while avoiding catastrophic forgetting. Inspired by network pruning techniques, we exploit redundancies in large deep networks to free up parameters that can then be employed to learn new tasks. By performing iterative pruning and network re-training, we are able to sequentially "pack" multiple tasks into a single network while ensuring minimal drop in performance and minimal storage overhead. Unlike prior work that uses proxy losses to maintain accuracy on older tasks, we always optimize for the task at hand. We perform extensive experiments on a variety of network architectures and large-scale datasets, and observe much better robustness against catastrophic forgetting than prior work. In particular, we are able to add three fine-grained classification tasks to a single ImageNet-trained VGG-16 network and achieve accuracies close to those of separately trained networks for each task. Code available at https://github.com/arunmallya/packnet},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/giulialanzillotta/Zotero/storage/HC7HN9JZ/Mallya and Lazebnik - 2018 - PackNet Adding Multiple Tasks to a Single Network by Iterative Pruning.pdf;/Users/giulialanzillotta/Zotero/storage/HTVKZ3PF/1711.html}
}

@article{maltoniContinuousLearningSingleincrementaltask2018,
  title = {Continuous Learning in Single-Incremental-Task Scenarios},
  author = {Maltoni, D. and Lomonaco, V.},
  year = 2018,
  journal = {arXiv:1806.08568},
  eprint = {1806.08568},
  archiveprefix = {arXiv}
}

@misc{maQuadraticApproximationMultiscale2022,
  title = {Beyond the {{Quadratic Approximation}}: The {{Multiscale Structure}} of {{Neural Network Loss Landscapes}}},
  shorttitle = {Beyond the {{Quadratic Approximation}}},
  author = {Ma, Chao and Kunin, Daniel and Wu, Lei and Ying, Lexing},
  year = 2022,
  month = jun,
  number = {arXiv:2204.11326},
  eprint = {2204.11326},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2204.11326},
  urldate = {2025-09-30},
  abstract = {A quadratic approximation of neural network loss landscapes has been extensively used to study the optimization process of these networks. Though, it usually holds in a very small neighborhood of the minimum, it cannot explain many phenomena observed during the optimization process. In this work, we study the structure of neural network loss functions and its implication on optimization in a region beyond the reach of a good quadratic approximation. Numerically, we observe that neural network loss functions possesses a multiscale structure, manifested in two ways: (1) in a neighborhood of minima, the loss mixes a continuum of scales and grows subquadratically, and (2) in a larger region, the loss shows several separate scales clearly. Using the subquadratic growth, we are able to explain the Edge of Stability phenomenon [5] observed for the gradient descent (GD) method. Using the separate scales, we explain the working mechanism of learning rate decay by simple examples. Finally, we study the origin of the multiscale structure and propose that the non-convexity of the models and the non-uniformity of training data is one of the causes. By constructing a two-layer neural network problem we show that training data with different magnitudes give rise to different scales of the loss function, producing subquadratic growth and multiple separate scales.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/VTFXJFC9/Ma et al. - 2022 - Beyond the Quadratic Approximation the Multiscale Structure of Neural Network Loss Landscapes.pdf;/Users/giulialanzillotta/Zotero/storage/GTSRGHFH/2204.html}
}

@article{martensNewInsightsPerspectives,
  title = {New {{Insights}} and {{Perspectives}} on the {{Natural Gradient Method}}},
  author = {Martens, James},
  abstract = {Natural gradient descent is an optimization method traditionally motivated from the perspective of information geometry, and works well for many applications as an alternative to stochastic gradient descent. In this paper we critically analyze this method and its properties, and show how it can be viewed as a type of 2nd-order optimization method, with the Fisher information matrix acting as a substitute for the Hessian. In many important cases, the Fisher information matrix is shown to be equivalent to the Generalized Gauss-Newton matrix, which both approximates the Hessian, but also has certain properties that favor its use over the Hessian. This perspective turns out to have significant implications for the design of a practical and robust natural gradient optimizer, as it motivates the use of techniques like trust regions and Tikhonov regularization. Additionally, we make a series of contributions to the understanding of natural gradient and 2nd-order methods, including: a thorough analysis of the convergence speed of stochastic natural gradient descent (and more general stochastic 2nd-order methods) as applied to convex quadratics, a critical examination of the oft-used ``empirical'' approximation of the Fisher matrix, and an analysis of the (approximate) parameterization invariance property possessed by natural gradient methods (which we show also holds for certain other curvature matrices, but notably not the Hessian).},
  langid = {english},
  file = {/Users/giulialanzillotta/Zotero/storage/5EC3DE5H/Martens - New Insights and Perspectives on the Natural Gradient Method.pdf}
}

@misc{masarczykTunnelEffectBuilding2023,
  title = {The {{Tunnel Effect}}: {{Building Data Representations}} in {{Deep Neural Networks}}},
  shorttitle = {The {{Tunnel Effect}}},
  author = {Masarczyk, Wojciech and Ostaszewski, Mateusz and Imani, Ehsan and Pascanu, Razvan and Mi{\l}o{\'s}, Piotr and Trzci{\'n}ski, Tomasz},
  year = 2023,
  month = oct,
  number = {arXiv:2305.19753},
  eprint = {2305.19753},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.19753},
  urldate = {2025-09-16},
  abstract = {Deep neural networks are widely known for their remarkable effectiveness across various tasks, with the consensus that deeper networks implicitly learn more complex data representations. This paper shows that sufficiently deep networks trained for supervised image classification split into two distinct parts that contribute to the resulting data representations differently. The initial layers create linearly-separable representations, while the subsequent layers, which we refer to as {\textbackslash}textit\{the tunnel\}, compress these representations and have a minimal impact on the overall performance. We explore the tunnel's behavior through comprehensive empirical studies, highlighting that it emerges early in the training process. Its depth depends on the relation between the network's capacity and task complexity. Furthermore, we show that the tunnel degrades out-of-distribution generalization and discuss its implications for continual learning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/QYX3YSAA/Masarczyk et al. - 2023 - The Tunnel Effect Building Data Representations in Deep Neural Networks.pdf;/Users/giulialanzillotta/Zotero/storage/CD4Y8V2E/2305.html}
}

@incollection{mccloskeyCatastrophicInterferenceConnectionist1989,
  title = {Catastrophic Interference in Connectionist Networks: {{The}} Sequential Learning Problem},
  booktitle = {Psychology of Learning and Motivation},
  author = {McCloskey, Michael and Cohen, Neal J},
  year = 1989,
  volume = {24},
  pages = {109--165},
  publisher = {Elsevier}
}

@misc{mehtaEmpiricalInvestigationRole2023,
  title = {An {{Empirical Investigation}} of the {{Role}} of {{Pre-training}} in {{Lifelong Learning}}},
  author = {Mehta, Sanket Vaibhav and Patil, Darshan and Chandar, Sarath and Strubell, Emma},
  year = 2023,
  month = aug,
  number = {arXiv:2112.09153},
  eprint = {2112.09153},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2112.09153},
  urldate = {2025-10-03},
  abstract = {The lifelong learning paradigm in machine learning is an attractive alternative to the more prominent isolated learning scheme not only due to its resemblance to biological learning but also its potential to reduce energy waste by obviating excessive model re-training. A key challenge to this paradigm is the phenomenon of catastrophic forgetting. With the increasing popularity and success of pre-trained models in machine learning, we pose the question: What role does pre-training play in lifelong learning, specifically with respect to catastrophic forgetting? We investigate existing methods in the context of large, pre-trained models and evaluate their performance on a variety of text and image classification tasks, including a large-scale study using a novel data set of 15 diverse NLP tasks. Across all settings, we observe that generic pre-training implicitly alleviates the effects of catastrophic forgetting when learning multiple tasks sequentially compared to randomly initialized models. We then further investigate why pre-training alleviates forgetting in this setting. We study this phenomenon by analyzing the loss landscape, finding that pre-trained weights appear to ease forgetting by leading to wider minima. Based on this insight, we propose jointly optimizing for current task loss and loss basin sharpness to explicitly encourage wider basins during sequential fine-tuning. We show that this optimization approach outperforms several state-of-the-art task-sequential continual learning algorithms across multiple settings, occasionally even without retaining a memory that scales in size with the number of tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/6YGSIXVR/Mehta et al. - 2023 - An Empirical Investigation of the Role of Pre-training in Lifelong Learning.pdf;/Users/giulialanzillotta/Zotero/storage/8537XAEY/2112.html}
}

@misc{mehtaEmpiricalInvestigationRole2023a,
  title = {An {{Empirical Investigation}} of the {{Role}} of {{Pre-training}} in {{Lifelong Learning}}},
  author = {Mehta, Sanket Vaibhav and Patil, Darshan and Chandar, Sarath and Strubell, Emma},
  year = 2023,
  month = aug,
  number = {arXiv:2112.09153},
  eprint = {2112.09153},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2112.09153},
  urldate = {2025-10-03},
  abstract = {The lifelong learning paradigm in machine learning is an attractive alternative to the more prominent isolated learning scheme not only due to its resemblance to biological learning but also its potential to reduce energy waste by obviating excessive model re-training. A key challenge to this paradigm is the phenomenon of catastrophic forgetting. With the increasing popularity and success of pre-trained models in machine learning, we pose the question: What role does pre-training play in lifelong learning, specifically with respect to catastrophic forgetting? We investigate existing methods in the context of large, pre-trained models and evaluate their performance on a variety of text and image classification tasks, including a large-scale study using a novel data set of 15 diverse NLP tasks. Across all settings, we observe that generic pre-training implicitly alleviates the effects of catastrophic forgetting when learning multiple tasks sequentially compared to randomly initialized models. We then further investigate why pre-training alleviates forgetting in this setting. We study this phenomenon by analyzing the loss landscape, finding that pre-trained weights appear to ease forgetting by leading to wider minima. Based on this insight, we propose jointly optimizing for current task loss and loss basin sharpness to explicitly encourage wider basins during sequential fine-tuning. We show that this optimization approach outperforms several state-of-the-art task-sequential continual learning algorithms across multiple settings, occasionally even without retaining a memory that scales in size with the number of tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/HLMRE4IY/Mehta et al. - 2023 - An Empirical Investigation of the Role of Pre-training in Lifelong Learning.pdf;/Users/giulialanzillotta/Zotero/storage/WVG7N4ID/2112.html}
}

@misc{mirzadehLinearModeConnectivity2020,
  title = {Linear {{Mode Connectivity}} in {{Multitask}} and {{Continual Learning}}},
  author = {Mirzadeh, Seyed Iman and Farajtabar, Mehrdad and Gorur, Dilan and Pascanu, Razvan and Ghasemzadeh, Hassan},
  year = 2020,
  month = oct,
  number = {arXiv:2010.04495},
  eprint = {2010.04495},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2010.04495},
  urldate = {2025-04-14},
  abstract = {Continual (sequential) training and multitask (simultaneous) training are often attempting to solve the same overall objective: to find a solution that performs well on all considered tasks. The main difference is in the training regimes, where continual learning can only have access to one task at a time, which for neural networks typically leads to catastrophic forgetting. That is, the solution found for a subsequent task does not perform well on the previous ones anymore. However, the relationship between the different minima that the two training regimes arrive at is not well understood. What sets them apart? Is there a local structure that could explain the difference in performance achieved by the two different schemes? Motivated by recent work showing that different minima of the same task are typically connected by very simple curves of low error, we investigate whether multitask and continual solutions are similarly connected. We empirically find that indeed such connectivity can be reliably achieved and, more interestingly, it can be done by a linear path, conditioned on having the same initialization for both. We thoroughly analyze this observation and discuss its significance for the continual learning process. Furthermore, we exploit this finding to propose an effective algorithm that constrains the sequentially learned minima to behave as the multitask solution. We show that our method outperforms several state of the art continual learning algorithms on various vision benchmarks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/WPGIP23J/Mirzadeh et al. - 2020 - Linear Mode Connectivity in Multitask and Continual Learning.pdf;/Users/giulialanzillotta/Zotero/storage/4EJVFUMZ/2010.html}
}

@misc{mirzadehLinearModeConnectivity2020a,
  title = {Linear {{Mode Connectivity}} in {{Multitask}} and {{Continual Learning}}},
  author = {Mirzadeh, Seyed Iman and Farajtabar, Mehrdad and Gorur, Dilan and Pascanu, Razvan and Ghasemzadeh, Hassan},
  year = 2020,
  month = oct,
  number = {arXiv:2010.04495},
  eprint = {2010.04495},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2010.04495},
  urldate = {2025-04-14},
  abstract = {Continual (sequential) training and multitask (simultaneous) training are often attempting to solve the same overall objective: to find a solution that performs well on all considered tasks. The main difference is in the training regimes, where continual learning can only have access to one task at a time, which for neural networks typically leads to catastrophic forgetting. That is, the solution found for a subsequent task does not perform well on the previous ones anymore. However, the relationship between the different minima that the two training regimes arrive at is not well understood. What sets them apart? Is there a local structure that could explain the difference in performance achieved by the two different schemes? Motivated by recent work showing that different minima of the same task are typically connected by very simple curves of low error, we investigate whether multitask and continual solutions are similarly connected. We empirically find that indeed such connectivity can be reliably achieved and, more interestingly, it can be done by a linear path, conditioned on having the same initialization for both. We thoroughly analyze this observation and discuss its significance for the continual learning process. Furthermore, we exploit this finding to propose an effective algorithm that constrains the sequentially learned minima to behave as the multitask solution. We show that our method outperforms several state of the art continual learning algorithms on various vision benchmarks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/5RLKD3YL/Mirzadeh et al. - 2020 - Linear Mode Connectivity in Multitask and Continual Learning.pdf;/Users/giulialanzillotta/Zotero/storage/KUFL2N2L/2010.html}
}

@misc{mirzadehUnderstandingRoleTraining2020,
  title = {Understanding the {{Role}} of {{Training Regimes}} in {{Continual Learning}}},
  author = {Mirzadeh, Seyed Iman and Farajtabar, Mehrdad and Pascanu, Razvan and Ghasemzadeh, Hassan},
  year = 2020,
  month = jun,
  number = {arXiv:2006.06958},
  eprint = {2006.06958},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2006.06958},
  urldate = {2025-10-01},
  abstract = {Catastrophic forgetting affects the training of neural networks, limiting their ability to learn multiple tasks sequentially. From the perspective of the well established plasticity-stability dilemma, neural networks tend to be overly plastic, lacking the stability necessary to prevent the forgetting of previous knowledge, which means that as learning progresses, networks tend to forget previously seen tasks. This phenomenon coined in the continual learning literature, has attracted much attention lately, and several families of approaches have been proposed with different degrees of success. However, there has been limited prior work extensively analyzing the impact that different training regimes -- learning rate, batch size, regularization method-- can have on forgetting. In this work, we depart from the typical approach of altering the learning algorithm to improve stability. Instead, we hypothesize that the geometrical properties of the local minima found for each task play an important role in the overall degree of forgetting. In particular, we study the effect of dropout, learning rate decay, and batch size, on forming training regimes that widen the tasks' local minima and consequently, on helping it not to forget catastrophically. Our study provides practical insights to improve stability via simple yet effective techniques that outperform alternative baselines.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/IWLHX5FP/Mirzadeh et al. - 2020 - Understanding the Role of Training Regimes in Continual Learning.pdf;/Users/giulialanzillotta/Zotero/storage/558EZJ7C/2006.html}
}

@misc{mitchellSelfExpandingNeuralNetworks2024,
  title = {Self-{{Expanding Neural Networks}}},
  author = {Mitchell, Rupert and Menzenbach, Robin and Kersting, Kristian and Mundt, Martin},
  year = 2024,
  month = feb,
  number = {arXiv:2307.04526},
  eprint = {2307.04526},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2307.04526},
  urldate = {2025-10-08},
  abstract = {The results of training a neural network are heavily dependent on the architecture chosen; and even a modification of only its size, however small, typically involves restarting the training process. In contrast to this, we begin training with a small architecture, only increase its capacity as necessary for the problem, and avoid interfering with previous optimization while doing so. We thereby introduce a natural gradient based approach which intuitively expands both the width and depth of a neural network when this is likely to substantially reduce the hypothetical converged training loss. We prove an upper bound on the ``rate'' at which neurons are added, and a computationally cheap lower bound on the expansion score. We illustrate the benefits of such Self-Expanding Neural Networks with full connectivity and convolutions in both classification and regression problems, including those where the appropriate architecture size is substantially uncertain a priori.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/BMKMMKVB/Mitchell et al. - 2024 - Self-Expanding Neural Networks.pdf;/Users/giulialanzillotta/Zotero/storage/YYMBSKS7/2307.html}
}

@article{mixonNeuralCollapseUnconstrained2022,
  title = {Neural Collapse with Unconstrained Features},
  author = {Mixon, Dustin G and Parshall, Hans and Pi, Jianzong},
  year = 2022,
  journal = {Sampling Theory, Signal Processing, and Data Analysis},
  volume = {20},
  number = {2},
  pages = {11},
  publisher = {Springer}
}

@article{mnihHumanlevelControlDeep2015,
  title = {Human-Level Control through Deep Reinforcement Learning},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and Petersen, Stig and others},
  year = 2015,
  journal = {Nature},
  volume = {518},
  number = {7540},
  pages = {529--533},
  publisher = {Nature Publishing Group}
}

@inproceedings{mokhtariOnlineOptimizationDynamic2016,
  title = {Online Optimization in Dynamic Environments: {{Improved}} Regret Rates for Strongly Convex Problems},
  booktitle = {2016 {{IEEE}} 55th {{Conference}} on {{Decision}} and {{Control}} ({{CDC}})},
  author = {Mokhtari, Aryan and Shahrampour, Shahin and Jadbabaie, Ali and Ribeiro, Alejandro},
  year = 2016,
  pages = {7195--7201},
  publisher = {IEEE}
}

@article{moslemiSurveyKnowledgeDistillation2024,
  title = {A Survey on Knowledge Distillation: {{Recent}} Advancements},
  shorttitle = {A Survey on Knowledge Distillation},
  author = {Moslemi, Amir and Briskina, Anna and Dang, Zubeka and Li, Jason},
  year = 2024,
  month = dec,
  journal = {Machine Learning with Applications},
  volume = {18},
  pages = {100605},
  issn = {2666-8270},
  doi = {10.1016/j.mlwa.2024.100605},
  urldate = {2025-10-13},
  abstract = {Deep learning has achieved notable success across academia, medicine, and industry. Its ability to identify complex patterns in large-scale data and to manage millions of parameters has made it highly advantageous. However, deploying deep learning models presents a significant challenge due to their high computational demands. Knowledge distillation (KD) has emerged as a key technique for model compression and efficient knowledge transfer, enabling the deployment of deep learning models on resource-limited devices without compromising performance. This survey examines recent advancements in KD, highlighting key innovations in architectures, training paradigms, and application domains. We categorize contemporary KD methods into traditional approaches, such as response-based, feature-based, and relation-based knowledge distillation, and novel advanced paradigms, including self-distillation, cross-modal distillation, and adversarial distillation strategies. Additionally, we discuss emerging challenges, particularly in the context of distillation under limited data scenarios, privacy-preserving KD, and the interplay with other model compression techniques like quantization. Our survey also explores applications across computer vision, natural language processing, and multimodal tasks, where KD has driven performance improvements and enhanced model compression. This review aims to provide researchers and practitioners with a comprehensive understanding of the state-of-the-art in knowledge distillation, bridging foundational concepts with the latest methodologies and practical implications.},
  keywords = {Adversarial distillation,Deep learning,Knowledge distillation,Model compression,Self-distillation},
  file = {/Users/giulialanzillotta/Zotero/storage/SJTK443F/S2666827024000811.html}
}

@article{moslemiSurveyKnowledgeDistillation2024a,
  title = {A Survey on Knowledge Distillation: {{Recent}} Advancements},
  shorttitle = {A Survey on Knowledge Distillation},
  author = {Moslemi, Amir and Briskina, Anna and Dang, Zubeka and Li, Jason},
  year = 2024,
  month = dec,
  journal = {Machine Learning with Applications},
  volume = {18},
  pages = {100605},
  issn = {2666-8270},
  doi = {10.1016/j.mlwa.2024.100605},
  urldate = {2025-10-17},
  abstract = {Deep learning has achieved notable success across academia, medicine, and industry. Its ability to identify complex patterns in large-scale data and to manage millions of parameters has made it highly advantageous. However, deploying deep learning models presents a significant challenge due to their high computational demands. Knowledge distillation (KD) has emerged as a key technique for model compression and efficient knowledge transfer, enabling the deployment of deep learning models on resource-limited devices without compromising performance. This survey examines recent advancements in KD, highlighting key innovations in architectures, training paradigms, and application domains. We categorize contemporary KD methods into traditional approaches, such as response-based, feature-based, and relation-based knowledge distillation, and novel advanced paradigms, including self-distillation, cross-modal distillation, and adversarial distillation strategies. Additionally, we discuss emerging challenges, particularly in the context of distillation under limited data scenarios, privacy-preserving KD, and the interplay with other model compression techniques like quantization. Our survey also explores applications across computer vision, natural language processing, and multimodal tasks, where KD has driven performance improvements and enhanced model compression. This review aims to provide researchers and practitioners with a comprehensive understanding of the state-of-the-art in knowledge distillation, bridging foundational concepts with the latest methodologies and practical implications.},
  keywords = {Adversarial distillation,Deep learning,Knowledge distillation,Model compression,Self-distillation},
  file = {/Users/giulialanzillotta/Zotero/storage/7UD7UPMV/S2666827024000811.html}
}

@article{mundtWholisticViewContinual2023,
  title = {A Wholistic View of Continual Learning with Deep Neural Networks: {{Forgotten}} Lessons and the Bridge to Active and Open World Learning},
  author = {Mundt, Martin and Hong, Yongwon and Pliushch, Iuliia and Ramesh, Visvanathan},
  year = 2023,
  journal = {Neural Networks},
  volume = {160},
  pages = {306--336},
  publisher = {Elsevier}
}

@inproceedings{murataWhatHappeningContinual2020,
  title = {What Is Happening inside a Continual Learning Model? A Representation-Based Evaluation of Representational Forgetting},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshops}}},
  author = {Murata, Kengo and Toyota, Tetsuya and Ohara, Kouzou},
  year = 2020,
  pages = {234--235}
}

@misc{nacsonConvergenceGradientDescent2019,
  title = {Convergence of {{Gradient Descent}} on {{Separable Data}}},
  author = {Nacson, Mor Shpigel and Lee, Jason D. and Gunasekar, Suriya and Savarese, Pedro H. P. and Srebro, Nathan and Soudry, Daniel},
  year = 2019,
  month = mar,
  number = {arXiv:1803.01905},
  eprint = {1803.01905},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1803.01905},
  urldate = {2025-09-15},
  abstract = {We provide a detailed study on the implicit bias of gradient descent when optimizing loss functions with strictly monotone tails, such as the logistic loss, over separable datasets. We look at two basic questions: (a) what are the conditions on the tail of the loss function under which gradient descent converges in the direction of the \$L\_2\$ maximum-margin separator? (b) how does the rate of margin convergence depend on the tail of the loss function and the choice of the step size? We show that for a large family of super-polynomial tailed losses, gradient descent iterates on linear networks of any depth converge in the direction of \$L\_2\$ maximum-margin solution, while this does not hold for losses with heavier tails. Within this family, for simple linear models we show that the optimal rates with fixed step size is indeed obtained for the commonly used exponentially tailed losses such as logistic loss. However, with a fixed step size the optimal convergence rate is extremely slow as \$1/{\textbackslash}log(t)\$, as also proved in Soudry et al. (2018). For linear models with exponential loss, we further prove that the convergence rate could be improved to \${\textbackslash}log (t) /{\textbackslash}sqrt\{t\}\$ by using aggressive step sizes that compensates for the rapidly vanishing gradients. Numerical results suggest this method might be useful for deep networks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/GGKI4SYR/Nacson et al. - 2019 - Convergence of Gradient Descent on Separable Data.pdf;/Users/giulialanzillotta/Zotero/storage/MAB7NC55/1803.html}
}

@misc{nacsonConvergenceGradientDescent2019a,
  title = {Convergence of {{Gradient Descent}} on {{Separable Data}}},
  author = {Nacson, Mor Shpigel and Lee, Jason D. and Gunasekar, Suriya and Savarese, Pedro H. P. and Srebro, Nathan and Soudry, Daniel},
  year = 2019,
  month = mar,
  number = {arXiv:1803.01905},
  eprint = {1803.01905},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1803.01905},
  urldate = {2025-09-16},
  abstract = {We provide a detailed study on the implicit bias of gradient descent when optimizing loss functions with strictly monotone tails, such as the logistic loss, over separable datasets. We look at two basic questions: (a) what are the conditions on the tail of the loss function under which gradient descent converges in the direction of the \$L\_2\$ maximum-margin separator? (b) how does the rate of margin convergence depend on the tail of the loss function and the choice of the step size? We show that for a large family of super-polynomial tailed losses, gradient descent iterates on linear networks of any depth converge in the direction of \$L\_2\$ maximum-margin solution, while this does not hold for losses with heavier tails. Within this family, for simple linear models we show that the optimal rates with fixed step size is indeed obtained for the commonly used exponentially tailed losses such as logistic loss. However, with a fixed step size the optimal convergence rate is extremely slow as \$1/{\textbackslash}log(t)\$, as also proved in Soudry et al. (2018). For linear models with exponential loss, we further prove that the convergence rate could be improved to \${\textbackslash}log (t) /{\textbackslash}sqrt\{t\}\$ by using aggressive step sizes that compensates for the rapidly vanishing gradients. Numerical results suggest this method might be useful for deep networks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/BXCS893F/Nacson et al. - 2019 - Convergence of Gradient Descent on Separable Data.pdf;/Users/giulialanzillotta/Zotero/storage/3BPJ83XZ/1803.html}
}

@article{nevesAdvancesChallengesLearning2024,
  title = {Advances and Challenges in Learning from Experience Replay},
  author = {Neves, Daniel Eug{\^e}nio and Ishitani, Lucila and {do Patroc{\'i}nio J{\'u}nior}, Zenilton Kleber Gon{\c c}alves},
  year = 2024,
  month = dec,
  journal = {Artificial Intelligence Review},
  volume = {58},
  number = {2},
  pages = {54},
  issn = {1573-7462},
  doi = {10.1007/s10462-024-11062-0},
  urldate = {2025-10-07},
  abstract = {From the first theoretical propositions in the 1950s to its application in real-world problems, Reinforcement Learning (RL) is still a fascinating and complex class of machine learning algorithms with overgrowing literature in recent years. In this work, we present an extensive and structured literature review and discuss how the Experience Replay (ER) technique has been fundamental in making various RL methods in most relevant problems and different domains more data efficient. ER is the central focus of this review. One of its main contributions is a taxonomy that organizes the many research works and the different RL methods that use ER. Here, the focus is on how RL methods improve and apply ER strategies, demonstrating their specificities and contributions while having ER as a prominent component. Another relevant contribution is the organization in a facet-oriented way, allowing different perspectives of reading, whether based on the fundamental problems of RL, focusing on algorithmic strategies and architectural decisions, or with a view to different applications of RL with ER. Moreover, we start by presenting a detailed formal theoretical foundation of RL and some of the most relevant algorithms and bring from the recent literature some of the main trends, challenges, and advances focused on ER formal basement and how to improve its propositions to make it even more efficient in different methods and domains. Lastly, we discuss challenges and open problems and present relevant paths to feature works.},
  langid = {english},
  keywords = {Deep reinforcement learning,Experience replay,Reinforcement learning,Survey},
  file = {/Users/giulialanzillotta/Zotero/storage/66W8TN2P/Neves et al. - 2024 - Advances and challenges in learning from experience replay.pdf}
}

@article{nevesAdvancesChallengesLearning2024a,
  title = {Advances and Challenges in Learning from Experience Replay},
  author = {Neves, Daniel Eug{\^e}nio and Ishitani, Lucila and {do Patroc{\'i}nio J{\'u}nior}, Zenilton Kleber Gon{\c c}alves},
  year = 2024,
  month = dec,
  journal = {Artificial Intelligence Review},
  volume = {58},
  number = {2},
  pages = {54},
  issn = {1573-7462},
  doi = {10.1007/s10462-024-11062-0},
  urldate = {2025-10-14},
  abstract = {From the first theoretical propositions in the 1950s to its application in real-world problems, Reinforcement Learning (RL) is still a fascinating and complex class of machine learning algorithms with overgrowing literature in recent years. In this work, we present an extensive and structured literature review and discuss how the Experience Replay (ER) technique has been fundamental in making various RL methods in most relevant problems and different domains more data efficient. ER is the central focus of this review. One of its main contributions is a taxonomy that organizes the many research works and the different RL methods that use ER. Here, the focus is on how RL methods improve and apply ER strategies, demonstrating their specificities and contributions while having ER as a prominent component. Another relevant contribution is the organization in a facet-oriented way, allowing different perspectives of reading, whether based on the fundamental problems of RL, focusing on algorithmic strategies and architectural decisions, or with a view to different applications of RL with ER. Moreover, we start by presenting a detailed formal theoretical foundation of RL and some of the most relevant algorithms and bring from the recent literature some of the main trends, challenges, and advances focused on ER formal basement and how to improve its propositions to make it even more efficient in different methods and domains. Lastly, we discuss challenges and open problems and present relevant paths to feature works.},
  langid = {english},
  keywords = {Deep reinforcement learning,Experience replay,Reinforcement learning,Survey},
  file = {/Users/giulialanzillotta/Zotero/storage/785DN3B2/Neves et al. - 2024 - Advances and challenges in learning from experience replay.pdf}
}

@inproceedings{nevesWhenLessMay2022,
  title = {When {{Less May Be More}}: {{Exploring Similarity}} to~{{Improve Experience Replay}}},
  shorttitle = {When {{Less May Be More}}},
  booktitle = {Intelligent {{Systems}}},
  author = {Neves, Daniel Eug{\^e}nio and Ishitani, Lucila and {do Patroc{\'i}nio J{\'u}nior}, Zenilton Kleber Gon{\c c}alves},
  editor = {{Xavier-Junior}, Jo{\~a}o Carlos and Rios, Ricardo Ara{\'u}jo},
  year = 2022,
  pages = {96--110},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-031-21689-3_8},
  abstract = {We propose the COMPact Experience Replay (COMPER) as a reinforcement learning method that seeks to reduce the required number of experiences to agent training regarding the total accumulated rewards in the long run. COMPER uses temporal difference learning with predicted target values for sets of similar transitions and a new experience replay approach based on two memories of transitions. We present an assessment of two possible neural network architectures for the target network with a complete analysis of the memories' behavior, along with detailed results for 100,000 frames and about 25,000 iterations with a small experience memory on eight challenging 2600 Atari games on the Arcade Learning Environment (ALE). We also present results for a Deep Q-Network (DQN) agent with the same experimental protocol on the same set of games as a baseline. We demonstrate that COMPER can approximate a good policy from a small number of frame observations using a compact memory and learning the similar transitions' sets dynamics using a recurrent neural network.},
  isbn = {978-3-031-21689-3},
  langid = {english},
  keywords = {Deep reinforcement learning,Experience replay,Recurrence on target value prediction,Similar transition sets,Transitions memories},
  file = {/Users/giulialanzillotta/Zotero/storage/LQHVYHGU/Neves et al. - 2022 - When Less May Be More Exploring Similarity toÂ Improve Experience Replay.pdf}
}

@misc{nguyenVariationalContinualLearning2018,
  title = {Variational {{Continual Learning}}},
  author = {Nguyen, Cuong V. and Li, Yingzhen and Bui, Thang D. and Turner, Richard E.},
  year = 2018,
  month = may,
  number = {arXiv:1710.10628},
  eprint = {1710.10628},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1710.10628},
  urldate = {2025-06-30},
  abstract = {This paper develops variational continual learning (VCL), a simple but general framework for continual learning that fuses online variational inference (VI) and recent advances in Monte Carlo VI for neural networks. The framework can successfully train both deep discriminative models and deep generative models in complex continual learning settings where existing tasks evolve over time and entirely new tasks emerge. Experimental results show that VCL outperforms state-of-the-art continual learning methods on a variety of tasks, avoiding catastrophic forgetting in a fully automatic way.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/J839RXFH/Nguyen et al. - 2018 - Variational Continual Learning.pdf;/Users/giulialanzillotta/Zotero/storage/JHUG5R9G/1710.html}
}

@misc{nikishinPrimacyBiasDeep2022,
  title = {The {{Primacy Bias}} in {{Deep Reinforcement Learning}}},
  author = {Nikishin, Evgenii and Schwarzer, Max and D'Oro, Pierluca and Bacon, Pierre-Luc and Courville, Aaron},
  year = 2022,
  month = may,
  number = {arXiv:2205.07802},
  eprint = {2205.07802},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2205.07802},
  urldate = {2025-05-02},
  abstract = {This work identifies a common flaw of deep reinforcement learning (RL) algorithms: a tendency to rely on early interactions and ignore useful evidence encountered later. Because of training on progressively growing datasets, deep RL agents incur a risk of overfitting to earlier experiences, negatively affecting the rest of the learning process. Inspired by cognitive science, we refer to this effect as the primacy bias. Through a series of experiments, we dissect the algorithmic aspects of deep RL that exacerbate this bias. We then propose a simple yet generally-applicable mechanism that tackles the primacy bias by periodically resetting a part of the agent. We apply this mechanism to algorithms in both discrete (Atari 100k) and continuous action (DeepMind Control Suite) domains, consistently improving their performance.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/NWS9GVJE/Nikishin et al. - 2022 - The Primacy Bias in Deep Reinforcement Learning.pdf;/Users/giulialanzillotta/Zotero/storage/23K2YQX2/2205.html}
}

@article{oehlertNoteDeltaMethod1992,
  title = {A {{Note}} on the {{Delta Method}}},
  author = {Oehlert, Gary W.},
  year = 1992,
  journal = {The American Statistician},
  volume = {46},
  number = {1},
  eprint = {2684406},
  eprinttype = {jstor},
  pages = {27--29},
  publisher = {[American Statistical Association, Taylor \& Francis, Ltd.]},
  issn = {0003-1305},
  doi = {10.2307/2684406},
  urldate = {2025-09-30},
  abstract = {The delta method is an intuitive technique for approximating the moments of functions of random variables. This note reviews the delta method and conditions under which delta-method approximate moments are accurate.},
  file = {/Users/giulialanzillotta/Zotero/storage/ERZL37KR/Oehlert - 1992 - A Note on the Delta Method.pdf}
}

@incollection{opperBayesianApproachOnline1999,
  title = {A {{Bayesian}} Approach to On-Line Learning},
  booktitle = {On-Line Learning in Neural Networks},
  author = {Opper, Manfred and Winther, Ole},
  editor = {Saad, David},
  year = 1999,
  month = jan,
  series = {Publications of the {{Newton Institute}}},
  pages = {363--378},
  publisher = {Cambridge University Press},
  address = {Cambridge},
  doi = {10.2277/0521652634},
  urldate = {2025-09-30},
  abstract = {Online learning is discussed from the viewpoint of Bayesian statistical inference. By replacing the true posterior distribution with a simpler parametric distribution, one can define an online algorithm by a repetition of two steps: An update of the approximate posterior, when a new example arrives, and an optimal projection into the parametric family. Choosing this family to be Gaussian, we show that the algorithm achieves asymptotic efficiency. An application to learning in single layer neural networks is given.},
  isbn = {978-0-262-19416-7},
  keywords = {asymptotic efficiency,Bayesian statistical inference,neural networks,Online learning}
}

@article{orabonaModernIntroductionOnline2019,
  title = {A Modern Introduction to Online Learning},
  author = {Orabona, Francesco},
  year = 2019,
  journal = {arXiv preprint arXiv:1912.13213},
  eprint = {1912.13213},
  archiveprefix = {arXiv}
}

@misc{orabonaModernIntroductionOnline2025,
  title = {A {{Modern Introduction}} to {{Online Learning}}},
  author = {Orabona, Francesco},
  year = 2025,
  month = may,
  number = {arXiv:1912.13213},
  eprint = {1912.13213},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1912.13213},
  urldate = {2025-05-03},
  abstract = {In this monograph, I introduce the basic concepts of Online Learning through a modern view of Online Convex Optimization. Here, online learning refers to the framework of regret minimization under worst-case assumptions. I present first-order and second-order algorithms for online learning with convex losses, in Euclidean and non-Euclidean settings. All the algorithms are clearly presented as instantiation of Online Mirror Descent or Follow-The-Regularized-Leader and their variants. Particular attention is given to the issue of tuning the parameters of the algorithms and learning in unbounded domains, through adaptive and parameter-free online learning algorithms. Non-convex losses are dealt through convex surrogate losses and through randomization. The bandit setting is also briefly discussed, touching on the problem of adversarial and stochastic multi-armed bandits. These notes do not require prior knowledge of convex analysis and all the required mathematical tools are rigorously explained. Moreover, all the included proofs have been carefully chosen to be as simple and as short as possible.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/6ZDKWRKF/Orabona - 2025 - A Modern Introduction to Online Learning.pdf;/Users/giulialanzillotta/Zotero/storage/LCAGUEY5/1912.html}
}

@misc{PacknetGoogleSearch,
  title = {Packnet - {{Google Search}}},
  urldate = {2025-10-01},
  howpublished = {https://www.google.com/search?client=safari\&rls=en\&q=Packnet\&ie=UTF-8\&oe=UTF-8},
  file = {/Users/giulialanzillotta/Zotero/storage/ZQZISEYM/search.html}
}

@misc{panContinualDeepLearning2021,
  title = {Continual {{Deep Learning}} by {{Functional Regularisation}} of {{Memorable Past}}},
  author = {Pan, Pingbo and Swaroop, Siddharth and Immer, Alexander and Eschenhagen, Runa and Turner, Richard E. and Khan, Mohammad Emtiyaz},
  year = 2021,
  month = jan,
  number = {arXiv:2004.14070},
  eprint = {2004.14070},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2004.14070},
  urldate = {2025-07-01},
  abstract = {Continually learning new skills is important for intelligent systems, yet standard deep learning methods suffer from catastrophic forgetting of the past. Recent works address this with weight regularisation. Functional regularisation, although computationally expensive, is expected to perform better, but rarely does so in practice. In this paper, we fix this issue by using a new functional-regularisation approach that utilises a few memorable past examples crucial to avoid forgetting. By using a Gaussian Process formulation of deep networks, our approach enables training in weight-space while identifying both the memorable past and a functional prior. Our method achieves state-of-the-art performance on standard benchmarks and opens a new direction for life-long learning where regularisation and memory-based methods are naturally combined.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/R6KJFSMV/Pan et al. - 2021 - Continual Deep Learning by Functional Regularisation of Memorable Past.pdf;/Users/giulialanzillotta/Zotero/storage/GXHABERX/2004.html}
}

@misc{panContinualDeepLearning2021a,
  title = {Continual {{Deep Learning}} by {{Functional Regularisation}} of {{Memorable Past}}},
  author = {Pan, Pingbo and Swaroop, Siddharth and Immer, Alexander and Eschenhagen, Runa and Turner, Richard E. and Khan, Mohammad Emtiyaz},
  year = 2021,
  month = jan,
  number = {arXiv:2004.14070},
  eprint = {2004.14070},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2004.14070},
  urldate = {2025-09-30},
  abstract = {Continually learning new skills is important for intelligent systems, yet standard deep learning methods suffer from catastrophic forgetting of the past. Recent works address this with weight regularisation. Functional regularisation, although computationally expensive, is expected to perform better, but rarely does so in practice. In this paper, we fix this issue by using a new functional-regularisation approach that utilises a few memorable past examples crucial to avoid forgetting. By using a Gaussian Process formulation of deep networks, our approach enables training in weight-space while identifying both the memorable past and a functional prior. Our method achieves state-of-the-art performance on standard benchmarks and opens a new direction for life-long learning where regularisation and memory-based methods are naturally combined.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/3XBBFVDH/Pan et al. - 2021 - Continual Deep Learning by Functional Regularisation of Memorable Past.pdf;/Users/giulialanzillotta/Zotero/storage/6HK8A6ZJ/2004.html}
}

@article{papyanPrevalenceNeuralCollapse2020,
  title = {Prevalence of Neural Collapse during the Terminal Phase of Deep Learning Training},
  author = {Papyan, Vardan and Han, {\relax XY} and Donoho, David L},
  year = 2020,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {117},
  number = {40},
  pages = {24652--24663},
  publisher = {National Acad Sciences}
}

@article{papyanPrevalenceNeuralCollapse2020a,
  title = {Prevalence of {{Neural Collapse}} during the Terminal Phase of Deep Learning Training},
  author = {Papyan, Vardan and Han, X. Y. and Donoho, David L.},
  year = 2020,
  month = oct,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {117},
  number = {40},
  eprint = {2008.08186},
  primaryclass = {cs},
  pages = {24652--24663},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.2015509117},
  urldate = {2025-09-16},
  abstract = {Modern practice for training classification deepnets involves a Terminal Phase of Training (TPT), which begins at the epoch where training error first vanishes; During TPT, the training error stays effectively zero while training loss is pushed towards zero. Direct measurements of TPT, for three prototypical deepnet architectures and across seven canonical classification datasets, expose a pervasive inductive bias we call Neural Collapse, involving four deeply interconnected phenomena: (NC1) Cross-example within-class variability of last-layer training activations collapses to zero, as the individual activations themselves collapse to their class-means; (NC2) The class-means collapse to the vertices of a Simplex Equiangular Tight Frame (ETF); (NC3) Up to rescaling, the last-layer classifiers collapse to the class-means, or in other words to the Simplex ETF, i.e. to a self-dual configuration; (NC4) For a given activation, the classifier's decision collapses to simply choosing whichever class has the closest train class-mean, i.e. the Nearest Class Center (NCC) decision rule. The symmetric and very simple geometry induced by the TPT confers important benefits, including better generalization performance, better robustness, and better interpretability.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/YDKLMV4M/Papyan et al. - 2020 - Prevalence of Neural Collapse during the terminal phase of deep learning training.pdf;/Users/giulialanzillotta/Zotero/storage/ZI8UVQND/2008.html}
}

@article{parisiContinualLifelongLearning2019,
  title = {Continual Lifelong Learning with Neural Networks: {{A}} Review},
  author = {Parisi, German I and Kemker, Ronald and Part, Jose L and Kanan, Christopher and Wermter, Stefan},
  year = 2019,
  journal = {Neural networks},
  volume = {113},
  pages = {54--71},
  publisher = {Elsevier}
}

@article{parisiContinualLifelongLearning2019a,
  title = {Continual Lifelong Learning with Neural Networks: {{A}} Review},
  shorttitle = {Continual Lifelong Learning with Neural Networks},
  author = {Parisi, German I. and Kemker, Ronald and Part, Jose L. and Kanan, Christopher and Wermter, Stefan},
  year = 2019,
  month = may,
  journal = {Neural Networks},
  volume = {113},
  pages = {54--71},
  issn = {08936080},
  doi = {10.1016/j.neunet.2019.01.012},
  urldate = {2025-09-30},
  abstract = {Humans and animals have the ability to continually acquire, fine-tune, and transfer knowledge and skills throughout their lifespan. This ability, referred to as lifelong learning, is mediated by a rich set of neurocognitive mechanisms that together contribute to the development and specialization of our sensorimotor skills as well as to long-term memory consolidation and retrieval. Consequently, lifelong learning capabilities are crucial for computational learning systems and autonomous agents interacting in the real world and processing continuous streams of information. However, lifelong learning remains a long-standing challenge for machine learning and neural network models since the continual acquisition of incrementally available information from non-stationary data distributions generally leads to catastrophic forgetting or interference. This limitation represents a major drawback for state-of-theart deep neural network models that typically learn representations from stationary batches of training data, thus without accounting for situations in which information becomes incrementally available over time. In this review, we critically summarize the main challenges linked to lifelong learning for artificial learning systems and compare existing neural network approaches that alleviate, to different extents, catastrophic forgetting. Although significant advances have been made in domain-specific learning with neural networks, extensive research efforts are required for the development of robust lifelong learning on autonomous agents and robots. We discuss well-established and emerging research motivated by lifelong learning factors in biological systems such as structural plasticity, memory replay, curriculum and transfer learning, intrinsic motivation, and multisensory integration.},
  langid = {english},
  file = {/Users/giulialanzillotta/Zotero/storage/X73IIPS8/Parisi et al. - 2019 - Continual lifelong learning with neural networks A review.pdf}
}

@inproceedings{parkhiCatsDogs,
  title = {Cats and {{Dogs}}},
  booktitle = {{{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Parkhi, Omkar M. and Vedaldi, Andrea and Zisserman, Andrew and Jawahar, C. V.},
  pages = {startpage--endpage},
  publisher = {IEEE}
}

@misc{pascanuOptimizersQualitativelyAlter2025,
  title = {Optimizers {{Qualitatively Alter Solutions And We Should Leverage This}}},
  author = {Pascanu, Razvan and Lyle, Clare and Modoranu, Ionut-Vlad and Borras, Naima Elosegui and Alistarh, Dan and Velickovic, Petar and Chandar, Sarath and De, Soham and Martens, James},
  year = 2025,
  month = jul,
  number = {arXiv:2507.12224},
  eprint = {2507.12224},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2507.12224},
  urldate = {2025-10-03},
  abstract = {Due to the nonlinear nature of Deep Neural Networks (DNNs), one can not guarantee convergence to a unique global minimum of the loss when using optimizers relying only on local information, such as SGD. Indeed, this was a primary source of skepticism regarding the feasibility of DNNs in the early days of the field. The past decades of progress in deep learning have revealed this skepticism to be misplaced, and a large body of empirical evidence shows that sufficiently large DNNs following standard training protocols exhibit well-behaved optimization dynamics that converge to performant solutions. This success has biased the community to use convex optimization as a mental model for learning, leading to a focus on training efficiency, either in terms of required iteration, FLOPs or wall-clock time, when improving optimizers. We argue that, while this perspective has proven extremely fruitful, another perspective specific to DNNs has received considerably less attention: the optimizer not only influences the rate of convergence, but also the qualitative properties of the learned solutions. Restated, the optimizer can and will encode inductive biases and change the effective expressivity of a given class of models. Furthermore, we believe the optimizer can be an effective way of encoding desiderata in the learning process. We contend that the community should aim at understanding the biases of already existing methods, as well as aim to build new optimizers with the explicit intent of inducing certain properties of the solution, rather than solely judging them based on their convergence rates. We hope our arguments will inspire research to improve our understanding of how the learning process can impact the type of solution we converge to, and lead to a greater recognition of optimizers design as a critical lever that complements the roles of architecture and data in shaping model outcomes.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/K5XR6MJP/Pascanu et al. - 2025 - Optimizers Qualitatively Alter Solutions And We Should Leverage This.pdf;/Users/giulialanzillotta/Zotero/storage/VA8KUGHA/2507.html}
}

@misc{PDFBayesianApproach,
  title = {({{PDF}}) {{A Bayesian}} Approach to on-Line Learning},
  journal = {ResearchGate},
  urldate = {2025-09-30},
  abstract = {PDF {\textbar} Online learning is discussed from the viewpoint of Bayesian statistical inference. By replacing the true posterior distribution with a simpler... {\textbar} Find, read and cite all the research you need on ResearchGate},
  howpublished = {https://www.researchgate.net/publication/40498341\_A\_Bayesian\_approach\_to\_on-line\_learning},
  langid = {english},
  file = {/Users/giulialanzillotta/Zotero/storage/2SZ7QTAK/40498341_A_Bayesian_approach_to_on-line_learning.html}
}

@article{pentinaLifelongLearningNoniid2015,
  title = {Lifelong Learning with Non-Iid Tasks},
  author = {Pentina, Anastasia and Lampert, Christoph H},
  year = 2015,
  journal = {Advances in Neural Information Processing Systems},
  volume = {28}
}

@article{pescador-barriosAdjustingModelSize,
  title = {Adjusting {{Model Size}} in {{Continual Gaussian Processes}}:  {{How Big}} Is {{Big Enough}}?},
  author = {{Pescador-Barrios}, Guiomar and Filippi, Sarah},
  abstract = {Many machine learning models require setting a parameter that controls their size before training, e.g. number of neurons in DNNs, or inducing points in GPs. Increasing capacity typically improves performance until all the information from the dataset is captured. After this point, computational cost keeps increasing, without improved performance. This leads to the question ``How big is big enough?'' We investigate this problem for Gaussian processes (single-layer neural networks) in continual learning. Here, data becomes available incrementally, and the final dataset size will therefore not be known before training, preventing the use of heuristics for setting a fixed model size. We develop a method to automatically adjust model size while maintaining near-optimal performance. Our experimental procedure follows the constraint that any hyperparameters must be set without seeing dataset properties, and we show that our method performs well across diverse datasets without the need to adjust its hyperparameter, showing it requires less tuning than others.},
  langid = {english},
  file = {/Users/giulialanzillotta/Zotero/storage/RW3L3WMI/Pescador-Barrios and Filippi - Adjusting Model Size in Continual Gaussian Processes  How Big is Big Enough.pdf}
}

@inproceedings{pescador-barriosAdjustingModelSize2025,
  title = {Adjusting {{Model Size}} in {{Continual Gaussian Processes}}: {{How Big}} Is {{Big Enough}}?},
  shorttitle = {Adjusting {{Model Size}} in {{Continual Gaussian Processes}}},
  booktitle = {Forty-Second {{International Conference}} on {{Machine Learning}}},
  author = {{Pescador-Barrios}, Guiomar and Filippi, Sarah Lucie and van der Wilk, Mark},
  year = 2025,
  month = jun,
  urldate = {2025-10-14},
  abstract = {Many machine learning models require setting a parameter that controls their size before training, e.g. number of neurons in DNNs, or inducing points in GPs. Increasing capacity typically improves performance until all the information from the dataset is captured. After this point, computational cost keeps increasing, without improved performance. This leads to the question "How big is big enough?" We investigate this problem for Gaussian processes (single-layer neural networks) in continual learning. Here, data becomes available incrementally, and the final dataset size will therefore not be known before training, preventing the use of heuristics for setting a fixed model size. We develop a method to automatically adjust model size while maintaining near-optimal performance. Our experimental procedure follows the constraint that any hyperparameters must be set without seeing dataset properties, and we show that our method performs well across diverse datasets without the need to adjust its hyperparameter, showing it requires less tuning than others.},
  langid = {english},
  file = {/Users/giulialanzillotta/Zotero/storage/H8VHZKK3/Pescador-Barrios et al. - 2025 - Adjusting Model Size in Continual Gaussian Processes How Big is Big Enough.pdf}
}

@misc{pfeifferAdapterFusionNonDestructiveTask2021,
  title = {{{AdapterFusion}}: {{Non-Destructive Task Composition}} for {{Transfer Learning}}},
  shorttitle = {{{AdapterFusion}}},
  author = {Pfeiffer, Jonas and Kamath, Aishwarya and R{\"u}ckl{\'e}, Andreas and Cho, Kyunghyun and Gurevych, Iryna},
  year = 2021,
  month = jan,
  number = {arXiv:2005.00247},
  eprint = {2005.00247},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2005.00247},
  urldate = {2025-09-29},
  abstract = {Sequential fine-tuning and multi-task learning are methods aiming to incorporate knowledge from multiple tasks; however, they suffer from catastrophic forgetting and difficulties in dataset balancing. To address these shortcomings, we propose AdapterFusion, a new two stage learning algorithm that leverages knowledge from multiple tasks. First, in the knowledge extraction stage we learn task specific parameters called adapters, that encapsulate the task-specific information. We then combine the adapters in a separate knowledge composition step. We show that by separating the two stages, i.e., knowledge extraction and knowledge composition, the classifier can effectively exploit the representations learned from multiple tasks in a non-destructive manner. We empirically evaluate AdapterFusion on 16 diverse NLU tasks, and find that it effectively combines various types of knowledge at different layers of the model. We show that our approach outperforms traditional strategies such as full fine-tuning as well as multi-task learning. Our code and adapters are available at AdapterHub.ml.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/giulialanzillotta/Zotero/storage/92CY3HZ9/Pfeiffer et al. - 2021 - AdapterFusion Non-Destructive Task Composition for Transfer Learning.pdf;/Users/giulialanzillotta/Zotero/storage/B39XISQJ/2005.html}
}

@misc{pfeifferAdapterFusionNonDestructiveTask2021a,
  title = {{{AdapterFusion}}: {{Non-Destructive Task Composition}} for {{Transfer Learning}}},
  shorttitle = {{{AdapterFusion}}},
  author = {Pfeiffer, Jonas and Kamath, Aishwarya and R{\"u}ckl{\'e}, Andreas and Cho, Kyunghyun and Gurevych, Iryna},
  year = 2021,
  month = jan,
  number = {arXiv:2005.00247},
  eprint = {2005.00247},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2005.00247},
  urldate = {2025-10-01},
  abstract = {Sequential fine-tuning and multi-task learning are methods aiming to incorporate knowledge from multiple tasks; however, they suffer from catastrophic forgetting and difficulties in dataset balancing. To address these shortcomings, we propose AdapterFusion, a new two stage learning algorithm that leverages knowledge from multiple tasks. First, in the knowledge extraction stage we learn task specific parameters called adapters, that encapsulate the task-specific information. We then combine the adapters in a separate knowledge composition step. We show that by separating the two stages, i.e., knowledge extraction and knowledge composition, the classifier can effectively exploit the representations learned from multiple tasks in a non-destructive manner. We empirically evaluate AdapterFusion on 16 diverse NLU tasks, and find that it effectively combines various types of knowledge at different layers of the model. We show that our approach outperforms traditional strategies such as full fine-tuning as well as multi-task learning. Our code and adapters are available at AdapterHub.ml.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/giulialanzillotta/Zotero/storage/BXRKTNQB/Pfeiffer et al. - 2021 - AdapterFusion Non-Destructive Task Composition for Transfer Learning.pdf;/Users/giulialanzillotta/Zotero/storage/9LP3M5KM/2005.html}
}

@misc{phamContinualNormalizationRethinking2022,
  title = {Continual {{Normalization}}: {{Rethinking Batch Normalization}} for {{Online Continual Learning}}},
  author = {Pham, Quang and Liu, Chenghao and Hoi, Steven},
  year = 2022
}

@article{poggioExplicitRegularizationImplicit2020,
  title = {Explicit Regularization and Implicit Bias in Deep Network Classifiers Trained with the Square Loss},
  author = {Poggio, Tomaso and Liao, Qianli},
  year = 2020,
  journal = {arXiv preprint arXiv:2101.00072},
  eprint = {2101.00072},
  archiveprefix = {arXiv}
}

@inproceedings{poOrthogonalAdaptationModular2024,
  title = {Orthogonal {{Adaptation}} for {{Modular Customization}} of {{Diffusion Models}}},
  booktitle = {2024 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Po, Ryan and Yang, Guandao and Aberman, Kfir and Wetzstein, Gordon},
  year = 2024,
  month = jun,
  pages = {7964--7973},
  publisher = {IEEE},
  address = {Seattle, WA, USA},
  doi = {10.1109/CVPR52733.2024.00761},
  urldate = {2025-10-01},
  abstract = {Customization techniques for text-to-image models have paved the way for a wide range of previously unattainable applications, enabling the generation of specific concepts across diverse contexts and styles. While existing methods facilitate high-fidelity customization for individual concepts or a limited, pre-defined set of them, they fall short of achieving scalability, where a single model can seamlessly render countless concepts. In this paper, we address a new problem called Modular Customization, with the goal of efficiently merging customized models that were fine-tuned independently for individual concepts. This allows the merged model to jointly synthesize concepts in one image without compromising fidelity or incurring any additional computational costs. To address this problem, we introduce Orthogonal Adaptation, a method designed to encourage the customized models, which do not have access to each other during fine-tuning, to have orthogonal residual weights. This ensures that during inference time, the customized models can be summed with minimal interference. Our proposed method is both simple and versatile, applicable to nearly all optimizable weights in the model architecture. Through an extensive set of quantitative and qualitative evaluations, our method consistently outperforms relevant baselines in terms of efficiency and identity preservation, demonstrating a significant leap toward scalable customization of diffusion models.},
  copyright = {https://doi.org/10.15223/policy-029},
  isbn = {979-8-3503-5300-6},
  langid = {english},
  file = {/Users/giulialanzillotta/Zotero/storage/LBTU3XL6/Po et al. - 2024 - Orthogonal Adaptation for Modular Customization of Diffusion Models.pdf}
}

@inproceedings{powersCoraBenchmarksBaselines2022,
  title = {Cora: {{Benchmarks}}, Baselines, and Metrics as a Platform for Continual Reinforcement Learning Agents},
  booktitle = {Conference on {{Lifelong Learning Agents}}},
  author = {Powers, Sam and Xing, Eliot and Kolve, Eric and Mottaghi, Roozbeh and Gupta, Abhinav},
  year = 2022,
  pages = {705--743},
  publisher = {PMLR}
}

@misc{prabhuComputationallyBudgetedContinual2023,
  title = {Computationally {{Budgeted Continual Learning}}: {{What Does Matter}}?},
  shorttitle = {Computationally {{Budgeted Continual Learning}}},
  author = {Prabhu, Ameya and Hammoud, Hasan Abed Al Kader and Dokania, Puneet and Torr, Philip H. S. and Lim, Ser-Nam and Ghanem, Bernard and Bibi, Adel},
  year = 2023,
  month = jul,
  number = {arXiv:2303.11165},
  eprint = {2303.11165},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2303.11165},
  urldate = {2025-10-07},
  abstract = {Continual Learning (CL) aims to sequentially train models on streams of incoming data that vary in distribution by preserving previous knowledge while adapting to new data. Current CL literature focuses on restricted access to previously seen data, while imposing no constraints on the computational budget for training. This is unreasonable for applications in-the-wild, where systems are primarily constrained by computational and time budgets, not storage. We revisit this problem with a large-scale benchmark and analyze the performance of traditional CL approaches in a compute-constrained setting, where effective memory samples used in training can be implicitly restricted as a consequence of limited computation. We conduct experiments evaluating various CL sampling strategies, distillation losses, and partial fine-tuning on two large-scale datasets, namely ImageNet2K and Continual Google Landmarks V2 in data incremental, class incremental, and time incremental settings. Through extensive experiments amounting to a total of over 1500 GPU-hours, we find that, under compute-constrained setting, traditional CL approaches, with no exception, fail to outperform a simple minimal baseline that samples uniformly from memory. Our conclusions are consistent in a different number of stream time steps, e.g., 20 to 200, and under several computational budgets. This suggests that most existing CL methods are particularly too computationally expensive for realistic budgeted deployment. Code for this project is available at: https://github.com/drimpossible/BudgetCL.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/2DAGM94J/Prabhu et al. - 2023 - Computationally Budgeted Continual Learning What Does Matter.pdf;/Users/giulialanzillotta/Zotero/storage/R9UHQU8D/2303.html}
}

@misc{prabhuComputationallyBudgetedContinual2023a,
  title = {Computationally {{Budgeted Continual Learning}}: {{What Does Matter}}?},
  shorttitle = {Computationally {{Budgeted Continual Learning}}},
  author = {Prabhu, Ameya and Hammoud, Hasan Abed Al Kader and Dokania, Puneet and Torr, Philip H. S. and Lim, Ser-Nam and Ghanem, Bernard and Bibi, Adel},
  year = 2023,
  month = jul,
  number = {arXiv:2303.11165},
  eprint = {2303.11165},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2303.11165},
  urldate = {2025-10-07},
  abstract = {Continual Learning (CL) aims to sequentially train models on streams of incoming data that vary in distribution by preserving previous knowledge while adapting to new data. Current CL literature focuses on restricted access to previously seen data, while imposing no constraints on the computational budget for training. This is unreasonable for applications in-the-wild, where systems are primarily constrained by computational and time budgets, not storage. We revisit this problem with a large-scale benchmark and analyze the performance of traditional CL approaches in a compute-constrained setting, where effective memory samples used in training can be implicitly restricted as a consequence of limited computation. We conduct experiments evaluating various CL sampling strategies, distillation losses, and partial fine-tuning on two large-scale datasets, namely ImageNet2K and Continual Google Landmarks V2 in data incremental, class incremental, and time incremental settings. Through extensive experiments amounting to a total of over 1500 GPU-hours, we find that, under compute-constrained setting, traditional CL approaches, with no exception, fail to outperform a simple minimal baseline that samples uniformly from memory. Our conclusions are consistent in a different number of stream time steps, e.g., 20 to 200, and under several computational budgets. This suggests that most existing CL methods are particularly too computationally expensive for realistic budgeted deployment. Code for this project is available at: https://github.com/drimpossible/BudgetCL.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/5MQ7F4R5/Prabhu et al. - 2023 - Computationally Budgeted Continual Learning What Does Matter.pdf;/Users/giulialanzillotta/Zotero/storage/XLJTWDGH/2303.html}
}

@misc{prabhuComputationallyBudgetedContinual2023b,
  title = {Computationally {{Budgeted Continual Learning}}: {{What Does Matter}}?},
  shorttitle = {Computationally {{Budgeted Continual Learning}}},
  author = {Prabhu, Ameya and Hammoud, Hasan Abed Al Kader and Dokania, Puneet and Torr, Philip H. S. and Lim, Ser-Nam and Ghanem, Bernard and Bibi, Adel},
  year = 2023,
  month = jul,
  number = {arXiv:2303.11165},
  eprint = {2303.11165},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2303.11165},
  urldate = {2025-10-07},
  abstract = {Continual Learning (CL) aims to sequentially train models on streams of incoming data that vary in distribution by preserving previous knowledge while adapting to new data. Current CL literature focuses on restricted access to previously seen data, while imposing no constraints on the computational budget for training. This is unreasonable for applications in-the-wild, where systems are primarily constrained by computational and time budgets, not storage. We revisit this problem with a large-scale benchmark and analyze the performance of traditional CL approaches in a compute-constrained setting, where effective memory samples used in training can be implicitly restricted as a consequence of limited computation. We conduct experiments evaluating various CL sampling strategies, distillation losses, and partial fine-tuning on two large-scale datasets, namely ImageNet2K and Continual Google Landmarks V2 in data incremental, class incremental, and time incremental settings. Through extensive experiments amounting to a total of over 1500 GPU-hours, we find that, under compute-constrained setting, traditional CL approaches, with no exception, fail to outperform a simple minimal baseline that samples uniformly from memory. Our conclusions are consistent in a different number of stream time steps, e.g., 20 to 200, and under several computational budgets. This suggests that most existing CL methods are particularly too computationally expensive for realistic budgeted deployment. Code for this project is available at: https://github.com/drimpossible/BudgetCL.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/LCSUY6IJ/Prabhu et al. - 2023 - Computationally Budgeted Continual Learning What Does Matter.pdf;/Users/giulialanzillotta/Zotero/storage/FB5BU9MY/2303.html}
}

@misc{prabhuComputationallyBudgetedContinual2023c,
  title = {Computationally {{Budgeted Continual Learning}}: {{What Does Matter}}?},
  shorttitle = {Computationally {{Budgeted Continual Learning}}},
  author = {Prabhu, Ameya and Hammoud, Hasan Abed Al Kader and Dokania, Puneet and Torr, Philip H. S. and Lim, Ser-Nam and Ghanem, Bernard and Bibi, Adel},
  year = 2023,
  month = jul,
  number = {arXiv:2303.11165},
  eprint = {2303.11165},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2303.11165},
  urldate = {2025-10-08},
  abstract = {Continual Learning (CL) aims to sequentially train models on streams of incoming data that vary in distribution by preserving previous knowledge while adapting to new data. Current CL literature focuses on restricted access to previously seen data, while imposing no constraints on the computational budget for training. This is unreasonable for applications in-the-wild, where systems are primarily constrained by computational and time budgets, not storage. We revisit this problem with a large-scale benchmark and analyze the performance of traditional CL approaches in a compute-constrained setting, where effective memory samples used in training can be implicitly restricted as a consequence of limited computation. We conduct experiments evaluating various CL sampling strategies, distillation losses, and partial fine-tuning on two large-scale datasets, namely ImageNet2K and Continual Google Landmarks V2 in data incremental, class incremental, and time incremental settings. Through extensive experiments amounting to a total of over 1500 GPU-hours, we find that, under compute-constrained setting, traditional CL approaches, with no exception, fail to outperform a simple minimal baseline that samples uniformly from memory. Our conclusions are consistent in a different number of stream time steps, e.g., 20 to 200, and under several computational budgets. This suggests that most existing CL methods are particularly too computationally expensive for realistic budgeted deployment. Code for this project is available at: https://github.com/drimpossible/BudgetCL.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/WS93IGGJ/Prabhu et al. - 2023 - Computationally Budgeted Continual Learning What Does Matter.pdf;/Users/giulialanzillotta/Zotero/storage/JAG6KSXC/2303.html}
}

@inproceedings{prabhuGdumbSimpleApproach2020,
  title = {Gdumb: {{A}} Simple Approach That Questions Our Progress in Continual Learning},
  booktitle = {Computer {{Vision}}--{{ECCV}} 2020: 16th {{European Conference}}, {{Glasgow}}, {{UK}}, {{August}} 23--28, 2020, {{Proceedings}}, {{Part II}} 16},
  author = {Prabhu, Ameya and Torr, Philip HS and Dokania, Puneet K},
  year = 2020,
  pages = {524--540},
  publisher = {Springer}
}

@article{prabhuRandomRepresentationsOutperform2024,
  title = {Random {{Representations Outperform Online Continually Learned Representations}}},
  author = {Prabhu, Ameya and Sinha, Shiven and Kumaraguru, Ponnurangam and Torr, Philip HS and Sener, Ozan and Dokania, Puneet K},
  year = 2024,
  journal = {arXiv preprint arXiv:2402.08823},
  eprint = {2402.08823},
  archiveprefix = {arXiv}
}

@misc{purushwalkamChallengesContinuousSelfSupervised2022,
  title = {The {{Challenges}} of {{Continuous Self-Supervised Learning}}},
  author = {Purushwalkam, Senthil and Morgado, Pedro and Gupta, Abhinav},
  year = 2022,
  month = mar,
  number = {arXiv:2203.12710},
  eprint = {2203.12710},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2203.12710},
  urldate = {2025-10-03},
  abstract = {Self-supervised learning (SSL) aims to eliminate one of the major bottlenecks in representation learning - the need for human annotations. As a result, SSL holds the promise to learn representations from data in-the-wild, i.e., without the need for finite and static datasets. Instead, true SSL algorithms should be able to exploit the continuous stream of data being generated on the internet or by agents exploring their environments. But do traditional self-supervised learning approaches work in this setup? In this work, we investigate this question by conducting experiments on the continuous self-supervised learning problem. While learning in the wild, we expect to see a continuous (infinite) non-IID data stream that follows a non-stationary distribution of visual concepts. The goal is to learn a representation that can be robust, adaptive yet not forgetful of concepts seen in the past. We show that a direct application of current methods to such continuous setup is 1) inefficient both computationally and in the amount of data required, 2) leads to inferior representations due to temporal correlations (non-IID data) in some sources of streaming data and 3) exhibits signs of catastrophic forgetting when trained on sources with non-stationary data distributions. We propose the use of replay buffers as an approach to alleviate the issues of inefficiency and temporal correlations. We further propose a novel method to enhance the replay buffer by maintaining the least redundant samples. Minimum redundancy (MinRed) buffers allow us to learn effective representations even in the most challenging streaming scenarios composed of sequential visual data obtained from a single embodied agent, and alleviates the problem of catastrophic forgetting when learning from data with non-stationary semantic distributions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/Z3AVT8ZK/Purushwalkam et al. - 2022 - The Challenges of Continuous Self-Supervised Learning.pdf;/Users/giulialanzillotta/Zotero/storage/INIHUTY2/2203.html}
}

@misc{radfordLearningTransferableVisual2021,
  title = {Learning {{Transferable Visual Models From Natural Language Supervision}}},
  author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
  year = 2021,
  month = feb,
  number = {arXiv:2103.00020},
  eprint = {2103.00020},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2103.00020},
  urldate = {2025-10-13},
  abstract = {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/C5G8DG2N/Radford et al. - 2021 - Learning Transferable Visual Models From Natural Language Supervision.pdf}
}

@inproceedings{rajasegaranRandomPathSelection2019,
  title = {Random {{Path Selection}} for {{Continual Learning}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Rajasegaran, Jathushan and Hayat, Munawar and Khan, Salman H and Khan, Fahad Shahbaz and Shao, Ling},
  year = 2019,
  volume = {32},
  publisher = {Curran Associates, Inc.},
  urldate = {2025-10-01},
  abstract = {Incremental life-long learning is a main challenge towards the long-standing goal of Artificial General Intelligence. In real-life settings, learning tasks arrive in a sequence and machine learning models must continually learn to increment already acquired knowledge. The existing incremental learning approaches fall well below the state-of-the-art cumulative models that use all training classes at once. In this paper, we propose a random path selection algorithm, called RPS-Net, that progressively chooses optimal paths for the new tasks while encouraging parameter sharing and reuse. Our approach avoids the overhead introduced by computationally expensive evolutionary and reinforcement learning based path selection strategies  while achieving considerable performance gains. As an added novelty, the proposed model integrates knowledge distillation and retrospection along with the path selection strategy to overcome catastrophic forgetting. In order to maintain an equilibrium between previous and newly acquired knowledge, we propose a simple controller to dynamically balance the model plasticity.  Through extensive experiments, we demonstrate that the proposed method surpasses the state-of-the-art performance on incremental learning and by utilizing parallel computation this method can run in constant time with nearly the same efficiency as a conventional deep convolutional neural network.},
  file = {/Users/giulialanzillotta/Zotero/storage/QTMI3EQC/Rajasegaran et al. - 2019 - Random Path Selection for Continual Learning.pdf}
}

@incollection{rakhlinMartingaleExtensionsVapnik2015,
  title = {On {{Martingale Extensions}} of {{Vapnik}}--{{Chervonenkis Theory}} with {{Applications}} to {{Online Learning}}},
  booktitle = {Measures of {{Complexity}}: {{Festschrift}} for {{Alexey Chervonenkis}}},
  author = {Rakhlin, Alexander and Sridharan, Karthik},
  year = 2015,
  pages = {197--215},
  publisher = {Springer}
}

@article{ramaseshAnatomyCatastrophicForgetting2020,
  title = {Anatomy of Catastrophic Forgetting: {{Hidden}} Representations and Task Semantics},
  author = {Ramasesh, Vinay V and Dyer, Ethan and Raghu, Maithra},
  year = 2020,
  journal = {arXiv preprint arXiv:2007.07400},
  eprint = {2007.07400},
  archiveprefix = {arXiv}
}

@misc{ramaseshAnatomyCatastrophicForgetting2020a,
  title = {Anatomy of {{Catastrophic Forgetting}}: {{Hidden Representations}} and {{Task Semantics}}},
  shorttitle = {Anatomy of {{Catastrophic Forgetting}}},
  author = {Ramasesh, Vinay V. and Dyer, Ethan and Raghu, Maithra},
  year = 2020,
  month = jul,
  number = {arXiv:2007.07400},
  eprint = {2007.07400},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2007.07400},
  urldate = {2025-10-01},
  abstract = {A central challenge in developing versatile machine learning systems is catastrophic forgetting: a model trained on tasks in sequence will suffer significant performance drops on earlier tasks. Despite the ubiquity of catastrophic forgetting, there is limited understanding of the underlying process and its causes. In this paper, we address this important knowledge gap, investigating how forgetting affects representations in neural network models. Through representational analysis techniques, we find that deeper layers are disproportionately the source of forgetting. Supporting this, a study of methods to mitigate forgetting illustrates that they act to stabilize deeper layers. These insights enable the development of an analytic argument and empirical picture relating the degree of forgetting to representational similarity between tasks. Consistent with this picture, we observe maximal forgetting occurs for task sequences with intermediate similarity. We perform empirical studies on the standard split CIFAR-10 setup and also introduce a novel CIFAR-100 based task approximating realistic input distribution shift.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@inproceedings{ramaseshContinualEvaluationLifelong2022,
  title = {Continual {{Evaluation}} for {{Lifelong Learning}}: {{Identifying}} the {{Stability Gap}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} ({{NeurIPS}})},
  author = {Ramasesh, Vinay V. and Dullerud, Natalie and Yang, Rose E. Wang and Raghunathan, Aditi and Finn, Chelsea and Talwalkar, Ameet},
  year = 2022
}

@inproceedings{ramaseshEffectScaleCatastrophic2021,
  title = {Effect of Scale on Catastrophic Forgetting in Neural Networks},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Ramasesh, Vinay Venkatesh and Lewkowycz, Aitor and Dyer, Ethan},
  year = 2021,
  month = oct,
  urldate = {2025-10-03},
  abstract = {Catastrophic forgetting presents a challenge in developing deep learning models capable of continual learning, i.e. learning tasks sequentially. Recently, both computer vision and natural-language processing have witnessed great progress through the use of large-scale pretrained models. In this work, we present an empirical study of catastrophic forgetting in this pretraining paradigm. Our experiments indicate that large, pretrained ResNets and Transformers are significantly more resistant to forgetting than randomly-initialized, trained-from-scratch models; this robustness systematically improves with scale of both model and pretraining dataset size. We take initial steps towards characterizing what aspect of model representations allows them to perform continual learning so well, finding that in the pretrained models, distinct class representations grow more orthogonal with scale. Our results suggest that, when possible, scale and a diverse pretraining dataset can be useful ingredients in mitigating catastrophic forgetting.},
  langid = {english},
  file = {/Users/giulialanzillotta/Zotero/storage/VDZJ69MF/Ramasesh et al. - 2021 - Effect of scale on catastrophic forgetting in neural networks.pdf}
}

@misc{rameshModelZooGrowing2022,
  title = {Model {{Zoo}}: {{A Growing}} "{{Brain}}" {{That Learns Continually}}},
  shorttitle = {Model {{Zoo}}},
  author = {Ramesh, Rahul and Chaudhari, Pratik},
  year = 2022,
  month = jun,
  number = {arXiv:2106.03027},
  eprint = {2106.03027},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2106.03027},
  urldate = {2025-10-01},
  abstract = {This paper argues that continual learning methods can benefit by splitting the capacity of the learner across multiple models. We use statistical learning theory and experimental analysis to show how multiple tasks can interact with each other in a non-trivial fashion when a single model is trained on them. The generalization error on a particular task can improve when it is trained with synergistic tasks, but can also deteriorate when trained with competing tasks. This theory motivates our method named Model Zoo which, inspired from the boosting literature, grows an ensemble of small models, each of which is trained during one episode of continual learning. We demonstrate that Model Zoo obtains large gains in accuracy on a variety of continual learning benchmark problems. Code is available at https://github.com/grasp-lyrl/modelzoo\_continual.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/SD7TSIZ2/Ramesh and Chaudhari - 2022 - Model Zoo A Growing Brain That Learns Continually.pdf;/Users/giulialanzillotta/Zotero/storage/RVVJ25VQ/2106.html}
}

@inproceedings{rangamaniFeatureLearningDeep2023,
  title = {Feature Learning in Deep Classifiers through {{Intermediate Neural Collapse}}},
  booktitle = {Proceedings of the 40th {{International Conference}} on {{Machine Learning}}},
  author = {Rangamani, Akshay and Lindegaard, Marius and Galanti, Tomer and Poggio, Tomaso A.},
  year = 2023,
  month = jul,
  pages = {28729--28745},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2025-09-16},
  abstract = {In this paper, we conduct an empirical study of the feature learning process in deep classifiers. Recent research has identified a training phenomenon called Neural Collapse (NC), in which the top-layer feature embeddings of samples from the same class tend to concentrate around their means, and the top layer's weights align with those features. Our study aims to investigate if these properties extend to intermediate layers. We empirically study the evolution of the covariance and mean of representations across different layers and show that as we move deeper into a trained neural network, the within-class covariance decreases relative to the between-class covariance. Additionally, we find that in the top layers, where the between-class covariance is dominant, the subspace spanned by the class means aligns with the subspace spanned by the most significant singular vector components of the weight matrix in the corresponding layer. Finally, we discuss the relationship between NC and Associative Memories (Willshaw et. al. 1969).},
  langid = {english},
  file = {/Users/giulialanzillotta/Zotero/storage/ERBUTDWK/Rangamani et al. - 2023 - Feature learning in deep classifiers through Intermediate Neural Collapse.pdf}
}

@misc{rebuffiICaRLIncrementalClassifier2017,
  title = {{{iCaRL}}: {{Incremental Classifier}} and {{Representation Learning}}},
  author = {Rebuffi, Sylvestre-Alvise and Kolesnikov, Alexander and Sperl, Georg and Lampert, Christoph H.},
  year = 2017
}

@misc{rebuffiLearningMultipleVisual2017,
  title = {Learning Multiple Visual Domains with Residual Adapters},
  author = {Rebuffi, Sylvestre-Alvise and Bilen, Hakan and Vedaldi, Andrea},
  year = 2017,
  month = nov,
  number = {arXiv:1705.08045},
  eprint = {1705.08045},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1705.08045},
  urldate = {2025-10-01},
  abstract = {There is a growing interest in learning data representations that work well for many different types of problems and data. In this paper, we look in particular at the task of learning a single visual representation that can be successfully utilized in the analysis of very different types of images, from dog breeds to stop signs and digits. Inspired by recent work on learning networks that predict the parameters of another, we develop a tunable deep network architecture that, by means of adapter residual modules, can be steered on the fly to diverse visual domains. Our method achieves a high degree of parameter sharing while maintaining or even improving the accuracy of domain-specific representations. We also introduce the Visual Decathlon Challenge, a benchmark that evaluates the ability of representations to capture simultaneously ten very different visual domains and measures their ability to recognize well uniformly.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Statistics - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/97SG6LWX/Rebuffi et al. - 2017 - Learning multiple visual domains with residual adapters.pdf;/Users/giulialanzillotta/Zotero/storage/N8HUY4RR/1705.html}
}

@article{riemerContinualLearningEnvironments2022,
  title = {Continual Learning in Environments with Polynomial Mixing Times},
  author = {Riemer, Matthew and Raparthy, Sharath Chandra and Cases, Ignacio and Subbaraj, Gopeshh and Puelma Touzel, Maximilian and Rish, Irina},
  year = 2022,
  journal = {Advances in Neural Information Processing Systems},
  volume = {35},
  pages = {21961--21973}
}

@article{riemerLearningLearnForgetting2018,
  title = {Learning to Learn without Forgetting by Maximizing Transfer and Minimizing Interference},
  author = {Riemer, Matthew and Cases, Ignacio and Ajemian, Robert and Liu, Miao and Rish, Irina and Tu, Yuhai and Tesauro, Gerald},
  year = 2018,
  journal = {arXiv preprint arXiv:1810.11910},
  eprint = {1810.11910},
  archiveprefix = {arXiv}
}

@misc{riemerLearningLearnForgetting2019,
  title = {Learning to {{Learn}} without {{Forgetting}} by {{Maximizing Transfer}} and {{Minimizing Interference}}},
  author = {Riemer, Matthew and Cases, Ignacio and Ajemian, Robert and Liu, Miao and Rish, Irina and Tu, Yuhai and Tesauro, Gerald},
  year = 2019,
  month = may,
  number = {arXiv:1810.11910},
  eprint = {1810.11910},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1810.11910},
  urldate = {2025-09-30},
  abstract = {Lack of performance when it comes to continual learning over non-stationary distributions of data remains a major challenge in scaling neural network learning to more human realistic settings. In this work we propose a new conceptualization of the continual learning problem in terms of a temporally symmetric trade-off between transfer and interference that can be optimized by enforcing gradient alignment across examples. We then propose a new algorithm, Meta-Experience Replay (MER), that directly exploits this view by combining experience replay with optimization based meta-learning. This method learns parameters that make interference based on future gradients less likely and transfer based on future gradients more likely. We conduct experiments across continual lifelong supervised learning benchmarks and non-stationary reinforcement learning environments demonstrating that our approach consistently outperforms recently proposed baselines for continual learning. Our experiments show that the gap between the performance of MER and baseline algorithms grows both as the environment gets more non-stationary and as the fraction of the total experiences stored gets smaller.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/S6IIVW6M/Riemer et al. - 2019 - Learning to Learn without Forgetting by Maximizing Transfer and Minimizing Interference.pdf;/Users/giulialanzillotta/Zotero/storage/ECNEW4T2/1810.html}
}

@inproceedings{riemerScalableRecollectionsContinual2019,
  title = {Scalable Recollections for Continual Lifelong Learning},
  booktitle = {Proceedings of the {{AAAI}} Conference on Artificial Intelligence},
  author = {Riemer, Matthew and Klinger, Tim and Bouneffouf, Djallel and Franceschini, Michele},
  year = 2019,
  volume = {33},
  pages = {1352--1359}
}

@phdthesis{ringContinualLearningReinforcement1994,
  title = {Continual Learning in Reinforcement Environments},
  author = {Ring, Mark Bishop},
  year = 1994,
  address = {USA},
  school = {University of Texas at Austin}
}

@article{ritterOnlineStructuredLaplace2018,
  title = {Online Structured Laplace Approximations for Overcoming Catastrophic Forgetting},
  author = {Ritter, Hippolyt and Botev, Aleksandar and Barber, David},
  year = 2018,
  journal = {Advances in Neural Information Processing Systems},
  volume = {31}
}

@misc{ritterOnlineStructuredLaplace2018a,
  title = {Online {{Structured Laplace Approximations For Overcoming Catastrophic Forgetting}}},
  author = {Ritter, Hippolyt and Botev, Aleksandar and Barber, David},
  year = 2018,
  month = may,
  number = {arXiv:1805.07810},
  eprint = {1805.07810},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1805.07810},
  urldate = {2025-09-30},
  abstract = {We introduce the Kronecker factored online Laplace approximation for overcoming catastrophic forgetting in neural networks. The method is grounded in a Bayesian online learning framework, where we recursively approximate the posterior after every task with a Gaussian, leading to a quadratic penalty on changes to the weights. The Laplace approximation requires calculating the Hessian around a mode, which is typically intractable for modern architectures. In order to make our method scalable, we leverage recent block-diagonal Kronecker factored approximations to the curvature. Our algorithm achieves over 90\% test accuracy across a sequence of 50 instantiations of the permuted MNIST dataset, substantially outperforming related methods for overcoming catastrophic forgetting.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/CX25STVI/Ritter et al. - 2018 - Online Structured Laplace Approximations For Overcoming Catastrophic Forgetting.pdf;/Users/giulialanzillotta/Zotero/storage/VL92E3CY/1805.html}
}

@inproceedings{robinsCatastrophicForgettingNeural1993,
  title = {Catastrophic Forgetting in Neural Networks: The Role of Rehearsal Mechanisms},
  shorttitle = {Catastrophic Forgetting in Neural Networks},
  booktitle = {Proceedings 1993 {{The First New Zealand International Two-Stream Conference}} on {{Artificial Neural Networks}} and {{Expert Systems}}},
  author = {Robins, A.},
  year = 1993,
  month = nov,
  pages = {65--68},
  doi = {10.1109/ANNES.1993.323080},
  urldate = {2025-09-30},
  abstract = {The author examines the problem of catastrophic forgetting-the overwriting of old information-in neural networks. He notes that R. Ratcliff's (1990) experiments with rehearsal regimes are a possible solution to catastrophic forgetting and describes sweep rehearsal-a much more effective regime. The use of sweep rehearsal, however, eventually encounters practical limits as the ability to recognize learned items begins to diminish. The author suggests that sweep rehearsal extends the approach of rehearsal mechanisms as far as is practicable, and exposes their eventual limitations.{$<>$}},
  keywords = {Computer science,Formal specifications,Information processing,Intelligent networks,Learning systems,Neural networks,Robustness,Stability,Supervised learning,Unsupervised learning},
  file = {/Users/giulialanzillotta/Zotero/storage/XM2FJ7NU/Robins - 1993 - Catastrophic forgetting in neural networks the role of rehearsal mechanisms.pdf}
}

@article{robinsCatastrophicForgettingRehearsal1995,
  title = {Catastrophic Forgetting, Rehearsal and Pseudorehearsal},
  author = {Robins, Anthony},
  year = 1995,
  journal = {Connection Science},
  volume = {7},
  number = {2},
  pages = {123--146},
  publisher = {Taylor \& Francis}
}

@inproceedings{rolnickExperienceReplayContinual2019,
  title = {Experience Replay for Continual Learning},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Rolnick, David and Ahuja, Arun and Schwarz, Jonathan and Lillicrap, Timothy and Wayne, Greg},
  year = 2019,
  volume = {32}
}

@article{rusuProgressiveNeuralNetworks2016,
  title = {Progressive Neural Networks},
  author = {Rusu, A. A. and Rabinowitz, N. C. and Desjardins, G. and Soyer, H. and Kirkpatrick, J. and Kavukcuoglu, K. and Pascanu, R. and Hadsell, R.},
  year = 2016,
  journal = {arXiv:1606.04671},
  eprint = {1606.04671},
  archiveprefix = {arXiv}
}

@misc{sagunEigenvaluesHessianDeep2017,
  title = {Eigenvalues of the {{Hessian}} in {{Deep Learning}}: {{Singularity}} and {{Beyond}}},
  shorttitle = {Eigenvalues of the {{Hessian}} in {{Deep Learning}}},
  author = {Sagun, Levent and Bottou, Leon and LeCun, Yann},
  year = 2017,
  month = oct,
  number = {arXiv:1611.07476},
  eprint = {1611.07476},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1611.07476},
  urldate = {2025-04-18},
  abstract = {We look at the eigenvalues of the Hessian of a loss function before and after training. The eigenvalue distribution is seen to be composed of two parts, the bulk which is concentrated around zero, and the edges which are scattered away from zero. We present empirical evidence for the bulk indicating how over-parametrized the system is, and for the edges that depend on the input data.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/LEFPI38L/Sagun et al. - 2017 - Eigenvalues of the Hessian in Deep Learning Singularity and Beyond.pdf;/Users/giulialanzillotta/Zotero/storage/NJ5EPM3Q/1611.html}
}

@misc{sagunEmpiricalAnalysisHessian2018,
  title = {Empirical {{Analysis}} of the {{Hessian}} of {{Over-Parametrized Neural Networks}}},
  author = {Sagun, Levent and Evci, Utku and Guney, V. Ugur and Dauphin, Yann and Bottou, Leon},
  year = 2018,
  month = may,
  number = {arXiv:1706.04454},
  eprint = {1706.04454},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1706.04454},
  urldate = {2025-04-18},
  abstract = {We study the properties of common loss surfaces through their Hessian matrix. In particular, in the context of deep learning, we empirically show that the spectrum of the Hessian is composed of two parts: (1) the bulk centered near zero, (2) and outliers away from the bulk. We present numerical evidence and mathematical justifications to the following conjectures laid out by Sagun et al. (2016): Fixing data, increasing the number of parameters merely scales the bulk of the spectrum; fixing the dimension and changing the data (for instance adding more clusters or making the data less separable) only affects the outliers. We believe that our observations have striking implications for non-convex optimization in high dimensions. First, the flatness of such landscapes (which can be measured by the singularity of the Hessian) implies that classical notions of basins of attraction may be quite misleading. And that the discussion of wide/narrow basins may be in need of a new perspective around over-parametrization and redundancy that are able to create large connected components at the bottom of the landscape. Second, the dependence of small number of large eigenvalues to the data distribution can be linked to the spectrum of the covariance matrix of gradients of model outputs. With this in mind, we may reevaluate the connections within the data-architecture-algorithm framework of a model, hoping that it would shed light into the geometry of high-dimensional and non-convex spaces in modern applications. In particular, we present a case that links the two observations: small and large batch gradient descent appear to converge to different basins of attraction but we show that they are in fact connected through their flat region and so belong to the same basin.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/UM5HQIYY/Sagun et al. - 2018 - Empirical Analysis of the Hessian of Over-Parametrized Neural Networks.pdf;/Users/giulialanzillotta/Zotero/storage/QCKPSVAN/1706.html}
}

@misc{sahaGradientProjectionMemory2021,
  title = {Gradient {{Projection Memory}} for {{Continual Learning}}},
  author = {Saha, Gobinda and Garg, Isha and Roy, Kaushik},
  year = 2021,
  month = mar,
  number = {arXiv:2103.09762},
  eprint = {2103.09762},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2103.09762},
  urldate = {2025-07-29},
  abstract = {The ability to learn continually without forgetting the past tasks is a desired attribute for artificial learning systems. Existing approaches to enable such learning in artificial neural networks usually rely on network growth, importance based weight update or replay of old data from the memory. In contrast, we propose a novel approach where a neural network learns new tasks by taking gradient steps in the orthogonal direction to the gradient subspaces deemed important for the past tasks. We find the bases of these subspaces by analyzing network representations (activations) after learning each task with Singular Value Decomposition (SVD) in a single shot manner and store them in the memory as Gradient Projection Memory (GPM). With qualitative and quantitative analyses, we show that such orthogonal gradient descent induces minimum to no interference with the past tasks, thereby mitigates forgetting. We evaluate our algorithm on diverse image classification datasets with short and long sequences of tasks and report better or on-par performance compared to the state-of-the-art approaches.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/T7BBZRI8/Saha et al. - 2021 - Gradient Projection Memory for Continual Learning.pdf;/Users/giulialanzillotta/Zotero/storage/YIZBYAJC/2103.html}
}

@misc{sahaGradientProjectionMemory2021a,
  title = {Gradient {{Projection Memory}} for {{Continual Learning}}},
  author = {Saha, Gobinda and Garg, Isha and Roy, Kaushik},
  year = 2021,
  month = mar,
  number = {arXiv:2103.09762},
  eprint = {2103.09762},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2103.09762},
  urldate = {2025-09-30},
  abstract = {The ability to learn continually without forgetting the past tasks is a desired attribute for artificial learning systems. Existing approaches to enable such learning in artificial neural networks usually rely on network growth, importance based weight update or replay of old data from the memory. In contrast, we propose a novel approach where a neural network learns new tasks by taking gradient steps in the orthogonal direction to the gradient subspaces deemed important for the past tasks. We find the bases of these subspaces by analyzing network representations (activations) after learning each task with Singular Value Decomposition (SVD) in a single shot manner and store them in the memory as Gradient Projection Memory (GPM). With qualitative and quantitative analyses, we show that such orthogonal gradient descent induces minimum to no interference with the past tasks, thereby mitigates forgetting. We evaluate our algorithm on diverse image classification datasets with short and long sequences of tasks and report better or on-par performance compared to the state-of-the-art approaches.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/P42Y8T4X/Saha et al. - 2021 - Gradient Projection Memory for Continual Learning.pdf;/Users/giulialanzillotta/Zotero/storage/3YQ7KG4D/2103.html}
}

@misc{saxeExactSolutionsNonlinear2014,
  title = {Exact Solutions to the Nonlinear Dynamics of Learning in Deep Linear Neural Networks},
  author = {Saxe, Andrew M. and McClelland, James L. and Ganguli, Surya},
  year = 2014,
  month = feb,
  number = {arXiv:1312.6120},
  eprint = {1312.6120},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1312.6120},
  urldate = {2025-09-15},
  abstract = {Despite the widespread practical success of deep learning methods, our theoretical understanding of the dynamics of learning in deep neural networks remains quite sparse. We attempt to bridge the gap between the theory and practice of deep learning by systematically analyzing learning dynamics for the restricted case of deep linear neural networks. Despite the linearity of their input-output map, such networks have nonlinear gradient descent dynamics on weights that change with the addition of each new hidden layer. We show that deep linear networks exhibit nonlinear learning phenomena similar to those seen in simulations of nonlinear networks, including long plateaus followed by rapid transitions to lower error solutions, and faster convergence from greedy unsupervised pretraining initial conditions than from random initial conditions. We provide an analytical description of these phenomena by finding new exact solutions to the nonlinear dynamics of deep learning. Our theoretical analysis also reveals the surprising finding that as the depth of a network approaches infinity, learning speed can nevertheless remain finite: for a special class of initial conditions on the weights, very deep networks incur only a finite, depth independent, delay in learning speed relative to shallow networks. We show that, under certain conditions on the training data, unsupervised pretraining can find this special class of initial conditions, while scaled random Gaussian initializations cannot. We further exhibit a new class of random orthogonal initial conditions on weights that, like unsupervised pre-training, enjoys depth independent learning times. We further show that these initial conditions also lead to faithful propagation of gradients even in deep nonlinear networks, as long as they operate in a special regime known as the edge of chaos.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Condensed Matter - Disordered Systems and Neural Networks,Quantitative Biology - Neurons and Cognition,Statistics - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/4ZJASVNS/Saxe et al. - 2014 - Exact solutions to the nonlinear dynamics of learning in deep linear neural networks.pdf;/Users/giulialanzillotta/Zotero/storage/TFAQNKJF/1312.html}
}

@misc{saxeExactSolutionsNonlinear2014a,
  title = {Exact Solutions to the Nonlinear Dynamics of Learning in Deep Linear Neural Networks},
  author = {Saxe, Andrew M. and McClelland, James L. and Ganguli, Surya},
  year = 2014,
  month = feb,
  number = {arXiv:1312.6120},
  eprint = {1312.6120},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1312.6120},
  urldate = {2025-09-16},
  abstract = {Despite the widespread practical success of deep learning methods, our theoretical understanding of the dynamics of learning in deep neural networks remains quite sparse. We attempt to bridge the gap between the theory and practice of deep learning by systematically analyzing learning dynamics for the restricted case of deep linear neural networks. Despite the linearity of their input-output map, such networks have nonlinear gradient descent dynamics on weights that change with the addition of each new hidden layer. We show that deep linear networks exhibit nonlinear learning phenomena similar to those seen in simulations of nonlinear networks, including long plateaus followed by rapid transitions to lower error solutions, and faster convergence from greedy unsupervised pretraining initial conditions than from random initial conditions. We provide an analytical description of these phenomena by finding new exact solutions to the nonlinear dynamics of deep learning. Our theoretical analysis also reveals the surprising finding that as the depth of a network approaches infinity, learning speed can nevertheless remain finite: for a special class of initial conditions on the weights, very deep networks incur only a finite, depth independent, delay in learning speed relative to shallow networks. We show that, under certain conditions on the training data, unsupervised pretraining can find this special class of initial conditions, while scaled random Gaussian initializations cannot. We further exhibit a new class of random orthogonal initial conditions on weights that, like unsupervised pre-training, enjoys depth independent learning times. We further show that these initial conditions also lead to faithful propagation of gradients even in deep nonlinear networks, as long as they operate in a special regime known as the edge of chaos.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Condensed Matter - Disordered Systems and Neural Networks,Quantitative Biology - Neurons and Cognition,Statistics - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/MEMBSQ6K/Saxe et al. - 2014 - Exact solutions to the nonlinear dynamics of learning in deep linear neural networks.pdf;/Users/giulialanzillotta/Zotero/storage/7TM5BXLY/1312.html}
}

@misc{ScalingLawDataEfficient,
  title = {Beyond {{Scaling Law}}: {{A Data-Efficient Distillation Framework}} for {{Reasoning}}},
  urldate = {2025-10-17},
  howpublished = {https://arxiv.org/html/2508.09883v1?utm\_source=chatgpt.com},
  file = {/Users/giulialanzillotta/Zotero/storage/VZS6L7U9/2508.html}
}

@article{schaulPRIORITIZEDEXPERIENCEREPLAY2016,
  title = {{{PRIORITIZED EXPERIENCE REPLAY}}},
  author = {Schaul, Tom and Quan, John and Antonoglou, Ioannis and Silver, David},
  year = 2016,
  abstract = {Experience replay lets online reinforcement learning agents remember and reuse experiences from the past. In prior work, experience transitions were uniformly sampled from a replay memory. However, this approach simply replays transitions at the same frequency that they were originally experienced, regardless of their significance. In this paper we develop a framework for prioritizing experience, so as to replay important transitions more frequently, and therefore learn more efficiently. We use prioritized experience replay in Deep Q-Networks (DQN), a reinforcement learning algorithm that achieved human-level performance across many Atari games. DQN with prioritized experience replay achieves a new stateof-the-art, outperforming DQN with uniform replay on 41 out of 49 games.},
  langid = {english},
  file = {/Users/giulialanzillotta/Zotero/storage/PSTMQA4B/Schaul et al. - 2016 - PRIORITIZED EXPERIENCE REPLAY.pdf}
}

@article{schulmanHighdimensionalContinuousControl2015,
  title = {High-Dimensional Continuous Control Using Generalized Advantage Estimation},
  author = {Schulman, John and Moritz, Philipp and Levine, Sergey and Jordan, Michael and Abbeel, Pieter},
  year = 2015,
  journal = {arXiv preprint arXiv:1506.02438},
  eprint = {1506.02438},
  archiveprefix = {arXiv}
}

@article{schulmanProximalPolicyOptimization2017,
  title = {Proximal {{Policy Optimization Algorithms}}},
  author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  year = 2017,
  journal = {arXiv preprint arXiv:1707.06347},
  eprint = {1707.06347},
  archiveprefix = {arXiv}
}

@inproceedings{schwarzProgressAmpCompress2018,
  title = {Progress \&amp; {{Compress}}: {{A}} Scalable Framework for Continual Learning},
  booktitle = {Proceedings of the 35th {{International Conference}} on {{Machine Learning}}},
  author = {Schwarz, Jonathan and Czarnecki, Wojciech and Luketina, Jelena and {Grabska-Barwinska}, Agnieszka and Teh, Yee Whye and Pascanu, Razvan and Hadsell, Raia},
  editor = {Dy, Jennifer and Krause, Andreas},
  year = 2018,
  month = jul,
  series = {Proceedings of {{Machine Learning Research}}},
  volume = {80},
  pages = {4528--4537},
  publisher = {PMLR}
}

@article{senerMultitaskLearningMultiobjective2018,
  title = {Multi-Task Learning as Multi-Objective Optimization},
  author = {Sener, Ozan and Koltun, Vladlen},
  year = 2018,
  journal = {Advances in neural information processing systems},
  volume = {31}
}

@inproceedings{seoLearningEquiangularRepresentations2024,
  title = {Learning Equi-Angular Representations for Online Continual Learning},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Seo, Minhyuk and Koh, Hyunseo and Jeung, Wonje and Lee, Minjae and Kim, San and Lee, Hankook and Cho, Sungjun and Choi, Sungik and Kim, Hyunwoo and Choi, Jonghyun},
  year = 2024,
  pages = {23933--23942}
}

@inproceedings{seoLearningEquiAngularRepresentations2024a,
  title = {Learning {{Equi-Angular Representations}} for {{Online Continual Learning}}},
  booktitle = {2024 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Seo, Minhyuk and Koh, Hyunseo and Jeung, Wonje and Lee, Minjae and Kim, San and Lee, Hankook and Cho, Sungjun and Choi, Sungik and Kim, Hyunwoo and Choi, Jonghyun},
  year = 2024,
  month = jun,
  pages = {23933--23942},
  publisher = {IEEE},
  address = {Seattle, WA, USA},
  doi = {10.1109/CVPR52733.2024.02259},
  urldate = {2025-09-16},
  abstract = {Online continual learning suffers from an underfitted solution due to insufficient training for prompt model update (e.g., single-epoch training). To address the challenge, we propose an efficient online continual learning method using the neural collapse phenomenon. In particular, we induce neural collapse to form a simplex equiangular tight frame (ETF) structure in the representation space so that the continuously learned model with a single epoch can better fit to the streamed data by proposing preparatory data training and residual correction in the representation space. With an extensive set of empirical validations using CIFAR10/100, TinyImageNet, ImageNet-200, and ImageNet-1K, we show that our proposed method outperforms state-ofthe-art methods by a noticeable margin in various online continual learning scenarios such as disjoint and Gaussian scheduled continuous (i.e., boundary-free) data setups. Code is available at https://github.com/ yonseivnl/earl.},
  copyright = {https://doi.org/10.15223/policy-029},
  isbn = {979-8-3503-5300-6},
  langid = {english},
  file = {/Users/giulialanzillotta/Zotero/storage/6W64WCUD/Seo et al. - 2024 - Learning Equi-Angular Representations for Online Continual Learning.pdf}
}

@misc{serraOvercomingCatastrophicForgetting2018,
  title = {Overcoming Catastrophic Forgetting with Hard Attention to the Task},
  author = {Serr{\`a}, Joan and Sur{\'i}s, D{\'i}dac and Miron, Marius and Karatzoglou, Alexandros},
  year = 2018,
  month = may,
  number = {arXiv:1801.01423},
  eprint = {1801.01423},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1801.01423},
  urldate = {2025-10-01},
  abstract = {Catastrophic forgetting occurs when a neural network loses the information learned in a previous task after training on subsequent tasks. This problem remains a hurdle for artificial intelligence systems with sequential learning capabilities. In this paper, we propose a task-based hard attention mechanism that preserves previous tasks' information without affecting the current task's learning. A hard attention mask is learned concurrently to every task, through stochastic gradient descent, and previous masks are exploited to condition such learning. We show that the proposed mechanism is effective for reducing catastrophic forgetting, cutting current rates by 45 to 80\%. We also show that it is robust to different hyperparameter choices, and that it offers a number of monitoring capabilities. The approach features the possibility to control both the stability and compactness of the learned knowledge, which we believe makes it also attractive for online learning or network compression applications.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/WRHPSDYA/SerrÃ  et al. - 2018 - Overcoming catastrophic forgetting with hard attention to the task.pdf;/Users/giulialanzillotta/Zotero/storage/XZ8T2I7X/1801.html}
}

@misc{shazeerOutrageouslyLargeNeural2017,
  title = {Outrageously {{Large Neural Networks}}: {{The Sparsely-Gated Mixture-of-Experts Layer}}},
  shorttitle = {Outrageously {{Large Neural Networks}}},
  author = {Shazeer, Noam and Mirhoseini, Azalia and Maziarz, Krzysztof and Davis, Andy and Le, Quoc and Hinton, Geoffrey and Dean, Jeff},
  year = 2017,
  month = jan,
  number = {arXiv:1701.06538},
  eprint = {1701.06538},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1701.06538},
  urldate = {2025-10-03},
  abstract = {The capacity of a neural network to absorb information is limited by its number of parameters. Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation. In practice, however, there are significant algorithmic and performance challenges. In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters. We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks. A trainable gating network determines a sparse combination of these experts to use for each example. We apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora. We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers. On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/SXFKV2KM/Shazeer et al. - 2017 - Outrageously Large Neural Networks The Sparsely-Gated Mixture-of-Experts Layer.pdf;/Users/giulialanzillotta/Zotero/storage/RGPGLAB5/1701.html}
}

@misc{shazeerOutrageouslyLargeNeural2017a,
  title = {Outrageously {{Large Neural Networks}}: {{The Sparsely-Gated Mixture-of-Experts Layer}}},
  shorttitle = {Outrageously {{Large Neural Networks}}},
  author = {Shazeer, Noam and Mirhoseini, Azalia and Maziarz, Krzysztof and Davis, Andy and Le, Quoc and Hinton, Geoffrey and Dean, Jeff},
  year = 2017,
  month = jan,
  number = {arXiv:1701.06538},
  eprint = {1701.06538},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1701.06538},
  urldate = {2025-10-03},
  abstract = {The capacity of a neural network to absorb information is limited by its number of parameters. Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation. In practice, however, there are significant algorithmic and performance challenges. In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters. We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks. A trainable gating network determines a sparse combination of these experts to use for each example. We apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora. We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers. On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/JMB4HKJF/Shazeer et al. - 2017 - Outrageously Large Neural Networks The Sparsely-Gated Mixture-of-Experts Layer.pdf;/Users/giulialanzillotta/Zotero/storage/ZVNTBNQG/1701.html}
}

@misc{shazeerOutrageouslyLargeNeural2017b,
  title = {Outrageously {{Large Neural Networks}}: {{The Sparsely-Gated Mixture-of-Experts Layer}}},
  shorttitle = {Outrageously {{Large Neural Networks}}},
  author = {Shazeer, Noam and Mirhoseini, Azalia and Maziarz, Krzysztof and Davis, Andy and Le, Quoc and Hinton, Geoffrey and Dean, Jeff},
  year = 2017,
  month = jan,
  number = {arXiv:1701.06538},
  eprint = {1701.06538},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1701.06538},
  urldate = {2025-10-03},
  abstract = {The capacity of a neural network to absorb information is limited by its number of parameters. Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation. In practice, however, there are significant algorithmic and performance challenges. In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters. We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks. A trainable gating network determines a sparse combination of these experts to use for each example. We apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora. We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers. On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/YN4MTKHN/Shazeer et al. - 2017 - Outrageously Large Neural Networks The Sparsely-Gated Mixture-of-Experts Layer.pdf;/Users/giulialanzillotta/Zotero/storage/T2LF6EBU/1701.html}
}

@misc{shiContinualLearningLarge2024,
  title = {Continual {{Learning}} of {{Large Language Models}}: {{A Comprehensive Survey}}},
  shorttitle = {Continual {{Learning}} of {{Large Language Models}}},
  author = {Shi, Haizhou and Xu, Zihao and Wang, Hengyi and Qin, Weiyi and Wang, Wenyuan and Wang, Yibin and Wang, Zifeng and Ebrahimi, Sayna and Wang, Hao},
  year = 2024,
  month = jun,
  number = {arXiv:2404.16789},
  eprint = {2404.16789},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2404.16789},
  urldate = {2025-10-08},
  abstract = {The recent success of large language models (LLMs) trained on static, pre-collected, general datasets has sparked numerous research directions and applications. One such direction addresses the non-trivial challenge of integrating pre-trained LLMs into dynamic data distributions, task structures, and user preferences. Pre-trained LLMs, when tailored for specific needs, often experience significant performance degradation in previous knowledge domains -- a phenomenon known as "catastrophic forgetting". While extensively studied in the continual learning (CL) community, it presents new manifestations in the realm of LLMs. In this survey, we provide a comprehensive overview of the current research progress on LLMs within the context of CL. This survey is structured into four main sections: we first describe an overview of continually learning LLMs, consisting of two directions of continuity: vertical continuity (or vertical continual learning), i.e., continual adaptation from general to specific capabilities, and horizontal continuity (or horizontal continual learning), i.e., continual adaptation across time and domains (Section 3). We then summarize three stages of learning LLMs in the context of modern CL: Continual Pre-Training (CPT), Domain-Adaptive Pre-training (DAP), and Continual Fine-Tuning (CFT) (Section 4). Then we provide an overview of evaluation protocols for continual learning with LLMs, along with the current available data sources (Section 5). Finally, we discuss intriguing questions pertaining to continual learning for LLMs (Section 6). The full list of papers examined in this survey is available at https://github.com/Wang-ML-Lab/llm-continual-learning-survey.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/KA3VHG3H/Shi et al. - 2024 - Continual Learning of Large Language Models A Comprehensive Survey.pdf;/Users/giulialanzillotta/Zotero/storage/NTE485SW/2404.html}
}

@misc{shiltonGradientDescentNeural2023,
  title = {Gradient {{Descent}} in {{Neural Networks}} as {{Sequential Learning}} in {{RKBS}}},
  author = {Shilton, Alistair and Gupta, Sunil and Rana, Santu and Venkatesh, Svetha},
  year = 2023,
  month = feb,
  number = {arXiv:2302.00205},
  eprint = {2302.00205},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2302.00205},
  urldate = {2025-07-28},
  abstract = {The study of Neural Tangent Kernels (NTKs) has provided much needed insight into convergence and generalization properties of neural networks in the over-parametrized (wide) limit by approximating the network using a first-order Taylor expansion with respect to its weights in the neighborhood of their initialization values. This allows neural network training to be analyzed from the perspective of reproducing kernel Hilbert spaces (RKHS), which is informative in the over-parametrized regime, but a poor approximation for narrower networks as the weights change more during training. Our goal is to extend beyond the limits of NTK toward a more general theory. We construct an exact power-series representation of the neural network in a finite neighborhood of the initial weights as an inner product of two feature maps, respectively from data and weight-step space, to feature space, allowing neural network training to be analyzed from the perspective of reproducing kernel \{{\textbackslash}em Banach\} space (RKBS). We prove that, regardless of width, the training sequence produced by gradient descent can be exactly replicated by regularized sequential learning in RKBS. Using this, we present novel bound on uniform convergence where the iterations count and learning rate play a central role, giving new theoretical insight into neural network training.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/VXNUW5VL/Shilton et al. - 2023 - Gradient Descent in Neural Networks as Sequential Learning in RKBS.pdf;/Users/giulialanzillotta/Zotero/storage/57IX57T9/2302.html}
}

@misc{shiltonNovelKernelModels2024,
  title = {Novel {{Kernel Models}} and {{Exact Representor Theory}} for {{Neural Networks Beyond}} the {{Over-Parameterized Regime}}},
  author = {Shilton, Alistair and Gupta, Sunil and Rana, Santu and Venkatesh, Svetha},
  year = 2024,
  month = may,
  number = {arXiv:2405.15254},
  eprint = {2405.15254},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2405.15254},
  urldate = {2025-07-28},
  abstract = {This paper presents two models of neural-networks and their training applicable to neural networks of arbitrary width, depth and topology, assuming only finite-energy neural activations; and a novel representor theory for neural networks in terms of a matrix-valued kernel. The first model is exact (un-approximated) and global, casting the neural network as an elements in a reproducing kernel Banach space (RKBS); we use this model to provide tight bounds on Rademacher complexity. The second model is exact and local, casting the change in neural network function resulting from a bounded change in weights and biases (ie. a training step) in reproducing kernel Hilbert space (RKHS) in terms of a local-intrinsic neural kernel (LiNK). This local model provides insight into model adaptation through tight bounds on Rademacher complexity of network adaptation. We also prove that the neural tangent kernel (NTK) is a first-order approximation of the LiNK kernel. Finally, and noting that the LiNK does not provide a representor theory for technical reasons, we present an exact novel representor theory for layer-wise neural network training with unregularized gradient descent in terms of a local-extrinsic neural kernel (LeNK). This representor theory gives insight into the role of higher-order statistics in neural network training and the effect of kernel evolution in neural-network kernel models. Throughout the paper (a) feedforward ReLU networks and (b) residual networks (ResNet) are used as illustrative examples.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/JNLNULNY/Shilton et al. - 2024 - Novel Kernel Models and Exact Representor Theory for Neural Networks Beyond the Over-Parameterized R.pdf;/Users/giulialanzillotta/Zotero/storage/WN9DD8QA/2405.html}
}

@inproceedings{shinContinualLearningDeep2017,
  title = {Continual Learning with Deep Generative Replay},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Shin, H. and Lee, J. K. and Kim, J. and Kim, J.},
  year = 2017,
  address = {Long Beach, CA}
}

@misc{shinContinualLearningDeep2017a,
  title = {Continual {{Learning}} with {{Deep Generative Replay}}},
  author = {Shin, Hanul and Lee, Jung Kwon and Kim, Jaehong and Kim, Jiwon},
  year = 2017,
  month = dec,
  number = {arXiv:1705.08690},
  eprint = {1705.08690},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1705.08690},
  urldate = {2025-10-14},
  abstract = {Attempts to train a comprehensive artificial intelligence capable of solving multiple tasks have been impeded by a chronic problem called catastrophic forgetting. Although simply replaying all previous data alleviates the problem, it requires large memory and even worse, often infeasible in real world applications where the access to past data is limited. Inspired by the generative nature of hippocampus as a short-term memory system in primate brain, we propose the Deep Generative Replay, a novel framework with a cooperative dual model architecture consisting of a deep generative model ("generator") and a task solving model ("solver"). With only these two models, training data for previous tasks can easily be sampled and interleaved with those for a new task. We test our methods in several sequential learning settings involving image classification tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/U3927MMU/Shin et al. - 2017 - Continual Learning with Deep Generative Replay.pdf;/Users/giulialanzillotta/Zotero/storage/TB4KEJID/1705.html}
}

@misc{shinContinualLearningDeep2017b,
  title = {Continual {{Learning}} with {{Deep Generative Replay}}},
  author = {Shin, Hanul and Lee, Jung Kwon and Kim, Jaehong and Kim, Jiwon},
  year = 2017,
  month = dec,
  number = {arXiv:1705.08690},
  eprint = {1705.08690},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1705.08690},
  urldate = {2025-10-14},
  abstract = {Attempts to train a comprehensive artificial intelligence capable of solving multiple tasks have been impeded by a chronic problem called catastrophic forgetting. Although simply replaying all previous data alleviates the problem, it requires large memory and even worse, often infeasible in real world applications where the access to past data is limited. Inspired by the generative nature of hippocampus as a short-term memory system in primate brain, we propose the Deep Generative Replay, a novel framework with a cooperative dual model architecture consisting of a deep generative model ("generator") and a task solving model ("solver"). With only these two models, training data for previous tasks can easily be sampled and interleaved with those for a new task. We test our methods in several sequential learning settings involving image classification tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/T798ZR5A/Shin et al. - 2017 - Continual Learning with Deep Generative Replay.pdf;/Users/giulialanzillotta/Zotero/storage/K3BRGP9I/1705.html}
}

@inproceedings{silverCLEARBenchmarkContinual2021,
  title = {The {{CLEAR Benchmark}}: {{Continual LEArning}} on {{Real-World Imagery}}},
  booktitle = {Proceedings of the {{Neural Information Processing Systems Track}} on {{Datasets}} and {{Benchmarks}} 1 ({{NeurIPS Datasets}} and {{Benchmarks}} 2021)},
  author = {Silver, Daniel L and Yang, Qiang and Li, Lianghao and McCloskey, Michael and Cohen, Neal J and Lin, Zhiqiu and Shi, Jia and Pathak, Deepak and Ramanan, Deva},
  year = 2021,
  volume = {24},
  pages = {109--165},
  publisher = {Elsevier}
}

@inproceedings{silverLifelongMachineLearning2013,
  title = {Lifelong Machine Learning Systems: {{Beyond}} Learning Algorithms},
  booktitle = {2013 {{AAAI}} Spring Symposium Series},
  author = {Silver, Daniel L and Yang, Qiang and Li, Lianghao},
  year = 2013
}

@misc{simonLearningGeodesicPath2021,
  title = {On {{Learning}} the {{Geodesic Path}} for {{Incremental Learning}}},
  author = {Simon, Christian and Koniusz, Piotr and Harandi, Mehrtash},
  year = 2021,
  month = apr,
  number = {arXiv:2104.08572},
  eprint = {2104.08572},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2104.08572},
  urldate = {2025-09-30},
  abstract = {Neural networks notoriously suffer from the problem of catastrophic forgetting, the phenomenon of forgetting the past knowledge when acquiring new knowledge. Overcoming catastrophic forgetting is of significant importance to emulate the process of "incremental learning", where the model is capable of learning from sequential experience in an efficient and robust way. State-of-the-art techniques for incremental learning make use of knowledge distillation towards preventing catastrophic forgetting. Therein, one updates the network while ensuring that the network's responses to previously seen concepts remain stable throughout updates. This in practice is done by minimizing the dissimilarity between current and previous responses of the network one way or another. Our work contributes a novel method to the arsenal of distillation techniques. In contrast to the previous state of the art, we propose to firstly construct low-dimensional manifolds for previous and current responses and minimize the dissimilarity between the responses along the geodesic connecting the manifolds. This induces a more formidable knowledge distillation with smooth properties which preserves the past knowledge more efficiently as observed by our comprehensive empirical study.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/DV9GR9K5/Simon et al. - 2021 - On Learning the Geodesic Path for Incremental Learning.pdf;/Users/giulialanzillotta/Zotero/storage/NH7VBEWQ/2104.html}
}

@misc{simonLearningGeodesicPath2021a,
  title = {On {{Learning}} the {{Geodesic Path}} for {{Incremental Learning}}},
  author = {Simon, Christian and Koniusz, Piotr and Harandi, Mehrtash},
  year = 2021,
  month = apr,
  number = {arXiv:2104.08572},
  eprint = {2104.08572},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2104.08572},
  urldate = {2025-10-03},
  abstract = {Neural networks notoriously suffer from the problem of catastrophic forgetting, the phenomenon of forgetting the past knowledge when acquiring new knowledge. Overcoming catastrophic forgetting is of significant importance to emulate the process of "incremental learning", where the model is capable of learning from sequential experience in an efficient and robust way. State-of-the-art techniques for incremental learning make use of knowledge distillation towards preventing catastrophic forgetting. Therein, one updates the network while ensuring that the network's responses to previously seen concepts remain stable throughout updates. This in practice is done by minimizing the dissimilarity between current and previous responses of the network one way or another. Our work contributes a novel method to the arsenal of distillation techniques. In contrast to the previous state of the art, we propose to firstly construct low-dimensional manifolds for previous and current responses and minimize the dissimilarity between the responses along the geodesic connecting the manifolds. This induces a more formidable knowledge distillation with smooth properties which preserves the past knowledge more efficiently as observed by our comprehensive empirical study.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/DYCASP8I/Simon et al. - 2021 - On Learning the Geodesic Path for Incremental Learning.pdf;/Users/giulialanzillotta/Zotero/storage/7YRBN8IS/2104.html}
}

@article{singhAnalyticInsightsStructure,
  title = {Analytic {{Insights}} into {{Structure}} and {{Rank}} of {{Neural Network Hessian Maps}}},
  author = {Singh, Sidak Pal and Bachmann, Gregor and Hofmann, Thomas},
  langid = {english},
  file = {/Users/giulialanzillotta/Zotero/storage/WCQS7522/Singh et al. - Analytic Insights into Structure and Rank of Neural Network Hessian Maps.pdf}
}

@misc{singhCosineDecayEffectiveness2025,
  title = {Beyond {{Cosine Decay}}: {{On}} the Effectiveness of {{Infinite Learning Rate Schedule}} for {{Continual Pre-training}}},
  shorttitle = {Beyond {{Cosine Decay}}},
  author = {Singh, Vaibhav and Janson, Paul and Mehrbod, Paria and Ibrahim, Adam and Rish, Irina and Belilovsky, Eugene and Th{\'e}rien, Benjamin},
  year = 2025,
  month = sep,
  number = {arXiv:2503.02844},
  eprint = {2503.02844},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2503.02844},
  urldate = {2025-10-13},
  abstract = {The ever-growing availability of unlabeled data presents both opportunities and challenges for training artificial intelligence systems. While self-supervised learning (SSL) has emerged as a powerful paradigm for extracting meaningful representations from vast amounts of unlabeled data, existing methods still struggle to adapt to the non-stationary, non-IID nature of real-world data streams without forgetting previously learned knowledge. Recent works have adopted a repeated cosine annealing schedule for large-scale continual pre-training; however, these schedules (1) inherently cause forgetting during the re-warming phase and (2) have not been systematically compared to existing continual SSL methods. In this work, we systematically compare the widely used cosine schedule with the recently proposed infinite learning rate schedule and empirically find the latter to be a more effective alternative. Our extensive empirical evaluation across diverse image and language datasets demonstrates that the infinite learning rate schedule consistently enhances continual pre-training performance compared to a repeated cosine decay without being restricted to a fixed iteration budget. For instance, in a small-scale MAE pre-training setup, it outperforms several strong baselines from the literature. We then scale up our experiments to larger MAE pre-training and autoregressive language model pre-training. Our results show that the infinite learning rate schedule remains effective at scale, surpassing repeated cosine decay for both MAE pre-training and zero-shot LM benchmarks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/GX89GP6M/Singh et al. - 2025 - Beyond Cosine Decay On the effectiveness of Infinite Learning Rate Schedule for Continual Pre-train.pdf;/Users/giulialanzillotta/Zotero/storage/8NZA2AJ8/2503.html}
}

@misc{singhCosineDecayEffectiveness2025a,
  title = {Beyond {{Cosine Decay}}: {{On}} the Effectiveness of {{Infinite Learning Rate Schedule}} for {{Continual Pre-training}}},
  shorttitle = {Beyond {{Cosine Decay}}},
  author = {Singh, Vaibhav and Janson, Paul and Mehrbod, Paria and Ibrahim, Adam and Rish, Irina and Belilovsky, Eugene and Th{\'e}rien, Benjamin},
  year = 2025,
  month = sep,
  number = {arXiv:2503.02844},
  eprint = {2503.02844},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2503.02844},
  urldate = {2025-10-13},
  abstract = {The ever-growing availability of unlabeled data presents both opportunities and challenges for training artificial intelligence systems. While self-supervised learning (SSL) has emerged as a powerful paradigm for extracting meaningful representations from vast amounts of unlabeled data, existing methods still struggle to adapt to the non-stationary, non-IID nature of real-world data streams without forgetting previously learned knowledge. Recent works have adopted a repeated cosine annealing schedule for large-scale continual pre-training; however, these schedules (1) inherently cause forgetting during the re-warming phase and (2) have not been systematically compared to existing continual SSL methods. In this work, we systematically compare the widely used cosine schedule with the recently proposed infinite learning rate schedule and empirically find the latter to be a more effective alternative. Our extensive empirical evaluation across diverse image and language datasets demonstrates that the infinite learning rate schedule consistently enhances continual pre-training performance compared to a repeated cosine decay without being restricted to a fixed iteration budget. For instance, in a small-scale MAE pre-training setup, it outperforms several strong baselines from the literature. We then scale up our experiments to larger MAE pre-training and autoregressive language model pre-training. Our results show that the infinite learning rate schedule remains effective at scale, surpassing repeated cosine decay for both MAE pre-training and zero-shot LM benchmarks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/9CPHTMWW/Singh et al. - 2025 - Beyond Cosine Decay On the effectiveness of Infinite Learning Rate Schedule for Continual Pre-train.pdf;/Users/giulialanzillotta/Zotero/storage/I33J296A/2503.html}
}

@article{slivkinsIntroductionMultiarmedBandits2019,
  title = {Introduction to Multi-Armed Bandits},
  author = {Slivkins, Aleksandrs and others},
  year = 2019,
  journal = {Foundations and Trends{\textregistered} in Machine Learning},
  volume = {12},
  number = {1-2},
  pages = {1--286},
  publisher = {Now Publishers, Inc.}
}

@misc{smithCODAPromptCOntinualDecomposed2023,
  title = {{{CODA-Prompt}}: {{COntinual Decomposed Attention-based Prompting}} for {{Rehearsal-Free Continual Learning}}},
  shorttitle = {{{CODA-Prompt}}},
  author = {Smith, James Seale and Karlinsky, Leonid and Gutta, Vyshnavi and {Cascante-Bonilla}, Paola and Kim, Donghyun and Arbelle, Assaf and Panda, Rameswar and Feris, Rogerio and Kira, Zsolt},
  year = 2023,
  month = mar,
  number = {arXiv:2211.13218},
  eprint = {2211.13218},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2211.13218},
  urldate = {2025-10-03},
  abstract = {Computer vision models suffer from a phenomenon known as catastrophic forgetting when learning novel concepts from continuously shifting training data. Typical solutions for this continual learning problem require extensive rehearsal of previously seen data, which increases memory costs and may violate data privacy. Recently, the emergence of large-scale pre-trained vision transformer models has enabled prompting approaches as an alternative to data-rehearsal. These approaches rely on a key-query mechanism to generate prompts and have been found to be highly resistant to catastrophic forgetting in the well-established rehearsal-free continual learning setting. However, the key mechanism of these methods is not trained end-to-end with the task sequence. Our experiments show that this leads to a reduction in their plasticity, hence sacrificing new task accuracy, and inability to benefit from expanded parameter capacity. We instead propose to learn a set of prompt components which are assembled with input-conditioned weights to produce input-conditioned prompts, resulting in a novel attention-based end-to-end key-query scheme. Our experiments show that we outperform the current SOTA method DualPrompt on established benchmarks by as much as 4.5\% in average final accuracy. We also outperform the state of art by as much as 4.4\% accuracy on a continual learning benchmark which contains both class-incremental and domain-incremental task shifts, corresponding to many practical settings. Our code is available at https://github.com/GT-RIPL/CODA-Prompt},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/6S4M3BLT/Smith et al. - 2023 - CODA-Prompt COntinual Decomposed Attention-based Prompting for Rehearsal-Free Continual Learning.pdf}
}

@misc{songDoesSGDReally2025,
  title = {Does {{SGD}} Really Happen in Tiny Subspaces?},
  author = {Song, Minhak and Ahn, Kwangjun and Yun, Chulhee},
  year = 2025,
  month = mar,
  number = {arXiv:2405.16002},
  eprint = {2405.16002},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2405.16002},
  urldate = {2025-04-18},
  abstract = {Understanding the training dynamics of deep neural networks is challenging due to their high-dimensional nature and intricate loss landscapes. Recent studies have revealed that, along the training trajectory, the gradient approximately aligns with a low-rank top eigenspace of the training loss Hessian, referred to as the dominant subspace. Given this alignment, this paper explores whether neural networks can be trained within the dominant subspace, which, if feasible, could lead to more efficient training methods. Our primary observation is that when the SGD update is projected onto the dominant subspace, the training loss does not decrease further. This suggests that the observed alignment between the gradient and the dominant subspace is spurious. Surprisingly, projecting out the dominant subspace proves to be just as effective as the original update, despite removing the majority of the original update component. We observe similar behavior across practical setups, including the large learning rate regime (also known as Edge of Stability), Sharpness-Aware Minimization, momentum, and adaptive optimizers. We discuss the main causes and implications of this spurious alignment, shedding light on the dynamics of neural network training.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/WAYPSUVD/Song et al. - 2025 - Does SGD really happen in tiny subspaces.pdf;/Users/giulialanzillotta/Zotero/storage/R5V93H39/2405.html}
}

@misc{soudryImplicitBiasGradient2024,
  title = {The {{Implicit Bias}} of {{Gradient Descent}} on {{Separable Data}}},
  author = {Soudry, Daniel and Hoffer, Elad and Nacson, Mor Shpigel and Gunasekar, Suriya and Srebro, Nathan},
  year = 2024,
  month = oct,
  number = {arXiv:1710.10345},
  eprint = {1710.10345},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1710.10345},
  urldate = {2025-09-15},
  abstract = {We examine gradient descent on unregularized logistic regression problems, with homogeneous linear predictors on linearly separable datasets. We show the predictor converges to the direction of the max-margin (hard margin SVM) solution. The result also generalizes to other monotone decreasing loss functions with an infimum at infinity, to multi-class problems, and to training a weight layer in a deep network in a certain restricted setting. Furthermore, we show this convergence is very slow, and only logarithmic in the convergence of the loss itself. This can help explain the benefit of continuing to optimize the logistic or cross-entropy loss even after the training error is zero and the training loss is extremely small, and, as we show, even if the validation loss increases. Our methodology can also aid in understanding implicit regularization n more complex models and with other optimization methods.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/2BAHBA98/Soudry et al. - 2024 - The Implicit Bias of Gradient Descent on Separable Data.pdf;/Users/giulialanzillotta/Zotero/storage/IX4HI37V/1710.html}
}

@misc{soudryImplicitBiasGradient2024a,
  title = {The {{Implicit Bias}} of {{Gradient Descent}} on {{Separable Data}}},
  author = {Soudry, Daniel and Hoffer, Elad and Nacson, Mor Shpigel and Gunasekar, Suriya and Srebro, Nathan},
  year = 2024,
  month = oct,
  number = {arXiv:1710.10345},
  eprint = {1710.10345},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1710.10345},
  urldate = {2025-09-15},
  abstract = {We examine gradient descent on unregularized logistic regression problems, with homogeneous linear predictors on linearly separable datasets. We show the predictor converges to the direction of the max-margin (hard margin SVM) solution. The result also generalizes to other monotone decreasing loss functions with an infimum at infinity, to multi-class problems, and to training a weight layer in a deep network in a certain restricted setting. Furthermore, we show this convergence is very slow, and only logarithmic in the convergence of the loss itself. This can help explain the benefit of continuing to optimize the logistic or cross-entropy loss even after the training error is zero and the training loss is extremely small, and, as we show, even if the validation loss increases. Our methodology can also aid in understanding implicit regularization n more complex models and with other optimization methods.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/ILXETZGX/Soudry et al. - 2024 - The Implicit Bias of Gradient Descent on Separable Data.pdf;/Users/giulialanzillotta/Zotero/storage/DS9SCRR4/1710.html}
}

@misc{soudryImplicitBiasGradient2024b,
  title = {The {{Implicit Bias}} of {{Gradient Descent}} on {{Separable Data}}},
  author = {Soudry, Daniel and Hoffer, Elad and Nacson, Mor Shpigel and Gunasekar, Suriya and Srebro, Nathan},
  year = 2024,
  month = oct,
  number = {arXiv:1710.10345},
  eprint = {1710.10345},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1710.10345},
  urldate = {2025-09-16},
  abstract = {We examine gradient descent on unregularized logistic regression problems, with homogeneous linear predictors on linearly separable datasets. We show the predictor converges to the direction of the max-margin (hard margin SVM) solution. The result also generalizes to other monotone decreasing loss functions with an infimum at infinity, to multi-class problems, and to training a weight layer in a deep network in a certain restricted setting. Furthermore, we show this convergence is very slow, and only logarithmic in the convergence of the loss itself. This can help explain the benefit of continuing to optimize the logistic or cross-entropy loss even after the training error is zero and the training loss is extremely small, and, as we show, even if the validation loss increases. Our methodology can also aid in understanding implicit regularization n more complex models and with other optimization methods.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/B6QBAXHW/Soudry et al. - 2024 - The Implicit Bias of Gradient Descent on Separable Data.pdf;/Users/giulialanzillotta/Zotero/storage/K9WYAINU/1710.html}
}

@inproceedings{spaceAnglesSubspacesFinite1983,
  title = {On {{Angles}} between {{Subspaces}} of a {{Finite Dimensional Inner Product Space Per {\AA}ke Wedin Institute}} of {{Information Processing}}, {{Dept}} of {{Numerical Analysis University}} of {{Ume{\aa}}}, {{S-901}} 87 {{UME{\AA}}}, {{SWEDEN}}},
  booktitle = {Matrix {{Pencils}}: {{Proceedings}} of a {{Conference Held}} at {{Pite Havsbad}}, {{Sweden}}, {{March}} 22-24, 1982},
  author = {Space, Finite Dimensional Inner Product},
  year = 1983,
  volume = {973},
  pages = {263},
  publisher = {Lecture Notes in Mathematics}
}

@inproceedings{standleyWhichTasksShould2020,
  title = {Which Tasks Should Be Learned Together in Multi-Task Learning?},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Standley, Trevor and Zamir, Amir and Chen, Dawn and Guibas, Leonidas and Malik, Jitendra and Savarese, Silvio},
  year = 2020,
  pages = {9120--9132},
  publisher = {PMLR}
}

@misc{sukenikNeuralCollapseGlobally2025,
  title = {Neural {{Collapse}} Is {{Globally Optimal}} in {{Deep Regularized ResNets}} and {{Transformers}}},
  author = {S{\'u}ken{\'i}k, Peter and Lampert, Christoph H. and Mondelli, Marco},
  year = 2025,
  month = may,
  number = {arXiv:2505.15239},
  eprint = {2505.15239},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2505.15239},
  urldate = {2025-09-16},
  abstract = {The empirical emergence of neural collapse -- a surprising symmetry in the feature representations of the training data in the penultimate layer of deep neural networks -- has spurred a line of theoretical research aimed at its understanding. However, existing work focuses on data-agnostic models or, when data structure is taken into account, it remains limited to multi-layer perceptrons. Our paper fills both these gaps by analyzing modern architectures in a data-aware regime: we prove that global optima of deep regularized transformers and residual networks (ResNets) with LayerNorm trained with cross entropy or mean squared error loss are approximately collapsed, and the approximation gets tighter as the depth grows. More generally, we formally reduce any end-to-end large-depth ResNet or transformer training into an equivalent unconstrained features model, thus justifying its wide use in the literature even beyond data-agnostic settings. Our theoretical results are supported by experiments on computer vision and language datasets showing that, as the depth grows, neural collapse indeed becomes more prominent.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/AQ5A7KL9/SÃºkenÃ­k et al. - 2025 - Neural Collapse is Globally Optimal in Deep Regularized ResNets and Transformers.pdf;/Users/giulialanzillotta/Zotero/storage/S2DBV627/2505.html}
}

@article{sunAttentiveExperienceReplay2020,
  title = {Attentive {{Experience Replay}}},
  author = {Sun, Peiquan and Zhou, Wengang and Li, Houqiang},
  year = 2020,
  month = apr,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {34},
  number = {04},
  pages = {5900--5907},
  issn = {2374-3468},
  doi = {10.1609/aaai.v34i04.6049},
  urldate = {2025-10-14},
  abstract = {Experience replay (ER) has become an important component of deep reinforcement learning (RL) algorithms. ER enables RL algorithms to reuse past experiences for the update of current policy. By reusing a previous state for training, the RL agent would learn more accurate value estimation and better decision on that state. However, as the policy is continually updated, some states in past experiences become rarely visited, and optimization over these states might not improve the overall performance of current policy. To tackle this issue, we propose a new replay strategy to prioritize the transitions that contain states frequently visited by current policy. We introduce Attentive Experience Replay (AER), a novel experience replay algorithm that samples transitions according to the similarities between their states and the agent's state. We couple AER with different off-policy algorithms and demonstrate that AER makes consistent improvements on the suite of OpenAI gym tasks.},
  copyright = {Copyright (c) 2020 Association for the Advancement of Artificial Intelligence},
  langid = {english},
  file = {/Users/giulialanzillotta/Zotero/storage/H9KVRY9T/Sun et al. - 2020 - Attentive Experience Replay.pdf}
}

@misc{sunExploringExampleInfluence2022,
  title = {Exploring {{Example Influence}} in {{Continual Learning}}},
  author = {Sun, Qing and Lyu, Fan and Shang, Fanhua and Feng, Wei and Wan, Liang},
  year = 2022,
  month = sep,
  number = {arXiv:2209.12241},
  eprint = {2209.12241},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2209.12241},
  urldate = {2025-09-30},
  abstract = {Continual Learning (CL) sequentially learns new tasks like human beings, with the goal to achieve better Stability (S, remembering past tasks) and Plasticity (P, adapting to new tasks). Due to the fact that past training data is not available, it is valuable to explore the influence difference on S and P among training examples, which may improve the learning pattern towards better SP. Inspired by Influence Function (IF), we first study example influence via adding perturbation to example weight and computing the influence derivation. To avoid the storage and calculation burden of Hessian inverse in neural networks, we propose a simple yet effective MetaSP algorithm to simulate the two key steps in the computation of IF and obtain the S- and P-aware example influence. Moreover, we propose to fuse two kinds of example influence by solving a dual-objective optimization problem, and obtain a fused influence towards SP Pareto optimality. The fused influence can be used to control the update of model and optimize the storage of rehearsal. Empirical results show that our algorithm significantly outperforms state-of-the-art methods on both task- and class-incremental benchmark CL datasets.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/PPJITPGZ/Sun et al. - 2022 - Exploring Example Influence in Continual Learning.pdf;/Users/giulialanzillotta/Zotero/storage/U6LUEXFT/2209.html}
}

@misc{swaroopImprovingUnderstandingVariational2019,
  title = {Improving and {{Understanding Variational Continual Learning}}},
  author = {Swaroop, Siddharth and Nguyen, Cuong V. and Bui, Thang D. and Turner, Richard E.},
  year = 2019,
  month = may,
  number = {arXiv:1905.02099},
  eprint = {1905.02099},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1905.02099},
  urldate = {2025-09-30},
  abstract = {In the continual learning setting, tasks are encountered sequentially. The goal is to learn whilst i) avoiding catastrophic forgetting, ii) efficiently using model capacity, and iii) employing forward and backward transfer learning. In this paper, we explore how the Variational Continual Learning (VCL) framework achieves these desiderata on two benchmarks in continual learning: split MNIST and permuted MNIST. We first report significantly improved results on what was already a competitive approach. The improvements are achieved by establishing a new best practice approach to mean-field variational Bayesian neural networks. We then look at the solutions in detail. This allows us to obtain an understanding of why VCL performs as it does, and we compare the solution to what an `ideal' continual learning solution might be.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/XF4RNNIS/Swaroop et al. - 2019 - Improving and Understanding Variational Continual Learning.pdf;/Users/giulialanzillotta/Zotero/storage/XGIUE35L/1905.html}
}

@misc{tatzelDebiasingMiniBatchQuadratics2025,
  title = {Debiasing {{Mini-Batch Quadratics}} for {{Applications}} in {{Deep Learning}}},
  author = {Tatzel, Lukas and Mucs{\'a}nyi, B{\'a}lint and Hackel, Osane and Hennig, Philipp},
  year = 2025,
  month = jan,
  number = {arXiv:2410.14325},
  eprint = {2410.14325},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2410.14325},
  urldate = {2025-04-26},
  abstract = {Quadratic approximations form a fundamental building block of machine learning methods. E.g., second-order optimizers try to find the Newton step into the minimum of a local quadratic proxy to the objective function; and the second-order approximation of a network's loss function can be used to quantify the uncertainty of its outputs via the Laplace approximation. When computations on the entire training set are intractable - typical for deep learning - the relevant quantities are computed on mini-batches. This, however, distorts and biases the shape of the associated stochastic quadratic approximations in an intricate way with detrimental effects on applications. In this paper, we (i) show that this bias introduces a systematic error, (ii) provide a theoretical explanation for it, (iii) explain its relevance for second-order optimization and uncertainty quantification via the Laplace approximation in deep learning, and (iv) develop and evaluate debiasing strategies.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/RNBRSXPU/Tatzel et al. - 2025 - Debiasing Mini-Batch Quadratics for Applications in Deep Learning.pdf;/Users/giulialanzillotta/Zotero/storage/6BXM5GWX/2410.html}
}

@incollection{thrunContinualLearningRobotics2020,
  title = {Continual Learning for Robotics: {{Definition}}, Framework, Learning Strategies, Opportunities and Challenges},
  booktitle = {The Biology and Technology of Intelligent Autonomous Agents},
  author = {Thrun, Sebastian and Mitchell, Tom M and Lesort, Timoth{\'e}e and Lomonaco, Vincenzo and Stoian, Andrei and Maltoni, Davide and Filliat, David and {D{\'i}az-Rodr{\'i}guez}, Natalia},
  year = 2020,
  volume = {58},
  pages = {52--68},
  publisher = {Springer},
  issn = {1566-2535},
  doi = {10.1016/j.inffus.2019.12.004},
  keywords = {Catastrophic Forgetting,Continual Learning,Deep Learning,Lifelong Learning,Reinforcement Learning,Robotics,Survey}
}

@incollection{thrunLifelongRobotLearning1995,
  title = {Lifelong Robot Learning},
  booktitle = {The Biology and Technology of Intelligent Autonomous Agents},
  author = {Thrun, Sebastian and Mitchell, Tom M},
  year = 1995,
  pages = {165--196},
  publisher = {Springer},
  keywords = {Classics,Robotics}
}

@article{tianVisualAutoregressiveModeling2024,
  title = {Visual {{Autoregressive Modeling}}: {{Scalable Image Generation}} via {{Next-Scale Prediction}}},
  shorttitle = {Visual {{Autoregressive Modeling}}},
  author = {Tian, Keyu and Jiang, Yi and Yuan, Zehuan and Peng, Bingyue and Wang, Liwei},
  year = 2024,
  month = dec,
  journal = {Advances in Neural Information Processing Systems},
  volume = {37},
  pages = {84839--84865},
  urldate = {2025-10-14},
  langid = {english},
  file = {/Users/giulialanzillotta/Zotero/storage/M6BCXUHR/Tian et al. - 2024 - Visual Autoregressive Modeling Scalable Image Generation via Next-Scale Prediction.pdf}
}

@misc{tirerExtendedUnconstrainedFeatures2022,
  title = {Extended {{Unconstrained Features Model}} for {{Exploring Deep Neural Collapse}}},
  author = {Tirer, Tom and Bruna, Joan},
  year = 2022,
  month = oct,
  number = {arXiv:2202.08087},
  eprint = {2202.08087},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2202.08087},
  urldate = {2025-09-16},
  abstract = {The modern strategy for training deep neural networks for classification tasks includes optimizing the network's weights even after the training error vanishes to further push the training loss toward zero. Recently, a phenomenon termed "neural collapse" (NC) has been empirically observed in this training procedure. Specifically, it has been shown that the learned features (the output of the penultimate layer) of within-class samples converge to their mean, and the means of different classes exhibit a certain tight frame structure, which is also aligned with the last layer's weights. Recent papers have shown that minimizers with this structure emerge when optimizing a simplified "unconstrained features model" (UFM) with a regularized cross-entropy loss. In this paper, we further analyze and extend the UFM. First, we study the UFM for the regularized MSE loss, and show that the minimizers' features can have a more delicate structure than in the cross-entropy case. This affects also the structure of the weights. Then, we extend the UFM by adding another layer of weights as well as ReLU nonlinearity to the model and generalize our previous results. Finally, we empirically demonstrate the usefulness of our nonlinear extended UFM in modeling the NC phenomenon that occurs with practical networks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/S2MXG4W8/Tirer and Bruna - 2022 - Extended Unconstrained Features Model for Exploring Deep Neural Collapse.pdf;/Users/giulialanzillotta/Zotero/storage/4RDB8PKU/2202.html}
}

@misc{tirerExtendedUnconstrainedFeatures2022a,
  title = {Extended {{Unconstrained Features Model}} for {{Exploring Deep Neural Collapse}}},
  author = {Tirer, Tom and Bruna, Joan},
  year = 2022,
  month = oct,
  number = {arXiv:2202.08087},
  eprint = {2202.08087},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2202.08087},
  urldate = {2025-09-16},
  abstract = {The modern strategy for training deep neural networks for classification tasks includes optimizing the network's weights even after the training error vanishes to further push the training loss toward zero. Recently, a phenomenon termed "neural collapse" (NC) has been empirically observed in this training procedure. Specifically, it has been shown that the learned features (the output of the penultimate layer) of within-class samples converge to their mean, and the means of different classes exhibit a certain tight frame structure, which is also aligned with the last layer's weights. Recent papers have shown that minimizers with this structure emerge when optimizing a simplified "unconstrained features model" (UFM) with a regularized cross-entropy loss. In this paper, we further analyze and extend the UFM. First, we study the UFM for the regularized MSE loss, and show that the minimizers' features can have a more delicate structure than in the cross-entropy case. This affects also the structure of the weights. Then, we extend the UFM by adding another layer of weights as well as ReLU nonlinearity to the model and generalize our previous results. Finally, we empirically demonstrate the usefulness of our nonlinear extended UFM in modeling the NC phenomenon that occurs with practical networks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/I5PS8UYV/Tirer and Bruna - 2022 - Extended Unconstrained Features Model for Exploring Deep Neural Collapse.pdf;/Users/giulialanzillotta/Zotero/storage/MSJBSIAX/2202.html}
}

@inproceedings{titsiasFunctionalRegularisationContinual2019,
  title = {Functional {{Regularisation}} for {{Continual Learning}} with {{Gaussian Processes}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Titsias, Michalis K and Schwarz, Jonathan and Matthews, Alexander G de G and Pascanu, Razvan and Teh, Yee Whye},
  year = 2019,
  keywords = {Regularization}
}

@misc{titsiasFunctionalRegularisationContinual2020,
  title = {Functional {{Regularisation}} for {{Continual Learning}} with {{Gaussian Processes}}},
  author = {Titsias, Michalis K. and Schwarz, Jonathan and Matthews, Alexander G. de G. and Pascanu, Razvan and Teh, Yee Whye},
  year = 2020,
  month = feb,
  number = {arXiv:1901.11356},
  eprint = {1901.11356},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1901.11356},
  urldate = {2025-09-30},
  abstract = {We introduce a framework for Continual Learning (CL) based on Bayesian inference over the function space rather than the parameters of a deep neural network. This method, referred to as functional regularisation for Continual Learning, avoids forgetting a previous task by constructing and memorising an approximate posterior belief over the underlying task-specific function. To achieve this we rely on a Gaussian process obtained by treating the weights of the last layer of a neural network as random and Gaussian distributed. Then, the training algorithm sequentially encounters tasks and constructs posterior beliefs over the task-specific functions by using inducing point sparse Gaussian process methods. At each step a new task is first learnt and then a summary is constructed consisting of (i) inducing inputs -- a fixed-size subset of the task inputs selected such that it optimally represents the task -- and (ii) a posterior distribution over the function values at these inputs. This summary then regularises learning of future tasks, through Kullback-Leibler regularisation terms. Our method thus unites approaches focused on (pseudo-)rehearsal with those derived from a sequential Bayesian inference perspective in a principled way, leading to strong results on accepted benchmarks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/LV9TE2K5/Titsias et al. - 2020 - Functional Regularisation for Continual Learning with Gaussian Processes.pdf;/Users/giulialanzillotta/Zotero/storage/I8RKR9FW/1901.html}
}

@misc{titsiasFunctionalRegularisationContinual2020a,
  title = {Functional {{Regularisation}} for {{Continual Learning}} with {{Gaussian Processes}}},
  author = {Titsias, Michalis K. and Schwarz, Jonathan and Matthews, Alexander G. de G. and Pascanu, Razvan and Teh, Yee Whye},
  year = 2020,
  month = feb,
  number = {arXiv:1901.11356},
  eprint = {1901.11356},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1901.11356},
  urldate = {2025-10-14},
  abstract = {We introduce a framework for Continual Learning (CL) based on Bayesian inference over the function space rather than the parameters of a deep neural network. This method, referred to as functional regularisation for Continual Learning, avoids forgetting a previous task by constructing and memorising an approximate posterior belief over the underlying task-specific function. To achieve this we rely on a Gaussian process obtained by treating the weights of the last layer of a neural network as random and Gaussian distributed. Then, the training algorithm sequentially encounters tasks and constructs posterior beliefs over the task-specific functions by using inducing point sparse Gaussian process methods. At each step a new task is first learnt and then a summary is constructed consisting of (i) inducing inputs -- a fixed-size subset of the task inputs selected such that it optimally represents the task -- and (ii) a posterior distribution over the function values at these inputs. This summary then regularises learning of future tasks, through Kullback-Leibler regularisation terms. Our method thus unites approaches focused on (pseudo-)rehearsal with those derived from a sequential Bayesian inference perspective in a principled way, leading to strong results on accepted benchmarks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/WW2WXB6I/Titsias et al. - 2020 - Functional Regularisation for Continual Learning with Gaussian Processes.pdf;/Users/giulialanzillotta/Zotero/storage/69Y347IJ/1901.html}
}

@misc{tiwariGCRGradientCoreset2022,
  title = {{{GCR}}: {{Gradient Coreset Based Replay Buffer Selection For Continual Learning}}},
  shorttitle = {{{GCR}}},
  author = {Tiwari, Rishabh and Killamsetty, Krishnateja and Iyer, Rishabh and Shenoy, Pradeep},
  year = 2022,
  month = apr,
  number = {arXiv:2111.11210},
  eprint = {2111.11210},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2111.11210},
  urldate = {2025-09-30},
  abstract = {Continual learning (CL) aims to develop techniques by which a single model adapts to an increasing number of tasks encountered sequentially, thereby potentially leveraging learnings across tasks in a resource-efficient manner. A major challenge for CL systems is catastrophic forgetting, where earlier tasks are forgotten while learning a new task. To address this, replay-based CL approaches maintain and repeatedly retrain on a small buffer of data selected across encountered tasks. We propose Gradient Coreset Replay (GCR), a novel strategy for replay buffer selection and update using a carefully designed optimization criterion. Specifically, we select and maintain a "coreset" that closely approximates the gradient of all the data seen so far with respect to current model parameters, and discuss key strategies needed for its effective application to the continual learning setting. We show significant gains (2\%-4\% absolute) over the state-of-the-art in the well-studied offline continual learning setting. Our findings also effectively transfer to online / streaming CL settings, showing upto 5\% gains over existing approaches. Finally, we demonstrate the value of supervised contrastive loss for continual learning, which yields a cumulative gain of up to 5\% accuracy when combined with our subset selection strategy.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/NTP9P3Q6/Tiwari et al. - 2022 - GCR Gradient Coreset Based Replay Buffer Selection For Continual Learning.pdf;/Users/giulialanzillotta/Zotero/storage/P4ARAW42/2111.html}
}

@inproceedings{toldoBringEvanescentRepresentations2022,
  title = {Bring {{Evanescent Representations}} to {{Life}} in {{Lifelong Class Incremental Learning}}},
  booktitle = {2022 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Toldo, Marco and Ozay, Mete},
  year = 2022,
  month = jun,
  pages = {16711--16720},
  publisher = {IEEE},
  address = {New Orleans, LA, USA},
  doi = {10.1109/CVPR52688.2022.01623},
  urldate = {2025-09-30},
  abstract = {In Class Incremental Learning (CIL), a classification model is progressively trained at each incremental step on an evolving dataset of new classes, while at the same time, it is required to preserve knowledge of all the classes observed so far. Prototypical representations can be leveraged to model feature distribution for the past data and inject information of former classes in later incremental steps without resorting to stored exemplars. However, if not updated, those representations become increasingly outdated as the incremental learning progresses with new classes. To address the aforementioned problems, we propose a framework which aims to (i) model the semantic drift by learning the relationship between representations of past and novel classes among incremental steps, and (ii) estimate the feature drift, defined as the evolution of the representations learned by models at each incremental step. Semantic and feature drifts are then jointly exploited to infer up-to-date representations of past classes (evanescent representations), and thereby infuse past knowledge into incremental training. We experimentally evaluate our framework achieving exemplar-free SotA results on multiple benchmarks. In the ablation study, we investigate nontrivial relationships between evanescent representations and models.},
  copyright = {https://doi.org/10.15223/policy-029},
  isbn = {978-1-6654-6946-3},
  langid = {english},
  file = {/Users/giulialanzillotta/Zotero/storage/2T3FJ635/Toldo and Ozay - 2022 - Bring Evanescent Representations to Life in Lifelong Class Incremental Learning.pdf}
}

@article{torralba80MillionTiny2008,
  title = {80 Million Tiny Images: {{A}} Large Data Set for Nonparametric Object and Scene Recognition},
  author = {Torralba, Antonio and Fergus, Rob and Freeman, William T},
  year = 2008,
  journal = {IEEE transactions on pattern analysis and machine intelligence},
  volume = {30},
  number = {11},
  pages = {1958--1970},
  publisher = {IEEE}
}

@article{tseranNaturalVariationalContinual,
  title = {Natural {{Variational Continual Learning}}},
  author = {Tseran, Hanna and Khan, Mohammad Emtiyaz and Harada, Tatsuya and Bui, Thang D},
  abstract = {The goal of continual learning is to sequentially learn new skills without forgetting old ones. Recent continual-learning approaches have employed gradient-based approximate Bayesian inference methods to derive such algorithms. In this paper, we propose a natural-gradient method that unifies two recent approaches based on Laplace approximation and variational inference, respectively. Our method enables a plug-and-play implementation where the accuracy of the approximation can be traded off for the ease of implementation. Our method also enables a principled application of approximate Bayesian inference for continual learning, and gives competitive performance to the existing approaches.},
  langid = {english},
  file = {/Users/giulialanzillotta/Zotero/storage/4MJ2UDTK/Tseran et al. - Natural Variational Continual Learning.pdf}
}

@inproceedings{tseranNaturalVariationalContinual2018,
  title = {Natural Variational Continual Learning},
  booktitle = {Continual {{Learning Workshop}}@ {{NeurIPS}}},
  author = {Tseran, Hanna and Khan, Mohammad Emtiyaz and Harada, Tatsuya and Bui, Thang D.},
  year = 2018,
  volume = {2},
  urldate = {2025-09-30},
  file = {/Users/giulialanzillotta/Zotero/storage/BT97KMUK/Tseran et al. - 2018 - Natural variational continual learning.pdf}
}

@article{tseranNaturalVariationalContinuala,
  title = {Natural {{Variational Continual Learning}}},
  author = {Tseran, Hanna and Khan, Mohammad Emtiyaz and Harada, Tatsuya and Bui, Thang D},
  abstract = {The goal of continual learning is to sequentially learn new skills without forgetting old ones. Recent continual-learning approaches have employed gradient-based approximate Bayesian inference methods to derive such algorithms. In this paper, we propose a natural-gradient method that unifies two recent approaches based on Laplace approximation and variational inference, respectively. Our method enables a plug-and-play implementation where the accuracy of the approximation can be traded off for the ease of implementation. Our method also enables a principled application of approximate Bayesian inference for continual learning, and gives competitive performance to the existing approaches.},
  langid = {english},
  file = {/Users/giulialanzillotta/Zotero/storage/AGVXQ6VP/Tseran et al. - Natural Variational Continual Learning.pdf}
}

@misc{tsilivisFlavorsMarginImplicit2025,
  title = {Flavors of {{Margin}}: {{Implicit Bias}} of {{Steepest Descent}} in {{Homogeneous Neural Networks}}},
  shorttitle = {Flavors of {{Margin}}},
  author = {Tsilivis, Nikolaos and Vardi, Gal and Kempe, Julia},
  year = 2025,
  month = apr,
  number = {arXiv:2410.22069},
  eprint = {2410.22069},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2410.22069},
  urldate = {2025-09-15},
  abstract = {We study the implicit bias of the general family of steepest descent algorithms with infinitesimal learning rate in deep homogeneous neural networks. We show that: (a) an algorithm-dependent geometric margin starts increasing once the networks reach perfect training accuracy, and (b) any limit point of the training trajectory corresponds to a KKT point of the corresponding margin-maximization problem. We experimentally zoom into the trajectories of neural networks optimized with various steepest descent algorithms, highlighting connections to the implicit bias of popular adaptive methods (Adam and Shampoo).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/IYD2TFZX/Tsilivis et al. - 2025 - Flavors of Margin Implicit Bias of Steepest Descent in Homogeneous Neural Networks.pdf;/Users/giulialanzillotta/Zotero/storage/R5PW76ER/2410.html}
}

@misc{tsilivisFlavorsMarginImplicit2025a,
  title = {Flavors of {{Margin}}: {{Implicit Bias}} of {{Steepest Descent}} in {{Homogeneous Neural Networks}}},
  shorttitle = {Flavors of {{Margin}}},
  author = {Tsilivis, Nikolaos and Vardi, Gal and Kempe, Julia},
  year = 2025,
  month = apr,
  number = {arXiv:2410.22069},
  eprint = {2410.22069},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2410.22069},
  urldate = {2025-09-16},
  abstract = {We study the implicit bias of the general family of steepest descent algorithms with infinitesimal learning rate in deep homogeneous neural networks. We show that: (a) an algorithm-dependent geometric margin starts increasing once the networks reach perfect training accuracy, and (b) any limit point of the training trajectory corresponds to a KKT point of the corresponding margin-maximization problem. We experimentally zoom into the trajectories of neural networks optimized with various steepest descent algorithms, highlighting connections to the implicit bias of popular adaptive methods (Adam and Shampoo).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/EL5RZNHG/Tsilivis et al. - 2025 - Flavors of Margin Implicit Bias of Steepest Descent in Homogeneous Neural Networks.pdf;/Users/giulialanzillotta/Zotero/storage/8W5TTV2V/2410.html}
}

@article{vandevenGenerativeReplayFeedback2018,
  title = {Generative Replay with Feedback Connections as a General Strategy for Continual Learning},
  author = {{Van de Ven}, Gido M and Tolias, Andreas S},
  year = 2018,
  journal = {arXiv preprint arXiv:1809.10635},
  eprint = {1809.10635},
  archiveprefix = {arXiv}
}

@article{vandevenThreeTypesIncremental2022,
  title = {Three Types of Incremental Learning},
  author = {{van de Ven}, Gido M and Tuytelaars, Tinne and Tolias, Andreas S},
  year = 2022,
  journal = {Nature Machine Intelligence},
  volume = {4},
  pages = {1185--1197},
  publisher = {Nature Publishing Group},
  doi = {10.1038/s42256-022-00568-3}
}

@article{vapnikPrinciplesRiskMinimization1991,
  title = {Principles of Risk Minimization for Learning Theory},
  author = {Vapnik, Vladimir},
  year = 1991,
  journal = {Advances in neural information processing systems},
  volume = {4}
}

@misc{veniatEfficientContinualLearning2021,
  title = {Efficient {{Continual Learning}} with {{Modular Networks}} and {{Task-Driven Priors}}},
  author = {Veniat, Tom and Denoyer, Ludovic and Ranzato, Marc'Aurelio},
  year = 2021,
  month = feb,
  number = {arXiv:2012.12631},
  eprint = {2012.12631},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2012.12631},
  urldate = {2025-10-01},
  abstract = {Existing literature in Continual Learning (CL) has focused on overcoming catastrophic forgetting, the inability of the learner to recall how to perform tasks observed in the past. There are however other desirable properties of a CL system, such as the ability to transfer knowledge from previous tasks and to scale memory and compute sub-linearly with the number of tasks. Since most current benchmarks focus only on forgetting using short streams of tasks, we first propose a new suite of benchmarks to probe CL algorithms across these new axes. Finally, we introduce a new modular architecture, whose modules represent atomic skills that can be composed to perform a certain task. Learning a task reduces to figuring out which past modules to re-use, and which new modules to instantiate to solve the current task. Our learning algorithm leverages a task-driven prior over the exponential search space of all possible ways to combine modules, enabling efficient learning on long streams of tasks. Our experiments show that this modular architecture and learning algorithm perform competitively on widely used CL benchmarks while yielding superior performance on the more challenging benchmarks we introduce in this work.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/SW3NRVWK/Veniat et al. - 2021 - Efficient Continual Learning with Modular Networks and Task-Driven Priors.pdf;/Users/giulialanzillotta/Zotero/storage/NJDMIX98/2012.html}
}

@article{verwimpContinualLearningApplications2023,
  title = {Continual Learning: {{Applications}} and the Road Forward},
  author = {Verwimp, Eli and {Ben-David}, Shai and Bethge, Matthias and Cossu, Andrea and Gepperth, Alexander and Hayes, Tyler L and H{\"u}llermeier, Eyke and Kanan, Christopher and Kudithipudi, Dhireesha and Lampert, Christoph H and others},
  year = 2023,
  journal = {arXiv preprint arXiv:2311.11908},
  eprint = {2311.11908},
  archiveprefix = {arXiv}
}

@article{verwimpMaintainingDiscriminationFairness2019,
  title = {Maintaining {{Discrimination}} and {{Fairness}} in {{Class Incremental Learning}}},
  author = {Verwimp, Eli and {Ben-David}, Shai and Bethge, Matthias and Cossu, Andrea and Gepperth, Alexander and Hayes, Tyler L and H{\"u}llermeier, Eyke and Kanan, Christopher and Kudithipudi, Dhireesha and Lampert, Christoph H and others and Khodak, Mikhail and Balcan, Maria-Florina F and Talwalkar, Ameet S and Mokhtari, Aryan and Shahrampour, Shahin and Jadbabaie, Ali and Ribeiro, Alejandro and Buzzega, Pietro and Boschini, Matteo and Porrello, Angelo and Calderara, Simone and Riemer, Matthew and Cases, Ignacio and Ajemian, Robert and Liu, Miao and Rish, Irina and Tu, Yuhai and Tesauro, Gerald and Chaudhry, Arslan and Rohrbach, Marcus and Elhoseiny, Mohamed and Ajanthan, Thalaiyasingam and Dokania, Puneet K and Torr, Philip HS and Ranzato, Marc'Aurelio and Aljundi, Rahaf and Lin, Min and Goujaud, Baptiste and Bengio, Yoshua and Aljundi, Rahaf and Belilovsky, Eugene and Tuytelaars, Tinne and Charlin, Laurent and Caccia, Massimo and Lin, Min and {Page-Caccia}, Lucas and Riemer, Matthew and Klinger, Tim and Bouneffouf, Djallel and Franceschini, Michele and Yoon, Jaehong and Madaan, Divyam and Yang, Eunho and Hwang, Sung Ju and Hacohen, Guy and Tuytelaars, Tinne and Papyan, Vardan and Han, {\relax XY} and Donoho, David L and Kothapalli, Vignesh and Zhao, Bowen and Xiao, Xi and Gan, Guojun and Zhang, Bin and Xia, Shutao},
  year = 2019,
  journal = {Proceedings of the AAAI conference on artificial intelligence},
  volume = {117},
  number = {40},
  pages = {24652--24663},
  publisher = {National Acad Sciences},
  annotation = {Backup Publisher: IEEE}
}

@misc{ViewArticle,
  title = {View Article},
  urldate = {2025-10-13},
  howpublished = {https://scholar.google.com/citations?view\_op=view\_citation\&hl=en\&user=Avse5gIAAAAJ\&sortby=pubdate\&citation\_for\_view=Avse5gIAAAAJ:zdjWy\_NXXwUC},
  file = {/Users/giulialanzillotta/Zotero/storage/M73KIGAF/citations.html}
}

@article{vyasFeatureLearningNetworksAre,
  title = {Feature-{{Learning Networks Are Consistent Across Widths At Realistic Scales}}},
  author = {Vyas, Nikhil and Atanasov, Alexander and Bordelon, Blake and Morwani, Depen and Sainathan, Sabarish and Pehlevan, Cengiz},
  abstract = {We study the effect of width on the dynamics of feature-learning neural networks across a variety of architectures and datasets. Early in training, wide neural networks trained on online data have not only identical loss curves but also agree in their point-wise test predictions throughout training. For simple tasks such as CIFAR-5m this holds throughout training for networks of realistic widths. We also show that structural properties of the models, including internal representations, preactivation distributions, edge of stability phenomena, and large learning rate effects are consistent across large widths. This motivates the hypothesis that phenomena seen in realistic models can be captured by infinite-width, feature-learning limits. For harder tasks (such as ImageNet and language modeling), and later training times, finite-width deviations grow systematically. Two distinct effects cause these deviations across widths. First, the network output has an initialization-dependent variance scaling inversely with width, which can be removed by ensembling networks. We observe, however, that ensembles of narrower networks perform worse than a single wide network. We call this the bias of narrower width. We conclude with a spectral perspective on the origin of this finite-width bias.},
  langid = {english},
  file = {/Users/giulialanzillotta/Zotero/storage/GI35DH9E/Vyas et al. - Feature-Learning Networks Are Consistent Across Widths At Realistic Scales.pdf}
}

@article{wahCaltechucsdBirds2002011Dataset2011,
  title = {The Caltech-Ucsd Birds-200-2011 Dataset},
  author = {Wah, Catherine and Branson, Steve and Welinder, Peter and Perona, Pietro and Belongie, Serge},
  year = 2011,
  publisher = {California Institute of Technology}
}

@misc{wangComprehensiveSurveyContinual2024,
  title = {A {{Comprehensive Survey}} of {{Continual Learning}}: {{Theory}}, {{Method}} and {{Application}}},
  shorttitle = {A {{Comprehensive Survey}} of {{Continual Learning}}},
  author = {Wang, Liyuan and Zhang, Xingxing and Su, Hang and Zhu, Jun},
  year = 2024,
  month = feb,
  number = {arXiv:2302.00487},
  eprint = {2302.00487},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2302.00487},
  urldate = {2025-09-30},
  abstract = {To cope with real-world dynamics, an intelligent system needs to incrementally acquire, update, accumulate, and exploit knowledge throughout its lifetime. This ability, known as continual learning, provides a foundation for AI systems to develop themselves adaptively. In a general sense, continual learning is explicitly limited by catastrophic forgetting, where learning a new task usually results in a dramatic performance degradation of the old tasks. Beyond this, increasingly numerous advances have emerged in recent years that largely extend the understanding and application of continual learning. The growing and widespread interest in this direction demonstrates its realistic significance as well as complexity. In this work, we present a comprehensive survey of continual learning, seeking to bridge the basic settings, theoretical foundations, representative methods, and practical applications. Based on existing theoretical and empirical results, we summarize the general objectives of continual learning as ensuring a proper stability-plasticity trade-off and an adequate intra/inter-task generalizability in the context of resource efficiency. Then we provide a state-of-the-art and elaborated taxonomy, extensively analyzing how representative methods address continual learning, and how they are adapted to particular challenges in realistic applications. Through an in-depth discussion of promising directions, we believe that such a holistic perspective can greatly facilitate subsequent exploration in this field and beyond.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/98WCH6XS/Wang et al. - 2024 - A Comprehensive Survey of Continual Learning Theory, Method and Application.pdf;/Users/giulialanzillotta/Zotero/storage/I4FJ4YC5/2302.html}
}

@misc{wangComprehensiveSurveyContinual2024a,
  title = {A {{Comprehensive Survey}} of {{Continual Learning}}: {{Theory}}, {{Method}} and {{Application}}},
  shorttitle = {A {{Comprehensive Survey}} of {{Continual Learning}}},
  author = {Wang, Liyuan and Zhang, Xingxing and Su, Hang and Zhu, Jun},
  year = 2024,
  month = feb,
  number = {arXiv:2302.00487},
  eprint = {2302.00487},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2302.00487},
  urldate = {2025-09-30},
  abstract = {To cope with real-world dynamics, an intelligent system needs to incrementally acquire, update, accumulate, and exploit knowledge throughout its lifetime. This ability, known as continual learning, provides a foundation for AI systems to develop themselves adaptively. In a general sense, continual learning is explicitly limited by catastrophic forgetting, where learning a new task usually results in a dramatic performance degradation of the old tasks. Beyond this, increasingly numerous advances have emerged in recent years that largely extend the understanding and application of continual learning. The growing and widespread interest in this direction demonstrates its realistic significance as well as complexity. In this work, we present a comprehensive survey of continual learning, seeking to bridge the basic settings, theoretical foundations, representative methods, and practical applications. Based on existing theoretical and empirical results, we summarize the general objectives of continual learning as ensuring a proper stability-plasticity trade-off and an adequate intra/inter-task generalizability in the context of resource efficiency. Then we provide a state-of-the-art and elaborated taxonomy, extensively analyzing how representative methods address continual learning, and how they are adapted to particular challenges in realistic applications. Through an in-depth discussion of promising directions, we believe that such a holistic perspective can greatly facilitate subsequent exploration in this field and beyond.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/Y5V34HIN/Wang et al. - 2024 - A Comprehensive Survey of Continual Learning Theory, Method and Application.pdf;/Users/giulialanzillotta/Zotero/storage/PVP3GR5W/2302.html}
}

@misc{wangDualPromptComplementaryPrompting2022,
  title = {{{DualPrompt}}: {{Complementary Prompting}} for {{Rehearsal-free Continual Learning}}},
  shorttitle = {{{DualPrompt}}},
  author = {Wang, Zifeng and Zhang, Zizhao and Ebrahimi, Sayna and Sun, Ruoxi and Zhang, Han and Lee, Chen-Yu and Ren, Xiaoqi and Su, Guolong and Perot, Vincent and Dy, Jennifer and Pfister, Tomas},
  year = 2022,
  month = aug,
  number = {arXiv:2204.04799},
  eprint = {2204.04799},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2204.04799},
  urldate = {2025-10-03},
  abstract = {Continual learning aims to enable a single model to learn a sequence of tasks without catastrophic forgetting. Top-performing methods usually require a rehearsal buffer to store past pristine examples for experience replay, which, however, limits their practical value due to privacy and memory constraints. In this work, we present a simple yet effective framework, DualPrompt, which learns a tiny set of parameters, called prompts, to properly instruct a pre-trained model to learn tasks arriving sequentially without buffering past examples. DualPrompt presents a novel approach to attach complementary prompts to the pre-trained backbone, and then formulates the objective as learning task-invariant and task-specific "instructions". With extensive experimental validation, DualPrompt consistently sets state-of-the-art performance under the challenging class-incremental setting. In particular, DualPrompt outperforms recent advanced continual learning methods with relatively large buffer sizes. We also introduce a more challenging benchmark, Split ImageNet-R, to help generalize rehearsal-free continual learning research. Source code is available at https://github.com/google-research/l2p.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/4J85454Z/Wang et al. - 2022 - DualPrompt Complementary Prompting for Rehearsal-free Continual Learning.pdf;/Users/giulialanzillotta/Zotero/storage/8YEM4L6X/2204.html}
}

@misc{wangExperienceReplayAddresses2025,
  title = {Experience {{Replay Addresses Loss}} of {{Plasticity}} in {{Continual Learning}}},
  author = {Wang, Jiuqi and Chandra, Rohan and Zhang, Shangtong},
  year = 2025,
  month = mar,
  number = {arXiv:2503.20018},
  eprint = {2503.20018},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2503.20018},
  urldate = {2025-10-13},
  abstract = {Loss of plasticity is one of the main challenges in continual learning with deep neural networks, where neural networks trained via backpropagation gradually lose their ability to adapt to new tasks and perform significantly worse than their freshly initialized counterparts. The main contribution of this paper is to propose a new hypothesis that experience replay addresses the loss of plasticity in continual learning. Here, experience replay is a form of memory. We provide supporting evidence for this hypothesis. In particular, we demonstrate in multiple different tasks, including regression, classification, and policy evaluation, that by simply adding an experience replay and processing the data in the experience replay with Transformers, the loss of plasticity disappears. Notably, we do not alter any standard components of deep learning. For example, we do not change backpropagation. We do not modify the activation functions. And we do not use any regularization. We conjecture that experience replay and Transformers can address the loss of plasticity because of the in-context learning phenomenon.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/giulialanzillotta/Zotero/storage/MEIGA8M5/Wang et al. - 2025 - Experience Replay Addresses Loss of Plasticity in Continual Learning.pdf;/Users/giulialanzillotta/Zotero/storage/X826ZHS5/2503.html}
}

@misc{wangHierarchicalDecompositionPromptBased2023,
  title = {Hierarchical {{Decomposition}} of {{Prompt-Based Continual Learning}}: {{Rethinking Obscured Sub-optimality}}},
  shorttitle = {Hierarchical {{Decomposition}} of {{Prompt-Based Continual Learning}}},
  author = {Wang, Liyuan and Xie, Jingyi and Zhang, Xingxing and Huang, Mingyi and Su, Hang and Zhu, Jun},
  year = 2023,
  month = oct,
  number = {arXiv:2310.07234},
  eprint = {2310.07234},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2310.07234},
  urldate = {2025-10-03},
  abstract = {Prompt-based continual learning is an emerging direction in leveraging pre-trained knowledge for downstream continual learning, and has almost reached the performance pinnacle under supervised pre-training. However, our empirical research reveals that the current strategies fall short of their full potential under the more realistic self-supervised pre-training, which is essential for handling vast quantities of unlabeled data in practice. This is largely due to the difficulty of task-specific knowledge being incorporated into instructed representations via prompt parameters and predicted by uninstructed representations at test time. To overcome the exposed sub-optimality, we conduct a theoretical analysis of the continual learning objective in the context of pre-training, and decompose it into hierarchical components: within-task prediction, task-identity inference, and task-adaptive prediction. Following these empirical and theoretical insights, we propose Hierarchical Decomposition (HiDe-)Prompt, an innovative approach that explicitly optimizes the hierarchical components with an ensemble of task-specific prompts and statistics of both uninstructed and instructed representations, further with the coordination of a contrastive regularization strategy. Our extensive experiments demonstrate the superior performance of HiDe-Prompt and its robustness to pre-training paradigms in continual learning (e.g., up to 15.01\% and 9.61\% lead on Split CIFAR-100 and Split ImageNet-R, respectively). Our code is available at {\textbackslash}url\{https://github.com/thu-ml/HiDe-Prompt\}.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/3PEEBULT/Wang et al. - 2023 - Hierarchical Decomposition of Prompt-Based Continual Learning Rethinking Obscured Sub-optimality.pdf;/Users/giulialanzillotta/Zotero/storage/VVHGJ33G/2310.html}
}

@misc{wangLearningPromptContinual2022,
  title = {Learning to {{Prompt}} for {{Continual Learning}}},
  author = {Wang, Zifeng and Zhang, Zizhao and Lee, Chen-Yu and Zhang, Han and Sun, Ruoxi and Ren, Xiaoqi and Su, Guolong and Perot, Vincent and Dy, Jennifer and Pfister, Tomas},
  year = 2022,
  month = mar,
  number = {arXiv:2112.08654},
  eprint = {2112.08654},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2112.08654},
  urldate = {2025-10-03},
  abstract = {The mainstream paradigm behind continual learning has been to adapt the model parameters to non-stationary data distributions, where catastrophic forgetting is the central challenge. Typical methods rely on a rehearsal buffer or known task identity at test time to retrieve learned knowledge and address forgetting, while this work presents a new paradigm for continual learning that aims to train a more succinct memory system without accessing task identity at test time. Our method learns to dynamically prompt (L2P) a pre-trained model to learn tasks sequentially under different task transitions. In our proposed framework, prompts are small learnable parameters, which are maintained in a memory space. The objective is to optimize prompts to instruct the model prediction and explicitly manage task-invariant and task-specific knowledge while maintaining model plasticity. We conduct comprehensive experiments under popular image classification benchmarks with different challenging continual learning settings, where L2P consistently outperforms prior state-of-the-art methods. Surprisingly, L2P achieves competitive results against rehearsal-based methods even without a rehearsal buffer and is directly applicable to challenging task-agnostic continual learning. Source code is available at https://github.com/google-research/l2p.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/NRJRPLQ7/Wang et al. - 2022 - Learning to Prompt for Continual Learning.pdf;/Users/giulialanzillotta/Zotero/storage/W8898AJB/2112.html}
}

@misc{wangRethinkingContinualLearning2025,
  title = {Rethinking {{Continual Learning}} with {{Progressive Neural Collapse}}},
  author = {Wang, Zheng and Yu, Wanhao and Yang, Li and Lin, Sen},
  year = 2025,
  month = may,
  number = {arXiv:2505.24254},
  eprint = {2505.24254},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2505.24254},
  urldate = {2025-09-16},
  abstract = {Continual Learning (CL) seeks to build an agent that can continuously learn a sequence of tasks, where a key challenge, namely Catastrophic Forgetting, persists due to the potential knowledge interference among different tasks. On the other hand, deep neural networks (DNNs) are shown to converge to a terminal state termed Neural Collapse during training, where all class prototypes geometrically form a static simplex equiangular tight frame (ETF). These maximally and equally separated class prototypes make the ETF an ideal target for model learning in CL to mitigate knowledge interference. Thus inspired, several studies have emerged very recently to leverage a fixed global ETF in CL, which however suffers from key drawbacks, such as impracticability and limited performance.To address these challenges and fully unlock the potential of ETF in CL, we propose Progressive Neural Collapse (ProNC), a novel framework that completely removes the need of a fixed global ETF in CL. Specifically, ProNC progressively expands the ETF target in a principled way by adding new class prototypes as vertices for new tasks, ensuring maximal separability across all encountered classes with minimal shifts from the previous ETF. We next develop a new CL framework by plugging ProNC into commonly used CL algorithm designs, where distillation is further leveraged to balance between target shifting for old classes and target aligning for new classes. Extensive experiments show that our approach significantly outperforms related baselines while maintaining superior flexibility, simplicity, and efficiency.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/RVN8JU4N/Wang et al. - 2025 - Rethinking Continual Learning with Progressive Neural Collapse.pdf;/Users/giulialanzillotta/Zotero/storage/68AMSBBN/2505.html}
}

@article{wengerDisconnectTheoryPractice2023,
  title = {On the {{Disconnect Between Theory}} and {{Practice}} of {{Overparametrized Neural Networks}}},
  author = {Wenger, Jonathan and Dangel, Felix and Kristiadi, Agustinus},
  year = 2023,
  journal = {arXiv preprint arXiv:2310.00137},
  eprint = {2310.00137},
  archiveprefix = {arXiv}
}

@article{wojtowytschEmergenceSimplexSymmetry2020,
  title = {On the Emergence of Simplex Symmetry in the Final and Penultimate Layers of Neural Network Classifiers},
  author = {Wojtowytsch, Stephan and others},
  year = 2020,
  journal = {arXiv preprint arXiv:2012.05420},
  eprint = {2012.05420},
  archiveprefix = {arXiv}
}

@article{wolczykContinualWorldRobotic2021,
  title = {Continual World: {{A}} Robotic Benchmark for Continual Reinforcement Learning},
  author = {Wo{\l}czyk, Maciej and Zaj{\k a}c, Micha{\l} and Pascanu, Razvan and Kuci{\'n}ski, {\L}ukasz and Mi{\l}o{\'s}, Piotr},
  year = 2021,
  journal = {Advances in Neural Information Processing Systems},
  volume = {34},
  pages = {28496--28510}
}

@misc{wortsmanSupermasksSuperposition2020,
  title = {Supermasks in {{Superposition}}},
  author = {Wortsman, Mitchell and Ramanujan, Vivek and Liu, Rosanne and Kembhavi, Aniruddha and Rastegari, Mohammad and Yosinski, Jason and Farhadi, Ali},
  year = 2020,
  month = oct,
  number = {arXiv:2006.14769},
  eprint = {2006.14769},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2006.14769},
  urldate = {2025-10-01},
  abstract = {We present the Supermasks in Superposition (SupSup) model, capable of sequentially learning thousands of tasks without catastrophic forgetting. Our approach uses a randomly initialized, fixed base network and for each task finds a subnetwork (supermask) that achieves good performance. If task identity is given at test time, the correct subnetwork can be retrieved with minimal memory usage. If not provided, SupSup can infer the task using gradient-based optimization to find a linear superposition of learned supermasks which minimizes the output entropy. In practice we find that a single gradient step is often sufficient to identify the correct mask, even among 2500 tasks. We also showcase two promising extensions. First, SupSup models can be trained entirely without task identity information, as they may detect when they are uncertain about new data and allocate an additional supermask for the new training distribution. Finally the entire, growing set of supermasks can be stored in a constant-sized reservoir by implicitly storing them as attractors in a fixed-sized Hopfield network.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/UB4C589X/Wortsman et al. - 2020 - Supermasks in Superposition.pdf;/Users/giulialanzillotta/Zotero/storage/368GVHTL/2006.html}
}

@inproceedings{wuLargeScaleIncremental2019,
  title = {Large Scale Incremental Learning},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF}} Conference on Computer Vision and Pattern Recognition},
  author = {Wu, Yue and Chen, Yinpeng and Wang, Lijuan and Ye, Yuancheng and Liu, Zicheng and Guo, Yandong and Fu, Yun},
  year = 2019,
  pages = {374--382}
}

@misc{wuLargeScaleIncremental2019a,
  title = {Large {{Scale Incremental Learning}}},
  author = {Wu, Yue and Chen, Yinpeng and Wang, Lijuan and Ye, Yuancheng and Liu, Zicheng and Guo, Yandong and Fu, Yun},
  year = 2019,
  month = may,
  number = {arXiv:1905.13260},
  eprint = {1905.13260},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1905.13260},
  urldate = {2025-10-01},
  abstract = {Modern machine learning suffers from catastrophic forgetting when learning new classes incrementally. The performance dramatically degrades due to the missing data of old classes. Incremental learning methods have been proposed to retain the knowledge acquired from the old classes, by using knowledge distilling and keeping a few exemplars from the old classes. However, these methods struggle to scale up to a large number of classes. We believe this is because of the combination of two factors: (a) the data imbalance between the old and new classes, and (b) the increasing number of visually similar classes. Distinguishing between an increasing number of visually similar classes is particularly challenging, when the training data is unbalanced. We propose a simple and effective method to address this data imbalance issue. We found that the last fully connected layer has a strong bias towards the new classes, and this bias can be corrected by a linear model. With two bias parameters, our method performs remarkably well on two large datasets: ImageNet (1000 classes) and MS-Celeb-1M (10000 classes), outperforming the state-of-the-art algorithms by 11.1\% and 13.2\% respectively.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/giulialanzillotta/Zotero/storage/GXY29CN5/Wu et al. - 2019 - Large Scale Incremental Learning.pdf;/Users/giulialanzillotta/Zotero/storage/NZ48CUIU/1905.html}
}

@misc{wuLargeScaleIncremental2019b,
  title = {Large {{Scale Incremental Learning}}},
  author = {Wu, Yue and Chen, Yinpeng and Wang, Lijuan and Ye, Yuancheng and Liu, Zicheng and Guo, Yandong and Fu, Yun},
  year = 2019,
  month = may,
  number = {arXiv:1905.13260},
  eprint = {1905.13260},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1905.13260},
  urldate = {2025-10-01},
  abstract = {Modern machine learning suffers from catastrophic forgetting when learning new classes incrementally. The performance dramatically degrades due to the missing data of old classes. Incremental learning methods have been proposed to retain the knowledge acquired from the old classes, by using knowledge distilling and keeping a few exemplars from the old classes. However, these methods struggle to scale up to a large number of classes. We believe this is because of the combination of two factors: (a) the data imbalance between the old and new classes, and (b) the increasing number of visually similar classes. Distinguishing between an increasing number of visually similar classes is particularly challenging, when the training data is unbalanced. We propose a simple and effective method to address this data imbalance issue. We found that the last fully connected layer has a strong bias towards the new classes, and this bias can be corrected by a linear model. With two bias parameters, our method performs remarkably well on two large datasets: ImageNet (1000 classes) and MS-Celeb-1M (10000 classes), outperforming the state-of-the-art algorithms by 11.1\% and 13.2\% respectively.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/giulialanzillotta/Zotero/storage/I2NL4TRS/Wu et al. - 2019 - Large Scale Incremental Learning.pdf;/Users/giulialanzillotta/Zotero/storage/9YCU5C5K/1905.html}
}

@misc{wuLinguisticCollapseNeural2024,
  title = {Linguistic {{Collapse}}: {{Neural Collapse}} in ({{Large}}) {{Language Models}}},
  shorttitle = {Linguistic {{Collapse}}},
  author = {Wu, Robert and Papyan, Vardan},
  year = 2024,
  month = nov,
  number = {arXiv:2405.17767},
  eprint = {2405.17767},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2405.17767},
  urldate = {2025-09-15},
  abstract = {Neural collapse (\${\textbackslash}mathcal\{NC\}\$) is a phenomenon observed in classification tasks where top-layer representations collapse into their class means, which become equinorm, equiangular and aligned with the classifiers. These behaviors -- associated with generalization and robustness -- would manifest under specific conditions: models are trained towards zero loss, with noise-free labels belonging to balanced classes, which do not outnumber the model's hidden dimension. Recent studies have explored \${\textbackslash}mathcal\{NC\}\$ in the absence of one or more of these conditions to extend and capitalize on the associated benefits of ideal geometries. Language modeling presents a curious frontier, as {\textbackslash}textit\{training by token prediction\} constitutes a classification task where none of the conditions exist: the vocabulary is imbalanced and exceeds the embedding dimension; different tokens might correspond to similar contextual embeddings; and large language models (LLMs) in particular are typically only trained for a few epochs. This paper empirically investigates the impact of scaling the architectures and training of causal language models (CLMs) on their progression towards \${\textbackslash}mathcal\{NC\}\$. We find that \${\textbackslash}mathcal\{NC\}\$ properties that develop with scale (and regularization) are linked to generalization. Moreover, there is evidence of some relationship between \${\textbackslash}mathcal\{NC\}\$ and generalization independent of scale. Our work thereby underscores the generality of \${\textbackslash}mathcal\{NC\}\$ as it extends to the novel and more challenging setting of language modeling. Downstream, we seek to inspire further research on the phenomenon to deepen our understanding of LLMs -- and neural networks at large -- and improve existing architectures based on \${\textbackslash}mathcal\{NC\}\$-related properties. Our code is hosted on GitHub at https://github.com/rhubarbwu/linguistic-collapse .},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/PWGDCXT5/Wu and Papyan - 2024 - Linguistic Collapse Neural Collapse in (Large) Language Models.pdf;/Users/giulialanzillotta/Zotero/storage/RY8E37J8/2405.html}
}

@misc{wuLinguisticCollapseNeural2024a,
  title = {Linguistic {{Collapse}}: {{Neural Collapse}} in ({{Large}}) {{Language Models}}},
  shorttitle = {Linguistic {{Collapse}}},
  author = {Wu, Robert and Papyan, Vardan},
  year = 2024,
  month = nov,
  number = {arXiv:2405.17767},
  eprint = {2405.17767},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2405.17767},
  urldate = {2025-09-15},
  abstract = {Neural collapse (\${\textbackslash}mathcal\{NC\}\$) is a phenomenon observed in classification tasks where top-layer representations collapse into their class means, which become equinorm, equiangular and aligned with the classifiers. These behaviors -- associated with generalization and robustness -- would manifest under specific conditions: models are trained towards zero loss, with noise-free labels belonging to balanced classes, which do not outnumber the model's hidden dimension. Recent studies have explored \${\textbackslash}mathcal\{NC\}\$ in the absence of one or more of these conditions to extend and capitalize on the associated benefits of ideal geometries. Language modeling presents a curious frontier, as {\textbackslash}textit\{training by token prediction\} constitutes a classification task where none of the conditions exist: the vocabulary is imbalanced and exceeds the embedding dimension; different tokens might correspond to similar contextual embeddings; and large language models (LLMs) in particular are typically only trained for a few epochs. This paper empirically investigates the impact of scaling the architectures and training of causal language models (CLMs) on their progression towards \${\textbackslash}mathcal\{NC\}\$. We find that \${\textbackslash}mathcal\{NC\}\$ properties that develop with scale (and regularization) are linked to generalization. Moreover, there is evidence of some relationship between \${\textbackslash}mathcal\{NC\}\$ and generalization independent of scale. Our work thereby underscores the generality of \${\textbackslash}mathcal\{NC\}\$ as it extends to the novel and more challenging setting of language modeling. Downstream, we seek to inspire further research on the phenomenon to deepen our understanding of LLMs -- and neural networks at large -- and improve existing architectures based on \${\textbackslash}mathcal\{NC\}\$-related properties. Our code is hosted on GitHub at https://github.com/rhubarbwu/linguistic-collapse .},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/GMNA8ELY/Wu and Papyan - 2024 - Linguistic Collapse Neural Collapse in (Large) Language Models.pdf;/Users/giulialanzillotta/Zotero/storage/MPMF4LLR/2405.html}
}

@misc{wuLinguisticCollapseNeural2024b,
  title = {Linguistic {{Collapse}}: {{Neural Collapse}} in ({{Large}}) {{Language Models}}},
  shorttitle = {Linguistic {{Collapse}}},
  author = {Wu, Robert and Papyan, Vardan},
  year = 2024,
  month = nov,
  number = {arXiv:2405.17767},
  eprint = {2405.17767},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2405.17767},
  urldate = {2025-09-16},
  abstract = {Neural collapse (\${\textbackslash}mathcal\{NC\}\$) is a phenomenon observed in classification tasks where top-layer representations collapse into their class means, which become equinorm, equiangular and aligned with the classifiers. These behaviors -- associated with generalization and robustness -- would manifest under specific conditions: models are trained towards zero loss, with noise-free labels belonging to balanced classes, which do not outnumber the model's hidden dimension. Recent studies have explored \${\textbackslash}mathcal\{NC\}\$ in the absence of one or more of these conditions to extend and capitalize on the associated benefits of ideal geometries. Language modeling presents a curious frontier, as {\textbackslash}textit\{training by token prediction\} constitutes a classification task where none of the conditions exist: the vocabulary is imbalanced and exceeds the embedding dimension; different tokens might correspond to similar contextual embeddings; and large language models (LLMs) in particular are typically only trained for a few epochs. This paper empirically investigates the impact of scaling the architectures and training of causal language models (CLMs) on their progression towards \${\textbackslash}mathcal\{NC\}\$. We find that \${\textbackslash}mathcal\{NC\}\$ properties that develop with scale (and regularization) are linked to generalization. Moreover, there is evidence of some relationship between \${\textbackslash}mathcal\{NC\}\$ and generalization independent of scale. Our work thereby underscores the generality of \${\textbackslash}mathcal\{NC\}\$ as it extends to the novel and more challenging setting of language modeling. Downstream, we seek to inspire further research on the phenomenon to deepen our understanding of LLMs -- and neural networks at large -- and improve existing architectures based on \${\textbackslash}mathcal\{NC\}\$-related properties. Our code is hosted on GitHub at https://github.com/rhubarbwu/linguistic-collapse .},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/RD4GIQYE/Wu and Papyan - 2024 - Linguistic Collapse Neural Collapse in (Large) Language Models.pdf;/Users/giulialanzillotta/Zotero/storage/TSTDBWWZ/2405.html}
}

@article{wuLinguisticCollapseNeural2025,
  title = {Linguistic Collapse: {{Neural}} Collapse in (Large) Language Models},
  author = {Wu, Robert and Papyan, Vardan},
  year = 2025,
  journal = {Advances in Neural Information Processing Systems},
  volume = {37},
  pages = {137432--137473}
}

@article{wuMeasuringRegularizingNetworks2019,
  title = {Measuring and Regularizing Networks in Function Space},
  author = {Wu, Robert and Papyan, Vardan and Ramasesh, Vinay V and Dyer, Ethan and Raghu, Maithra and Murata, Kengo and Toyota, Tetsuya and Ohara, Kouzou and Hess, Timm and Verwimp, Eli and {van de Ven}, Gido M and Tuytelaars, Tinne and Zhang, Xiao and Dou, Dejing and Wu, Ji and Hou, Saihui and Pan, Xinyu and Loy, Chen Change and Wang, Zilei and Lin, Dahua and Yu, Longhui and Hu, Tianyang and Hong, Lanqing and Liu, Zhen and Weller, Adrian and Liu, Weiyang and Chaudhry, Arslan and Gordo, Albert and Dokania, Puneet and Torr, Philip and {Lopez-Paz}, David and Hui, Like and Belkin, Mikhail and Nakkiran, Preetum and Lu, Jianfeng and Steinerberger, Stefan and Hong, Wanli and Ling, Shuyang and Mixon, Dustin G and Parshall, Hans and Pi, Jianzong and Wojtowytsch, Stephan and others and Fang, Cong and He, Hangfeng and Long, Qi and Su, Weijie J and Han, {\relax XY} and Papyan, Vardan and Donoho, David L and Liu, Xuantong and Zhang, Jianfeng and Hu, Tianyang and Cao, He and Yao, Yuan and Pan, Lujia and Poggio, Tomaso and Liao, Qianli and Galanti, Tomer and Gy{\"o}rgy, Andr{\'a}s and Hutter, Marcus and Borsos, Zal{\'a}n and Mutny, Mojmir and Krause, Andreas and Torralba, Antonio and Fergus, Rob and Freeman, William T and Wah, Catherine and Branson, Steve and Welinder, Peter and Perona, Pietro and Belongie, Serge and Wu, Yue and Chen, Yinpeng and Wang, Lijuan and Ye, Yuancheng and Liu, Zicheng and Guo, Yandong and Fu, Yun and Benjamin, Ari S. and Rolnick, David and Kording, Konrad},
  year = 2019,
  journal = {Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  volume = {30},
  number = {11},
  pages = {374--382},
  publisher = {California Institute of Technology},
  annotation = {Backup Publisher: PMLR}
}

@inproceedings{wuMultiTaskLearningUpper2023,
  title = {Is {{Multi-Task Learning}} an {{Upper Bound}} for {{Continual Learning}}?},
  booktitle = {{{ICASSP}} 2023 - 2023 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Wu, Zihao and Tran, Huy and Pirsiavash, Hamed and Kolouri, Soheil},
  year = 2023,
  pages = {1--5}
}

@misc{wuReFTRepresentationFinetuning2024,
  title = {{{ReFT}}: {{Representation Finetuning}} for {{Language Models}}},
  shorttitle = {{{ReFT}}},
  author = {Wu, Zhengxuan and Arora, Aryaman and Wang, Zheng and Geiger, Atticus and Jurafsky, Dan and Manning, Christopher D. and Potts, Christopher},
  year = 2024,
  month = may,
  number = {arXiv:2404.03592},
  eprint = {2404.03592},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2404.03592},
  urldate = {2025-09-29},
  abstract = {Parameter-efficient finetuning (PEFT) methods seek to adapt large neural models via updates to a small number of weights. However, much prior interpretability work has shown that representations encode rich semantic information, suggesting that editing representations might be a more powerful alternative. We pursue this hypothesis by developing a family of Representation Finetuning (ReFT) methods. ReFT methods operate on a frozen base model and learn task-specific interventions on hidden representations. We define a strong instance of the ReFT family, Low-rank Linear Subspace ReFT (LoReFT), and we identify an ablation of this method that trades some performance for increased efficiency. Both are drop-in replacements for existing PEFTs and learn interventions that are 15x--65x more parameter-efficient than LoRA. We showcase LoReFT on eight commonsense reasoning tasks, four arithmetic reasoning tasks, instruction-tuning, and GLUE. In all these evaluations, our ReFTs deliver the best balance of efficiency and performance, and almost always outperform state-of-the-art PEFTs. We release a generic ReFT training library publicly at https://github.com/stanfordnlp/pyreft.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/9WI6DQ2W/Wu et al. - 2024 - ReFT Representation Finetuning for Language Models.pdf;/Users/giulialanzillotta/Zotero/storage/3SRTIX97/2404.html}
}

@misc{wuScalingLawDataEfficient2025,
  title = {Beyond {{Scaling Law}}: {{A Data-Efficient Distillation Framework}} for {{Reasoning}}},
  shorttitle = {Beyond {{Scaling Law}}},
  author = {Wu, Xiaojun and Jiang, Xiaoguang and Li, Huiyang and Zhai, Jucai and Liu, Dengfeng and Hao, Qiaobo and Liu, Huang and Yang, Zhiguo and Xie, Ji and Gu, Ninglun and Yang, Jin and Zhang, Kailai and Bao, Yelun and Wang, Jun},
  year = 2025,
  month = aug,
  number = {arXiv:2508.09883},
  eprint = {2508.09883},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2508.09883},
  urldate = {2025-10-17},
  abstract = {Large language models (LLMs) demonstrate remarkable reasoning capabilities in tasks such as algorithmic coding and mathematical problem-solving. Recent methods have improved reasoning through expanded corpus and multistage training combining reinforcement learning and supervised fine-tuning. Although some methods suggest that small but targeted dataset can incentivize reasoning via only distillation, a reasoning scaling laws is still taking shape, increasing computational costs. To address this, we propose a data-efficient distillation framework (DED) that optimizes the Pareto frontier of reasoning distillation. Inspired by the on-policy learning and diverse roll-out strategies of reinforcement learning, the key idea of our approach is threefold: (1) We identify that benchmark scores alone do not determine an effective teacher model. Through comprehensive comparisons of leading reasoning LLMs, we develop a method to select an optimal teacher model. (2) While scaling distillation can enhance reasoning, it often degrades out-of-domain performance. A carefully curated, smaller corpus achieves a balanced trade-off between in-domain and out-of-domain capabilities. (3) Diverse reasoning trajectories encourage the student model to develop robust reasoning skills. We validate our method through evaluations on mathematical reasoning (AIME 2024/2025, MATH-500) and code generation (LiveCodeBench), achieving state-of-the-art results with only 0.8k carefully curated examples, bypassing the need for extensive scaling. Our systematic analysis demonstrates that DED outperforms existing methods by considering factors beyond superficial hardness, token length, or teacher model capability. This work offers a practical and efficient pathway to advanced reasoning while preserving general capabilities.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/FH4MYVIT/Wu et al. - 2025 - Beyond Scaling Law A Data-Efficient Distillation Framework for Reasoning.pdf;/Users/giulialanzillotta/Zotero/storage/GABGBN8C/2508.html}
}

@misc{wuSDLoRAScalableDecoupled2025,
  title = {{{SD-LoRA}}: {{Scalable Decoupled Low-Rank Adaptation}} for {{Class Incremental Learning}}},
  shorttitle = {{{SD-LoRA}}},
  author = {Wu, Yichen and Piao, Hongming and Huang, Long-Kai and Wang, Renzhen and Li, Wanhua and Pfister, Hanspeter and Meng, Deyu and Ma, Kede and Wei, Ying},
  year = 2025,
  month = mar,
  number = {arXiv:2501.13198},
  eprint = {2501.13198},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2501.13198},
  urldate = {2025-10-01},
  abstract = {Continual Learning (CL) with foundation models has recently emerged as a promising paradigm to exploit abundant knowledge acquired during pre-training for tackling sequential tasks. However, existing prompt-based and Low-Rank Adaptation-based (LoRA-based) methods often require expanding a prompt/LoRA pool or retaining samples of previous tasks, which poses significant scalability challenges as the number of tasks grows. To address these limitations, we propose Scalable Decoupled LoRA (SD-LoRA) for class incremental learning, which continually separates the learning of the magnitude and direction of LoRA components without rehearsal. Our empirical and theoretical analysis reveals that SD-LoRA tends to follow a low-loss trajectory and converges to an overlapping low-loss region for all learned tasks, resulting in an excellent stability-plasticity trade-off. Building upon these insights, we introduce two variants of SD-LoRA with further improved parameter efficiency. All parameters of SD-LoRAs can be end-to-end optimized for CL objectives. Meanwhile, they support efficient inference by allowing direct evaluation with the finally trained model, obviating the need for component selection. Extensive experiments across multiple CL benchmarks and foundation models consistently validate the effectiveness of SD-LoRA. The code is available at https://github.com/WuYichen-97/SD-Lora-CL.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/4XJH4E7G/Wu et al. - 2025 - SD-LoRA Scalable Decoupled Low-Rank Adaptation for Class Incremental Learning.pdf;/Users/giulialanzillotta/Zotero/storage/TA7NVJX3/2501.html}
}

@article{xuComparativeKnowledgeDistillation,
  title = {Comparative {{Knowledge Distillation}}},
  author = {Xu, Alex Tianyi and Wilf, Alex and Liang, Paul Pu and Obolenskiy, Alexander and Fried, Daniel and Morency, Louis-Philippe},
  abstract = {In the era of large-scale pretrained models, Knowledge Distillation (KD) serves an important role in transferring the wisdom of computationally-heavy teacher models to lightweight, efficient student models while preserving performance. Yet KD settings often assume readily available access to teacher models capable of performing many inferences---a notion increasingly at odds with the realities of costly large-scale models. Addressing this gap, we study an important question: how KD algorithms fare as the number of teacher inferences decreases, a setting we term Reduced-Teacher-Inference Knowledge Distillation (RTIKD). We observe that the performance of prevalent KD techniques and state-of-the-art data augmentation strategies suffers considerably as the number of teacher inferences is reduced. One class of approaches, termed ``relational'' knowledge distillation underperforms the rest, yet we hypothesize that they hold promise for reduced dependency on teacher models because they can augment the effective dataset size without additional teacher calls. We find that a simple change --- performing high-dimensional comparisons instead of low-dimensional relations, which we term Comparative Knowledge Distillation --- vaults performance well over existing KD approaches. We perform empirical evaluation across varied experimental settings and rigorous analysis to understand the learning outcomes of our method. All code is made publicly available.},
  langid = {english},
  file = {/Users/giulialanzillotta/Zotero/storage/4EL9MAYA/Xu et al. - Comparative Knowledge Distillation.pdf}
}

@article{xuComparativeKnowledgeDistillationa,
  title = {Comparative {{Knowledge Distillation}}},
  author = {Xu, Alex Tianyi and Wilf, Alex and Liang, Paul Pu and Obolenskiy, Alexander and Fried, Daniel and Morency, Louis-Philippe},
  abstract = {In the era of large-scale pretrained models, Knowledge Distillation (KD) serves an important role in transferring the wisdom of computationally-heavy teacher models to lightweight, efficient student models while preserving performance. Yet KD settings often assume readily available access to teacher models capable of performing many inferences---a notion increasingly at odds with the realities of costly large-scale models. Addressing this gap, we study an important question: how KD algorithms fare as the number of teacher inferences decreases, a setting we term Reduced-Teacher-Inference Knowledge Distillation (RTIKD). We observe that the performance of prevalent KD techniques and state-of-the-art data augmentation strategies suffers considerably as the number of teacher inferences is reduced. One class of approaches, termed ``relational'' knowledge distillation underperforms the rest, yet we hypothesize that they hold promise for reduced dependency on teacher models because they can augment the effective dataset size without additional teacher calls. We find that a simple change --- performing high-dimensional comparisons instead of low-dimensional relations, which we term Comparative Knowledge Distillation --- vaults performance well over existing KD approaches. We perform empirical evaluation across varied experimental settings and rigorous analysis to understand the learning outcomes of our method. All code is made publicly available.},
  langid = {english},
  file = {/Users/giulialanzillotta/Zotero/storage/D27E52GS/Xu et al. - Comparative Knowledge Distillation.pdf}
}

@misc{yanDynamicallyExpandableRepresentation2021,
  title = {{{DER}}: {{Dynamically Expandable Representation}} for {{Class Incremental Learning}}},
  shorttitle = {{{DER}}},
  author = {Yan, Shipeng and Xie, Jiangwei and He, Xuming},
  year = 2021,
  month = mar,
  number = {arXiv:2103.16788},
  eprint = {2103.16788},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2103.16788},
  urldate = {2025-09-30},
  abstract = {We address the problem of class incremental learning, which is a core step towards achieving adaptive vision intelligence. In particular, we consider the task setting of incremental learning with limited memory and aim to achieve better stability-plasticity trade-off. To this end, we propose a novel two-stage learning approach that utilizes a dynamically expandable representation for more effective incremental concept modeling. Specifically, at each incremental step, we freeze the previously learned representation and augment it with additional feature dimensions from a new learnable feature extractor. This enables us to integrate new visual concepts with retaining learned knowledge. We dynamically expand the representation according to the complexity of novel concepts by introducing a channel-level mask-based pruning strategy. Moreover, we introduce an auxiliary loss to encourage the model to learn diverse and discriminate features for novel concepts. We conduct extensive experiments on the three class incremental learning benchmarks and our method consistently outperforms other methods with a large margin.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/JL83GJJG/Yan et al. - 2021 - DER Dynamically Expandable Representation for Class Incremental Learning.pdf;/Users/giulialanzillotta/Zotero/storage/IV5TJ3GJ/2103.html}
}

@misc{yangFeatureLearningInfiniteWidth2022,
  title = {Feature {{Learning}} in {{Infinite-Width Neural Networks}}},
  author = {Yang, Greg and Hu, Edward J.},
  year = 2022,
  month = jul,
  number = {arXiv:2011.14522},
  eprint = {2011.14522},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2011.14522},
  urldate = {2025-05-12},
  abstract = {As its width tends to infinity, a deep neural network's behavior under gradient descent can become simplified and predictable (e.g. given by the Neural Tangent Kernel (NTK)), if it is parametrized appropriately (e.g. the NTK parametrization). However, we show that the standard and NTK parametrizations of a neural network do not admit infinite-width limits that can learn features, which is crucial for pretraining and transfer learning such as with BERT. We propose simple modifications to the standard parametrization to allow for feature learning in the limit. Using the *Tensor Programs* technique, we derive explicit formulas for such limits. On Word2Vec and few-shot learning on Omniglot via MAML, two canonical tasks that rely crucially on feature learning, we compute these limits exactly. We find that they outperform both NTK baselines and finite-width networks, with the latter approaching the infinite-width feature learning performance as width increases. More generally, we classify a natural space of neural network parametrizations that generalizes standard, NTK, and Mean Field parametrizations. We show 1) any parametrization in this space either admits feature learning or has an infinite-width training dynamics given by kernel gradient descent, but not both; 2) any such infinite-width limit can be computed using the Tensor Programs technique. Code for our experiments can be found at github.com/edwardjhu/TP4.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Condensed Matter - Disordered Systems and Neural Networks},
  file = {/Users/giulialanzillotta/Zotero/storage/GCNV3XDG/Yang and Hu - 2022 - Feature Learning in Infinite-Width Neural Networks.pdf;/Users/giulialanzillotta/Zotero/storage/4GKS4ZBI/2011.html}
}

@article{yangNeuralCollapseInspired2023,
  title = {Neural Collapse Inspired Feature-Classifier Alignment for Few-Shot Class Incremental Learning},
  author = {Yang, Yibo and Yuan, Haobo and Li, Xiangtai and Lin, Zhouchen and Torr, Philip and Tao, Dacheng},
  year = 2023,
  journal = {arXiv preprint arXiv:2302.03004},
  eprint = {2302.03004},
  archiveprefix = {arXiv}
}

@misc{yangNeuralCollapseTerminus2023,
  title = {Neural {{Collapse Terminus}}: {{A Unified Solution}} for {{Class Incremental Learning}} and {{Its Variants}}},
  shorttitle = {Neural {{Collapse Terminus}}},
  author = {Yang, Yibo and Yuan, Haobo and Li, Xiangtai and Wu, Jianlong and Zhang, Lefei and Lin, Zhouchen and Torr, Philip and Tao, Dacheng and Ghanem, Bernard},
  year = 2023,
  month = aug,
  number = {arXiv:2308.01746},
  eprint = {2308.01746},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2308.01746},
  urldate = {2025-09-16},
  abstract = {How to enable learnability for new classes while keeping the capability well on old classes has been a crucial challenge for class incremental learning. Beyond the normal case, long-tail class incremental learning and few-shot class incremental learning are also proposed to consider the data imbalance and data scarcity, respectively, which are common in real-world implementations and further exacerbate the well-known problem of catastrophic forgetting. Existing methods are specifically proposed for one of the three tasks. In this paper, we offer a unified solution to the misalignment dilemma in the three tasks. Concretely, we propose neural collapse terminus that is a fixed structure with the maximal equiangular inter-class separation for the whole label space. It serves as a consistent target throughout the incremental training to avoid dividing the feature space incrementally. For CIL and LTCIL, we further propose a prototype evolving scheme to drive the backbone features into our neural collapse terminus smoothly. Our method also works for FSCIL with only minor adaptations. Theoretical analysis indicates that our method holds the neural collapse optimality in an incremental fashion regardless of data imbalance or data scarcity. We also design a generalized case where we do not know the total number of classes and whether the data distribution is normal, long-tail, or few-shot for each coming session, to test the generalizability of our method. Extensive experiments with multiple datasets are conducted to demonstrate the effectiveness of our unified solution to all the three tasks and the generalized case.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/W4Y4X9JB/Yang et al. - 2023 - Neural Collapse Terminus A Unified Solution for Class Incremental Learning and Its Variants.pdf;/Users/giulialanzillotta/Zotero/storage/FWJQHVRF/2308.html}
}

@misc{yangTensorProgramsII2020,
  title = {Tensor {{Programs II}}: {{Neural Tangent Kernel}} for {{Any Architecture}}},
  shorttitle = {Tensor {{Programs II}}},
  author = {Yang, Greg},
  year = 2020,
  month = nov,
  number = {arXiv:2006.14548},
  eprint = {2006.14548},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2006.14548},
  urldate = {2025-07-28},
  abstract = {We prove that a randomly initialized neural network of *any architecture* has its Tangent Kernel (NTK) converge to a deterministic limit, as the network widths tend to infinity. We demonstrate how to calculate this limit. In prior literature, the heuristic study of neural network gradients often assumes every weight matrix used in forward propagation is independent from its transpose used in backpropagation (Schoenholz et al. 2017). This is known as the *gradient independence assumption (GIA)*. We identify a commonly satisfied condition, which we call *Simple GIA Check*, such that the NTK limit calculation based on GIA is correct. Conversely, when Simple GIA Check fails, we show GIA can result in wrong answers. Our material here presents the NTK results of Yang (2019a) in a friendly manner and showcases the *tensor programs* technique for understanding wide neural networks. We provide reference implementations of infinite-width NTKs of recurrent neural network, transformer, and batch normalization at https://github.com/thegregyang/NTK4A.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Condensed Matter - Disordered Systems and Neural Networks,Statistics - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/8HYK7HU5/Yang - 2020 - Tensor Programs II Neural Tangent Kernel for Any Architecture.pdf;/Users/giulialanzillotta/Zotero/storage/I7WBVLUZ/2006.html}
}

@article{yinOptimizationGeneralizationRegularizationbased2020,
  title = {Optimization and Generalization of Regularization-Based Continual Learning: A Loss Approximation Viewpoint},
  author = {Yin, Dong and Farajtabar, Mehrdad and Li, Ang and Levine, Nir and Mott, Alex},
  year = 2020,
  journal = {arXiv preprint arXiv:2006.10974},
  eprint = {2006.10974},
  archiveprefix = {arXiv}
}

@misc{yinOptimizationGeneralizationRegularizationBased2021,
  title = {Optimization and {{Generalization}} of {{Regularization-Based Continual Learning}}: A {{Loss Approximation Viewpoint}}},
  shorttitle = {Optimization and {{Generalization}} of {{Regularization-Based Continual Learning}}},
  author = {Yin, Dong and Farajtabar, Mehrdad and Li, Ang and Levine, Nir and Mott, Alex},
  year = 2021,
  month = feb,
  number = {arXiv:2006.10974},
  eprint = {2006.10974},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2006.10974},
  urldate = {2025-06-30},
  abstract = {Neural networks have achieved remarkable success in many cognitive tasks. However, when they are trained sequentially on multiple tasks without access to old data, their performance on early tasks tend to drop significantly. This problem is often referred to as catastrophic forgetting, a key challenge in continual learning of neural networks. The regularization-based approach is one of the primary classes of methods to alleviate catastrophic forgetting. In this paper, we provide a novel viewpoint of regularization-based continual learning by formulating it as a second-order Taylor approximation of the loss function of each task. This viewpoint leads to a unified framework that can be instantiated to derive many existing algorithms such as Elastic Weight Consolidation and Kronecker factored Laplace approximation. Based on this viewpoint, we study the optimization aspects (i.e., convergence) as well as generalization properties (i.e., finite-sample guarantees) of regularization-based continual learning. Our theoretical results indicate the importance of accurate approximation of the Hessian matrix. The experimental results on several benchmarks provide empirical validation of our theoretical findings.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/TS65JY4S/Yin et al. - 2021 - Optimization and Generalization of Regularization-Based Continual Learning a Loss Approximation Vie.pdf;/Users/giulialanzillotta/Zotero/storage/2B5JS22Q/2006.html}
}

@misc{yinOptimizationGeneralizationRegularizationBased2021a,
  title = {Optimization and {{Generalization}} of {{Regularization-Based Continual Learning}}: A {{Loss Approximation Viewpoint}}},
  shorttitle = {Optimization and {{Generalization}} of {{Regularization-Based Continual Learning}}},
  author = {Yin, Dong and Farajtabar, Mehrdad and Li, Ang and Levine, Nir and Mott, Alex},
  year = 2021,
  month = feb,
  number = {arXiv:2006.10974},
  eprint = {2006.10974},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2006.10974},
  urldate = {2025-09-30},
  abstract = {Neural networks have achieved remarkable success in many cognitive tasks. However, when they are trained sequentially on multiple tasks without access to old data, their performance on early tasks tend to drop significantly. This problem is often referred to as catastrophic forgetting, a key challenge in continual learning of neural networks. The regularization-based approach is one of the primary classes of methods to alleviate catastrophic forgetting. In this paper, we provide a novel viewpoint of regularization-based continual learning by formulating it as a second-order Taylor approximation of the loss function of each task. This viewpoint leads to a unified framework that can be instantiated to derive many existing algorithms such as Elastic Weight Consolidation and Kronecker factored Laplace approximation. Based on this viewpoint, we study the optimization aspects (i.e., convergence) as well as generalization properties (i.e., finite-sample guarantees) of regularization-based continual learning. Our theoretical results indicate the importance of accurate approximation of the Hessian matrix. The experimental results on several benchmarks provide empirical validation of our theoretical findings.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/X8RIUTCA/Yin et al. - 2021 - Optimization and Generalization of Regularization-Based Continual Learning a Loss Approximation Vie.pdf;/Users/giulialanzillotta/Zotero/storage/2ZV9ACEU/2006.html}
}

@misc{yinStochasticVariationalPropagation2025,
  title = {Stochastic {{Variational Propagation}}: {{Local}}, {{Scalable}} and {{Efficient Alternative}} to {{Backpropagation}}},
  shorttitle = {Stochastic {{Variational Propagation}}},
  author = {Yin, Bojian and Corradi, Federico},
  year = 2025,
  month = may,
  number = {arXiv:2505.05181},
  eprint = {2505.05181},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2505.05181},
  urldate = {2025-05-09},
  abstract = {Backpropagation (BP) is the cornerstone of deep learning, but its reliance on global gradient synchronization limits scalability and imposes significant memory overhead. We propose Stochastic Variational Propagation (SVP), a scalable alternative that reframes training as hierarchical variational inference. SVP treats layer activations as latent variables and optimizes local Evidence Lower Bounds (ELBOs), enabling independent, local updates while preserving global coherence. However, directly applying KL divergence in layer-wise ELBOs risks inter-layer's representation collapse due to excessive compression. To prevent this, SVP projects activations into low-dimensional spaces via fixed random matrices, ensuring information preservation and representational diversity. Combined with a feature alignment loss for inter-layer consistency, SVP achieves competitive accuracy with BP across diverse architectures (MLPs, CNNs, Transformers) and datasets (MNIST to ImageNet), reduces memory usage by up to 4x, and significantly improves scalability. More broadly, SVP introduces a probabilistic perspective to deep representation learning, opening pathways toward more modular and interpretable neural network design.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/8QHVKBT8/Yin and Corradi - 2025 - Stochastic Variational Propagation Local, Scalable and Efficient Alternative to Backpropagation.pdf;/Users/giulialanzillotta/Zotero/storage/Q2M2PDBY/2505.html}
}

@misc{yoonLifelongLearningDynamically2018,
  title = {Lifelong {{Learning}} with {{Dynamically Expandable Networks}}},
  author = {Yoon, Jaehong and Yang, Eunho and Lee, Jeongtae and Hwang, Sung Ju},
  year = 2018,
  month = jun,
  number = {arXiv:1708.01547},
  eprint = {1708.01547},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1708.01547},
  urldate = {2025-10-01},
  abstract = {We propose a novel deep network architecture for lifelong learning which we refer to as Dynamically Expandable Network (DEN), that can dynamically decide its network capacity as it trains on a sequence of tasks, to learn a compact overlapping knowledge sharing structure among tasks. DEN is efficiently trained in an online manner by performing selective retraining, dynamically expands network capacity upon arrival of each task with only the necessary number of units, and effectively prevents semantic drift by splitting/duplicating units and timestamping them. We validate DEN on multiple public datasets under lifelong learning scenarios, on which it not only significantly outperforms existing lifelong learning methods for deep networks, but also achieves the same level of performance as the batch counterparts with substantially fewer number of parameters. Further, the obtained network fine-tuned on all tasks obtained significantly better performance over the batch models, which shows that it can be used to estimate the optimal network structure even when all tasks are available in the first place.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/ECNXHXZR/Yoon et al. - 2018 - Lifelong Learning with Dynamically Expandable Networks.pdf;/Users/giulialanzillotta/Zotero/storage/SXAAVRHU/1708.html}
}

@article{yoonOnlineCoresetSelection2021,
  title = {Online Coreset Selection for Rehearsal-Based Continual Learning},
  author = {Yoon, Jaehong and Madaan, Divyam and Yang, Eunho and Hwang, Sung Ju},
  year = 2021,
  journal = {arXiv preprint arXiv:2106.01085},
  eprint = {2106.01085},
  archiveprefix = {arXiv}
}

@misc{yoonOnlineCoresetSelection2022,
  title = {Online {{Coreset Selection}} for {{Rehearsal-based Continual Learning}}},
  author = {Yoon, Jaehong and Madaan, Divyam and Yang, Eunho and Hwang, Sung Ju},
  year = 2022,
  month = mar,
  number = {arXiv:2106.01085},
  eprint = {2106.01085},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2106.01085},
  urldate = {2025-09-30},
  abstract = {A dataset is a shred of crucial evidence to describe a task. However, each data point in the dataset does not have the same potential, as some of the data points can be more representative or informative than others. This unequal importance among the data points may have a large impact in rehearsal-based continual learning, where we store a subset of the training examples (coreset) to be replayed later to alleviate catastrophic forgetting. In continual learning, the quality of the samples stored in the coreset directly affects the model's effectiveness and efficiency. The coreset selection problem becomes even more important under realistic settings, such as imbalanced continual learning or noisy data scenarios. To tackle this problem, we propose Online Coreset Selection (OCS), a simple yet effective method that selects the most representative and informative coreset at each iteration and trains them in an online manner. Our proposed method maximizes the model's adaptation to a current dataset while selecting high-affinity samples to past tasks, which directly inhibits catastrophic forgetting. We validate the effectiveness of our coreset selection mechanism over various standard, imbalanced, and noisy datasets against strong continual learning baselines, demonstrating that it improves task adaptation and prevents catastrophic forgetting in a sample-efficient manner.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/giulialanzillotta/Zotero/storage/LUMKCQG6/Yoon et al. - 2022 - Online Coreset Selection for Rehearsal-based Continual Learning.pdf;/Users/giulialanzillotta/Zotero/storage/X6NBSJAX/2106.html}
}

@article{yuContinualLearningModeling2022,
  title = {Continual Learning by Modeling Intra-Class Variation},
  author = {Yu, Longhui and Hu, Tianyang and Hong, Lanqing and Liu, Zhen and Weller, Adrian and Liu, Weiyang},
  year = 2022,
  journal = {arXiv preprint arXiv:2210.05398},
  eprint = {2210.05398},
  archiveprefix = {arXiv}
}

@article{yuGradientSurgeryMultitask2020,
  title = {Gradient Surgery for Multi-Task Learning},
  author = {Yu, Tianhe and Kumar, Saurabh and Gupta, Abhishek and Levine, Sergey and Hausman, Karol and Finn, Chelsea},
  year = 2020,
  journal = {Advances in Neural Information Processing Systems},
  volume = {33},
  pages = {5824--5836}
}

@inproceedings{yuMetaWorldBenchmarkEvaluation2020,
  title = {Meta-{{World}}: {{A Benchmark}} and {{Evaluation}} for {{Multi-Task}} and {{Meta Reinforcement Learning}}},
  booktitle = {Proceedings of the {{Conference}} on {{Robot Learning}} ({{CoRL}})},
  author = {Yu, Ting and Quillen, Daniel and He, Zhizhong and Julian, Roberto and Hausman, Kyle and Finn, Chelsea and Levine, Sergey},
  year = 2020,
  pages = {1094--1100},
  publisher = {PMLR},
  doi = {10.48550/arXiv.1910.10897}
}

@inproceedings{zenkeContinualLearningSynaptic2017,
  title = {Continual Learning through Synaptic Intelligence},
  booktitle = {Proceedings of the 34th {{International Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {Zenke, Friedeman and Poole, Ben and Ganguli, Surya},
  year = 2017,
  volume = {70},
  pages = {3987--3995},
  publisher = {PMLR}
}

@article{zhaExperienceReplayOptimization2019,
  title = {Experience {{Replay Optimization}}},
  author = {Zha, Daochen and Lai, Kwei-Herng and Zhou, Kaixiong and Hu, Xia},
  year = 2019,
  pages = {4243--4249},
  urldate = {2025-10-14},
  abstract = {Electronic proceedings of IJCAI 2019},
  file = {/Users/giulialanzillotta/Zotero/storage/ST4MPUL2/589.html}
}

@inproceedings{zhangContinualSequenceGeneration2022,
  title = {Continual {{Sequence Generation}} with {{Adaptive Compositional Modules}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Zhang, Yanzhe and Wang, Xuezhi and Yang, Diyi},
  editor = {Muresan, Smaranda and Nakov, Preslav and Villavicencio, Aline},
  year = 2022,
  month = may,
  pages = {3653--3667},
  publisher = {Association for Computational Linguistics},
  address = {Dublin, Ireland},
  doi = {10.18653/v1/2022.acl-long.255},
  urldate = {2025-10-01},
  abstract = {Continual learning is essential for real-world deployment when there is a need to quickly adapt the model to new tasks without forgetting knowledge of old tasks. Existing work on continual sequence generation either always reuses existing parameters to learn new tasks, which is vulnerable to catastrophic forgetting on dissimilar tasks, or blindly adds new parameters for every new task, which could prevent knowledge sharing between similar tasks. To get the best of both worlds, in this work, we propose continual sequence generation with adaptive compositional modules to adaptively add modules in transformer architectures and compose both old and new modules for new tasks. We also incorporate pseudo experience replay to facilitate knowledge transfer in those shared modules. Experiment results on various sequences of generation tasks show that our framework can adaptively add modules or reuse modules based on task similarity, outperforming state-of-the-art baselines in terms of both performance and parameter efficiency. We make our code public at https://github.com/GT-SALT/Adaptive-Compositional-Modules.},
  file = {/Users/giulialanzillotta/Zotero/storage/DPLG9Z6M/Zhang et al. - 2022 - Continual Sequence Generation with Adaptive Compositional Modules.pdf}
}

@article{zhangDeeperLookExperience2020,
  title = {A {{Deeper Look}} at {{Experience Replay}}},
  author = {Zhang, Shangtong and Sutton, Richard S.},
  year = 2020,
  journal = {Journal or Conference Name},
  volume = {Volume Number},
  pages = {Page Numbers}
}

@article{zhangFeatureForgettingContinual2022,
  title = {Feature Forgetting in Continual Representation Learning},
  author = {Zhang, Xiao and Dou, Dejing and Wu, Ji},
  year = 2022,
  journal = {arXiv preprint arXiv:2205.13359},
  eprint = {2205.13359},
  archiveprefix = {arXiv}
}

@inproceedings{zhangLessBetterEfficient2025,
  title = {Less, but {{Better}}: {{Efficient Multilingual Expansion}} for {{LLMs}} via {{Layer-wise Mixture-of-Experts}}},
  shorttitle = {Less, but {{Better}}},
  booktitle = {Proceedings of the 63rd {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Zhang, Xue and Liang, Yunlong and Meng, Fandong and Zhang, Songming and Chen, Yufeng and Xu, Jinan and Zhou, Jie},
  editor = {Che, Wanxiang and Nabende, Joyce and Shutova, Ekaterina and Pilehvar, Mohammad Taher},
  year = 2025,
  month = jul,
  pages = {17948--17963},
  publisher = {Association for Computational Linguistics},
  address = {Vienna, Austria},
  doi = {10.18653/v1/2025.acl-long.878},
  urldate = {2025-10-01},
  abstract = {Continually expanding new languages for existing large language models (LLMs) is a promising yet challenging approach to building powerful multilingual LLMs.The biggest challenge is to make the model continuously learn new languages while preserving the proficient ability of old languages.To achieve this, recent work utilizes the Mixture-of-Experts (MoE) architecture to expand new languages by adding new experts and avoid catastrophic forgetting of old languages by routing corresponding tokens to the original model backbone (old experts).Although intuitive, this kind of method is parameter-costly when expanding new languages and still inevitably impacts the performance of old languages.To address these limitations, we analyze the language characteristics of different layers in LLMs and propose a layer-wise expert allocation algorithm (LayerMoE) to determine the appropriate number of new experts for each layer.Specifically, we find different layers in LLMs exhibit different representation similarities between languages and then utilize the similarity as the indicator to allocate experts for each layer, i.e., the higher similarity, the fewer experts.Additionally, to further mitigate the forgetting of old languages, we add a classifier in front of the router network on the layers with higher similarity to guide the routing of old language tokens.Experimental results show that our method outperforms the previous state-of-the-art baseline with 60\% fewer experts in the single-expansion setting and with 33.3\% fewer experts in the lifelong-expansion setting, demonstrating the effectiveness of our method.},
  isbn = {979-8-89176-251-0},
  file = {/Users/giulialanzillotta/Zotero/storage/EIEKUDXF/Zhang et al. - 2025 - Less, but Better Efficient Multilingual Expansion for LLMs via Layer-wise Mixture-of-Experts.pdf}
}

@article{zhangUnderstandingDeepLearning2021,
  title = {Understanding Deep Learning (Still) Requires Rethinking Generalization},
  author = {Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  year = 2021,
  journal = {Communications of the ACM},
  volume = {64},
  number = {3},
  pages = {107--115},
  publisher = {ACM New York, NY, USA}
}

@misc{zhaoMaintainingDiscriminationFairness2019,
  title = {Maintaining {{Discrimination}} and {{Fairness}} in {{Class Incremental Learning}}},
  author = {Zhao, Bowen and Xiao, Xi and Gan, Guojun and Zhang, Bin and Xia, Shutao},
  year = 2019,
  month = nov,
  number = {arXiv:1911.07053},
  eprint = {1911.07053},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1911.07053},
  urldate = {2025-10-01},
  abstract = {Deep neural networks (DNNs) have been applied in class incremental learning, which aims to solve common real-world problems of learning new classes continually. One drawback of standard DNNs is that they are prone to catastrophic forgetting. Knowledge distillation (KD) is a commonly used technique to alleviate this problem. In this paper, we demonstrate it can indeed help the model to output more discriminative results within old classes. However, it cannot alleviate the problem that the model tends to classify objects into new classes, causing the positive effect of KD to be hidden and limited. We observed that an important factor causing catastrophic forgetting is that the weights in the last fully connected (FC) layer are highly biased in class incremental learning. In this paper, we propose a simple and effective solution motivated by the aforementioned observations to address catastrophic forgetting. Firstly, we utilize KD to maintain the discrimination within old classes. Then, to further maintain the fairness between old classes and new classes, we propose Weight Aligning (WA) that corrects the biased weights in the FC layer after normal training process. Unlike previous work, WA does not require any extra parameters or a validation set in advance, as it utilizes the information provided by the biased weights themselves. The proposed method is evaluated on ImageNet-1000, ImageNet-100, and CIFAR-100 under various settings. Experimental results show that the proposed method can effectively alleviate catastrophic forgetting and significantly outperform state-of-the-art methods.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/giulialanzillotta/Zotero/storage/PMYNECLT/Zhao et al. - 2019 - Maintaining Discrimination and Fairness in Class Incremental Learning.pdf;/Users/giulialanzillotta/Zotero/storage/I37DZ22K/1911.html}
}

@inproceedings{zhouAreAllLayers2025,
  title = {Are All Layers Created Equal: {{A}} Neural Collapse Perspective},
  shorttitle = {Are All Layers Created Equal},
  booktitle = {Conference on {{Parsimony}} and {{Learning}}},
  author = {Zhou, Jinxin and Jiang, Jiachen and Zhu, Zhihui},
  year = 2025,
  month = jun,
  pages = {1307--1327},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2025-09-16},
  abstract = {Understanding how features evolve layer by layer is crucial for uncovering the inner workings of deep neural networks. {\textbackslash}textit\{Progressive neural collapse\}, where successive layers increasingly compress within-class features and enhance class separation, has been primarily studied empirically in small architectures on simple tasks or theoretically within linear network contexts. However, its behavior in larger architectures and complex datasets remains underexplored. In this work, we extend the study of progressive neural collapse to larger models and more complex datasets, including clean and noisy data settings, offering a comprehensive understanding of its role in generalization and robustness. Our findings reveal three key insights:1. Layer inequality: Deeper layers significantly enhance neural collapse and play a vital role in generalization but are also more susceptible to memorization. 2. Depth-dependent behavior: In deeper models, middle layers contribute minimally due to a diminished neural collapse enhancement leading to redundancy and limited generalization improvements, which validates the effectiveness of layer pruning. 3. Architectural differences: Transformer models outperform convolutional models in enhancing neural collapse on larger datasets and exhibit greater robustness to memorization, with deeper Transformers reducing memorization while deeper convolutional models show the opposite trend. These findings provide new insights into the hierarchical roles of layers and their interplay with architectural design, shedding light on how deep neural networks process data and generalize across challenging conditions.},
  langid = {english}
}

@inproceedings{zhouFortuitousForgettingConnectionist2021,
  title = {Fortuitous Forgetting in Connectionist Networks},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Zhou, Hattie and Vani, Ankit and Larochelle, Hugo and Courville, Aaron},
  year = 2021
}

@inproceedings{zhouOnlineIncrementalFeature2012,
  title = {Online Incremental Feature Learning with Denoising Autoencoders},
  booktitle = {International {{Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Zhou, G. and Sohn, K. and Lee, H.},
  year = 2012,
  pages = {1453--1461}
}

@article{zhuAnglesSubspacesTheir2013,
  title = {Angles between Subspaces and Their Tangents},
  author = {Zhu, Peizhen and Knyazev, Andrew V},
  year = 2013,
  journal = {Journal of Numerical Mathematics},
  volume = {21},
  number = {4},
  pages = {325--340},
  publisher = {De Gruyter}
}

@inproceedings{zihaowuMultiTaskLearningUpper2019,
  title = {Is {{Multi-Task Learning}} an {{Upper Bound}} for {{Continual Learning}}?},
  booktitle = {Proceedings of the 36th {{International Conference}} on {{Machine Learning}}},
  author = {Zihao Wu, Huy Tran, Hamed Pirsiavash and Kolouri, Soheil},
  year = 2019,
  pages = {6056--6065},
  publisher = {PMLR}
}

@inproceedings{zihaowuOnlineLearningComprehensive2018,
  title = {Online {{Learning}}: {{A Comprehensive Survey}}},
  booktitle = {Proceedings of the {{Conference}} on {{Robot Learning}} ({{CoRL}})},
  author = {Zihao Wu, Huy Tran, Hamed Pirsiavash and Kolouri, Soheil and Hadsell, Raia and Rao, Dushyant and Rusu, Andrei A and Pascanu, Razvan and Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg and Yu, Ting and Quillen, Daniel and He, Zhizhong and Julian, Roberto and Hausman, Kyle and Finn, Chelsea and Levine, Sergey and Hoi, Steven C. H. and Sahoo, Doyen and Lu, Jing and Zhao, Peilin},
  year = 2018,
  volume = {24},
  pages = {1094--1100},
  publisher = {Elsevier},
  doi = {10.48550/arXiv.1910.10897},
  annotation = {Backup Publisher: PMLR}
}

@inproceedings{zinkevichOnlineConvexProgramming2003,
  title = {Online Convex Programming and Generalized Infinitesimal Gradient Ascent},
  booktitle = {Proceedings of the 20th International Conference on Machine Learning (Icml-03)},
  author = {Zinkevich, Martin},
  year = 2003,
  pages = {928--936}
}
